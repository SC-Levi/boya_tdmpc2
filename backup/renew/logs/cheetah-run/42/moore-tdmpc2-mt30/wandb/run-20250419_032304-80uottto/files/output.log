[34m[1mLogs will be synced with wandb.
Config(env_type='MooreMultiTask', n_contexts=1, obs_dim=17, action_dim=6, hidden_dim=256, n_experts=4, moore_temperature=1.0, use_softmax=True, gamma=0.99, stddev=0.1, actor_updates=1, use_ema=True, ensemble_size=5, update_freq=2, seed=42, device=device(type='cuda', index=0), task='cheetah-run', obs='state', multitask=False, steps=10000, batch_size=256, reward_coef=0.1, value_coef=0.1, consistency_coef=20, rho=0.5, lr=0.0003, enc_lr_scale=0.3, grad_clip_norm=20, tau=0.01, discount_denom=5, discount_min=0.95, discount_max=0.995, buffer_size=100000, exp_name='moore-tdmpc2-mt30', eval_freq=50000, eval_episodes=10, moore={'n_experts': 4, 'temperature': 1.0, 'use_softmax': True, 'debug_task_emb': False}, mpc=True, iterations=6, num_samples=512, num_elites=64, num_pi_trajs=24, horizon=3, min_std=0.05, max_std=2, temperature=0.5, log_std_min=-10, log_std_max=2, entropy_coef=0.0001, model_size=5, num_enc_layers=2, enc_dim=256, num_channels=32, mlp_dim=512, latent_dim=512, task_dim=0, num_q=5, dropout=0.01, simnorm_dim=8, num_bins=101, vmin=-10, vmax=10, wandb_project='moore-tdmpc2', wandb_entity='OA-MBRL', wandb_silent=False, enable_wandb=True, save_csv=True, save_video=False, save_agent=True, compile=False, work_dir=PosixPath('/home/levi/Documents/MOORE-TDMPC/moore_tdmpc/logs/cheetah-run/42/moore-tdmpc2-mt30'), task_title='Cheetah Run', tasks=['cheetah-run'], obs_shape={'state': (17,)}, episode_length=500, obs_shapes='???', episode_lengths='???', seed_steps=2500, bin_size=0.2, data_dir='/media/levi/Singe4linux/Moore-TDMPC/TDMPC2/data/mt30')
Architecture: TD-MPC2 Moore World Model
Task Encoder: MooreTaskEncoder(obs_dim=17, latent_dim=512, hidden_dim=256, n_experts=4, temperature=1.0, use_softmax=True)
Dynamics: MoEDynamicsModel(latent_dim=512, action_dim=6, input_dim=518, n_experts=4, temperature=0.5, use_softmax=True)
Reward: MoERewardModel(latent_dim=512, action_dim=6, hidden_dim=256, n_experts=4, temperature=0.5, use_softmax=True, reward_dim=101)
Policy prior: Sequential(
  (0): Linear(in_features=512, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=12, bias=True)
)
Q-functions: Ensemble(n_models=5)
Learnable parameters: 5,596,617
[32m[1må¼€å§‹Moore-TDMPC2è®­ç»ƒ...
 [32meval[39m    [34mE:[39m 0            [34mI:[39m 0            [34mR:[39m 0.8          [34mS:[39m 0.0          [34mT:[39m 0:00:05
 [34mtrain[39m   [34mE:[39m 0            [34mI:[39m 500          [34mR:[39m 8.6          [34mS:[39m 0.0          [34mT:[39m 0:00:05
Buffer capacity: 10,000
Storage required: 0.00 GB
Using CUDA:0 memory for storage.
 [34mtrain[39m   [34mE:[39m 1            [34mI:[39m 1,000        [34mR:[39m 7.9          [34mS:[39m 0.0          [34mT:[39m 0:00:05
 [34mtrain[39m   [34mE:[39m 2            [34mI:[39m 1,500        [34mR:[39m 10.9         [34mS:[39m 0.0          [34mT:[39m 0:00:05
 [34mtrain[39m   [34mE:[39m 3            [34mI:[39m 2,000        [34mR:[39m 10.2         [34mS:[39m 0.0          [34mT:[39m 0:00:05
 [34mtrain[39m   [34mE:[39m 4            [34mI:[39m 2,500        [34mR:[39m 7.9          [34mS:[39m 0.0          [34mT:[39m 0:00:05
Pretraining agent on seed data...
Error executing job with overrides: ['task=cheetah-run', 'steps=10000']
Traceback (most recent call last):
  File "/home/levi/Documents/MOORE-TDMPC/moore_tdmpc/train_moore.py", line 84, in train
    trainer.train()
  File "/home/levi/Documents/MOORE-TDMPC/TDMPC2/tdmpc2/trainer/online_trainer.py", line 111, in train
    _train_metrics = self.agent.update(self.buffer)
  File "/home/levi/Documents/MOORE-TDMPC/moore_tdmpc/train_moore_agent.py", line 963, in update
    return self._update(obs, action, reward, **kwargs)
  File "/home/levi/Documents/MOORE-TDMPC/moore_tdmpc/train_moore_agent.py", line 791, in _update
    loss.backward()
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/torch/_tensor.py", line 624, in backward
    torch.autograd.backward(
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [101]], which is output 0 of AsStridedBackward0, is at version 3076; expected version 3075 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
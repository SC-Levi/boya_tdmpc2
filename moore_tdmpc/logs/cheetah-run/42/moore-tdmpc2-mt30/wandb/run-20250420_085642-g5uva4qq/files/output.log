[34m[1mLogs will be synced with wandb.
Config(env_type='MooreMultiTask', n_contexts=1, obs_dim=17, action_dim=6, hidden_dim=256, n_experts=4, moore_temperature=1.0, use_softmax=True, gamma=0.99, stddev=0.1, actor_updates=1, use_ema=True, ensemble_size=5, update_freq=2, seed=42, device=device(type='cuda', index=0), task='cheetah-run', obs='state', multitask=False, steps=10000, batch_size=256, reward_coef=0.1, value_coef=0.1, consistency_coef=20, rho=0.5, lr=0.0003, enc_lr_scale=0.3, grad_clip_norm=20, tau=0.01, discount_denom=5, discount_min=0.95, discount_max=0.995, buffer_size=100000, exp_name='moore-tdmpc2-mt30', eval_freq=50000, eval_episodes=10, moore={'n_experts': 4, 'temperature': 1.0, 'use_softmax': True, 'debug_task_emb': False}, mpc=True, iterations=6, num_samples=512, num_elites=64, num_pi_trajs=24, horizon=3, min_std=0.05, max_std=2, temperature=0.5, log_std_min=-10, log_std_max=2, entropy_coef=0.0001, model_size=5, num_enc_layers=2, enc_dim=256, num_channels=32, mlp_dim=512, latent_dim=512, task_dim=0, num_q=5, dropout=0.01, simnorm_dim=8, num_bins=101, vmin=-10, vmax=10, wandb_project='moore-tdmpc2', wandb_entity='OA-MBRL', wandb_silent=False, enable_wandb=True, save_csv=True, save_video=False, save_agent=True, compile=False, work_dir=PosixPath('/home/levi/Documents/MOORE-TDMPC/moore_tdmpc/logs/cheetah-run/42/moore-tdmpc2-mt30'), task_title='Cheetah Run', tasks=['cheetah-run'], obs_shape={'state': (17,)}, episode_length=500, obs_shapes='???', episode_lengths='???', seed_steps=2500, bin_size=0.2, data_dir='/media/levi/Singe4linux/Moore-TDMPC/TDMPC2/data/mt30')
Architecture: TD-MPC2 Moore World Model
Task Encoder: MooreTaskEncoder(obs_dim=17, latent_dim=512, hidden_dim=256, n_experts=4, temperature=1.0, use_softmax=True)
Dynamics: MoEDynamicsModel(latent_dim=512, action_dim=6, hidden_dim=256, n_experts=4, temperature=0.5, use_softmax=True)
Reward: MoERewardModel(latent_dim=512, action_dim=6, hidden_dim=256, n_experts=4, temperature=0.5, use_softmax=True, reward_dim=101, top_k=1)
Policy prior: Sequential(
  (0): Linear(in_features=512, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): ReLU()
  (4): Linear(in_features=256, out_features=12, bias=True)
)
Q-functions: Ensemble(n_models=5)
Learnable parameters: 5,991,369
[32m[1m开始Moore-TDMPC2训练...
[36mPROFILER STATUS: Enabled=True, Functions=2, Total Calls=6
[32mOnlineTrainer: Starting training loop with profiling enabled
[36mPROFILER STATUS: Enabled=True, Functions=2, Total Calls=6
[36mPROFILER: forward executed in 1.67ms (call 3)
 [32meval[39m    [34mE:[39m 0            [34mI:[39m 0            [34mR:[39m 0.8          [34mS:[39m 0.0          [34mT:[39m 0:00:07
 [34mtrain[39m   [34mE:[39m 0            [34mI:[39m 500          [34mR:[39m 6.6          [34mS:[39m 0.0          [34mT:[39m 0:00:07
Buffer capacity: 10,000
Storage required: 0.00 GB
Using CUDA:0 memory for storage.
 [34mtrain[39m   [34mE:[39m 1            [34mI:[39m 1,000        [34mR:[39m 7.6          [34mS:[39m 0.0          [34mT:[39m 0:00:07
 [34mtrain[39m   [34mE:[39m 2            [34mI:[39m 1,500        [34mR:[39m 6.4          [34mS:[39m 0.0          [34mT:[39m 0:00:07
 [34mtrain[39m   [34mE:[39m 3            [34mI:[39m 2,000        [34mR:[39m 7.8          [34mS:[39m 0.0          [34mT:[39m 0:00:07
 [34mtrain[39m   [34mE:[39m 4            [34mI:[39m 2,500        [34mR:[39m 9.3          [34mS:[39m 0.0          [34mT:[39m 0:00:07
Pretraining agent on seed data...
[TIME] encode(future):  450.21 ms
[TIME] compute TD targets:    1.37 ms
[TIME] encode(initial): 1528.10 ms
[TIME] rollout & consistency (vectorized):  477.72 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.16 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 58.52 ms
[TIME-FLAT] Gate forward: 1.12 ms
[TIME-FLAT] Softmax + TopK: 0.39 ms
[TIME-FLAT] Sparse Expert Forward: 914.92 ms
[TIME-FLAT] Total flat forward: 975.34 ms
[TIME-VEC] _forward_flat: 975.51 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 975.83 ms
[TIME] reward_preds total:  975.94 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  976.09 ms
[TIME] Q preds:    5.54 ms
[TIME] reward loss:    7.38 ms
[TIME] value loss:    2.40 ms
[TIME] loss computation:    0.24 ms
[TIME] backward: 1785.05 ms
[TIME] grad clipping:    4.28 ms
[TIME] optim.step:   25.73 ms
[TIME] _update total: 5265.77 ms
[36mPROFILER: _update executed in 5294.62ms (call 1)
[TIME] encode(future):  446.61 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1527.55 ms
[TIME] rollout & consistency (vectorized):  450.64 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.62 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 802.25 ms
[TIME-FLAT] Total flat forward: 804.25 ms
[TIME-VEC] _forward_flat: 804.39 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 804.60 ms
[TIME] reward_preds total:  804.69 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  804.84 ms
[TIME] Q preds:    5.45 ms
[TIME] reward loss:    0.86 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1480.12 ms
[TIME] grad clipping:    2.28 ms
[TIME] optim.step:    1.07 ms
[TIME] _update total: 4724.14 ms
[36mPROFILER: _update executed in 4755.73ms (call 2)
[TIME] encode(future):  439.47 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1498.28 ms
[TIME] rollout & consistency (vectorized):  442.01 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 887.25 ms
[TIME-FLAT] Total flat forward: 889.11 ms
[TIME-VEC] _forward_flat: 889.23 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 889.46 ms
[TIME] reward_preds total:  889.57 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  889.75 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1455.37 ms
[TIME] grad clipping:    2.27 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4738.67 ms
[36mPROFILER: _update executed in 4770.73ms (call 3)
[TIME] encode(future):  443.39 ms
[TIME] compute TD targets:    0.97 ms
[TIME] encode(initial): 1495.42 ms
[TIME] rollout & consistency (vectorized):  444.09 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 780.34 ms
[TIME-FLAT] Total flat forward: 782.11 ms
[TIME-VEC] _forward_flat: 782.23 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 782.46 ms
[TIME] reward_preds total:  782.55 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  782.72 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.01 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1456.78 ms
[TIME] grad clipping:    1.91 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4636.27 ms
[TIME] encode(future):  438.01 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1513.91 ms
[TIME] rollout & consistency (vectorized):  443.46 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 779.50 ms
[TIME-FLAT] Total flat forward: 781.40 ms
[TIME-VEC] _forward_flat: 781.53 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 781.76 ms
[TIME] reward_preds total:  781.87 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  782.06 ms
[TIME] Q preds:    5.02 ms
[TIME] reward loss:    0.77 ms
[TIME] value loss:    2.01 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1487.28 ms
[TIME] grad clipping:    1.99 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 4678.26 ms
[TIME] encode(future):  448.16 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1619.08 ms
[TIME] rollout & consistency (vectorized):  451.29 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 787.02 ms
[TIME-FLAT] Total flat forward: 788.93 ms
[TIME-VEC] _forward_flat: 789.07 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 789.29 ms
[TIME] reward_preds total:  789.39 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  789.57 ms
[TIME] Q preds:    5.27 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1484.20 ms
[TIME] grad clipping:    1.86 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4806.08 ms
[TIME] encode(future):  448.54 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1534.12 ms
[TIME] rollout & consistency (vectorized):  451.25 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 799.22 ms
[TIME-FLAT] Total flat forward: 801.07 ms
[TIME-VEC] _forward_flat: 801.19 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 801.40 ms
[TIME] reward_preds total:  801.50 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  801.67 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1496.35 ms
[TIME] grad clipping:    2.43 ms
[TIME] optim.step:    1.18 ms
[TIME] _update total: 4746.21 ms
[TIME] encode(future):  448.52 ms
[TIME] compute TD targets:    0.98 ms
[TIME] encode(initial): 1639.38 ms
[TIME] rollout & consistency (vectorized):  453.03 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 798.64 ms
[TIME-FLAT] Total flat forward: 800.42 ms
[TIME-VEC] _forward_flat: 800.55 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 800.75 ms
[TIME] reward_preds total:  800.84 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.99 ms
[TIME] Q preds:    5.26 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1503.85 ms
[TIME] grad clipping:    2.72 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4860.43 ms
[TIME] encode(future):  450.63 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1516.26 ms
[TIME] rollout & consistency (vectorized):  455.72 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 786.70 ms
[TIME-FLAT] Total flat forward: 788.49 ms
[TIME-VEC] _forward_flat: 788.63 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 788.88 ms
[TIME] reward_preds total:  788.97 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  789.13 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    2.17 ms
[TIME] loss computation:    0.22 ms
[TIME] backward: 1498.91 ms
[TIME] grad clipping:    1.90 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4724.61 ms
[TIME] encode(future):  448.53 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1600.81 ms
[TIME] rollout & consistency (vectorized):  451.93 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 788.10 ms
[TIME-FLAT] Total flat forward: 789.91 ms
[TIME-VEC] _forward_flat: 790.06 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 790.28 ms
[TIME] reward_preds total:  790.37 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  790.52 ms
[TIME] Q preds:    5.23 ms
[TIME] reward loss:    0.88 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.20 ms
[TIME] backward: 1489.80 ms
[TIME] grad clipping:    2.10 ms
[TIME] optim.step:    1.26 ms
[TIME] _update total: 4795.93 ms
[TIME] encode(future):  444.22 ms
[TIME] compute TD targets:    1.09 ms
[TIME] encode(initial): 1513.98 ms
[TIME] rollout & consistency (vectorized):  452.48 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.92 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 790.32 ms
[TIME-FLAT] Total flat forward: 792.28 ms
[TIME-VEC] _forward_flat: 792.42 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 792.63 ms
[TIME] reward_preds total:  792.72 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  792.90 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1485.66 ms
[TIME] grad clipping:    2.04 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4702.99 ms
[TIME] encode(future):  451.10 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1634.51 ms
[TIME] rollout & consistency (vectorized):  451.95 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.89 ms
[TIME-FLAT] Softmax + TopK: 0.47 ms
[TIME-FLAT] Sparse Expert Forward: 792.19 ms
[TIME-FLAT] Total flat forward: 794.26 ms
[TIME-VEC] _forward_flat: 794.37 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 794.61 ms
[TIME] reward_preds total:  794.73 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  794.97 ms
[TIME] Q preds:    5.00 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.18 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1484.66 ms
[TIME] grad clipping:    2.10 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4831.17 ms
[TIME] encode(future):  450.13 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1530.61 ms
[TIME] rollout & consistency (vectorized):  454.03 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 784.29 ms
[TIME-FLAT] Total flat forward: 786.07 ms
[TIME-VEC] _forward_flat: 786.21 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 786.41 ms
[TIME] reward_preds total:  786.50 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  786.67 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1490.74 ms
[TIME] grad clipping:    2.01 ms
[TIME] optim.step:    1.19 ms
[TIME] _update total: 4726.02 ms
[TIME] encode(future):  449.53 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1522.81 ms
[TIME] rollout & consistency (vectorized):  453.95 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 1.08 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 892.83 ms
[TIME-FLAT] Total flat forward: 894.88 ms
[TIME-VEC] _forward_flat: 895.01 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 895.21 ms
[TIME] reward_preds total:  895.32 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  895.49 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.21 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1496.84 ms
[TIME] grad clipping:    2.25 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4832.86 ms
[TIME] encode(future):  455.19 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1527.86 ms
[TIME] rollout & consistency (vectorized):  447.44 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 788.96 ms
[TIME-FLAT] Total flat forward: 790.79 ms
[TIME-VEC] _forward_flat: 790.90 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 791.12 ms
[TIME] reward_preds total:  791.21 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  791.38 ms
[TIME] Q preds:    4.99 ms
[TIME] reward loss:    0.77 ms
[TIME] value loss:    2.24 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1487.78 ms
[TIME] grad clipping:    2.93 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4724.46 ms
[TIME] encode(future):  449.26 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1533.70 ms
[TIME] rollout & consistency (vectorized):  455.56 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 898.13 ms
[TIME-FLAT] Total flat forward: 900.01 ms
[TIME-VEC] _forward_flat: 900.15 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 900.41 ms
[TIME] reward_preds total:  900.50 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  900.71 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1463.72 ms
[TIME] grad clipping:    2.56 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4817.17 ms
[TIME] encode(future):  441.48 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1507.46 ms
[TIME] rollout & consistency (vectorized):  443.00 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 784.08 ms
[TIME-FLAT] Total flat forward: 785.97 ms
[TIME-VEC] _forward_flat: 786.08 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 786.31 ms
[TIME] reward_preds total:  786.43 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  786.60 ms
[TIME] Q preds:    5.03 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.02 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1462.04 ms
[TIME] grad clipping:    2.48 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4654.54 ms
[TIME] encode(future):  436.37 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1498.39 ms
[TIME] rollout & consistency (vectorized):  439.28 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.64 ms
[TIME-FLAT] Gate forward: 0.91 ms
[TIME-FLAT] Softmax + TopK: 0.48 ms
[TIME-FLAT] Sparse Expert Forward: 773.77 ms
[TIME-FLAT] Total flat forward: 775.97 ms
[TIME-VEC] _forward_flat: 776.08 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 776.30 ms
[TIME] reward_preds total:  776.39 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  776.54 ms
[TIME] Q preds:    5.04 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    1.98 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1455.77 ms
[TIME] grad clipping:    1.91 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4620.19 ms
[TIME] encode(future):  452.25 ms
[TIME] compute TD targets:    1.12 ms
[TIME] encode(initial): 1589.45 ms
[TIME] rollout & consistency (vectorized):  440.89 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 789.58 ms
[TIME-FLAT] Total flat forward: 791.39 ms
[TIME-VEC] _forward_flat: 791.50 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 791.73 ms
[TIME] reward_preds total:  791.82 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  792.00 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.02 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1453.30 ms
[TIME] grad clipping:    1.78 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4741.44 ms
[TIME] encode(future):  440.90 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1494.35 ms
[TIME] rollout & consistency (vectorized):  440.37 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 782.60 ms
[TIME-FLAT] Total flat forward: 784.49 ms
[TIME-VEC] _forward_flat: 784.61 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 784.82 ms
[TIME] reward_preds total:  784.91 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  785.08 ms
[TIME] Q preds:    5.03 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    1.99 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1493.91 ms
[TIME] grad clipping:    2.48 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4668.59 ms
[TIME] encode(future):  452.93 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1625.24 ms
[TIME] rollout & consistency (vectorized):  451.96 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.54 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.48 ms
[TIME-FLAT] Sparse Expert Forward: 798.22 ms
[TIME-FLAT] Total flat forward: 800.32 ms
[TIME-VEC] _forward_flat: 800.46 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 800.68 ms
[TIME] reward_preds total:  800.80 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.99 ms
[TIME] Q preds:    5.24 ms
[TIME] reward loss:    0.86 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1507.98 ms
[TIME] grad clipping:    2.57 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4853.61 ms
[TIME] encode(future):  454.54 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1527.75 ms
[TIME] rollout & consistency (vectorized):  454.21 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.88 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 801.84 ms
[TIME-FLAT] Total flat forward: 803.74 ms
[TIME-VEC] _forward_flat: 803.87 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 804.10 ms
[TIME] reward_preds total:  804.20 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  804.38 ms
[TIME] Q preds:    5.25 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1513.92 ms
[TIME] grad clipping:    2.79 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4769.52 ms
[TIME] encode(future):  452.34 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1628.95 ms
[TIME] rollout & consistency (vectorized):  454.48 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.44 ms
[TIME-FLAT] Sparse Expert Forward: 795.71 ms
[TIME-FLAT] Total flat forward: 797.67 ms
[TIME-VEC] _forward_flat: 797.81 ms
[TIME-VEC] Reshape back: 0.10 ms
[TIME-VEC] Total vector reward: 798.06 ms
[TIME] reward_preds total:  798.15 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  798.33 ms
[TIME] Q preds:    5.18 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1514.14 ms
[TIME] grad clipping:    2.70 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4862.74 ms
[TIME] encode(future):  455.07 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1527.72 ms
[TIME] rollout & consistency (vectorized):  452.19 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 795.02 ms
[TIME-FLAT] Total flat forward: 796.82 ms
[TIME-VEC] _forward_flat: 796.96 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 797.21 ms
[TIME] reward_preds total:  797.32 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.47 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.24 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1503.03 ms
[TIME] grad clipping:    2.34 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4749.77 ms
[TIME] encode(future):  452.34 ms
[TIME] compute TD targets:    1.08 ms
[TIME] encode(initial): 1521.58 ms
[TIME] rollout & consistency (vectorized):  453.72 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.89 ms
[TIME-FLAT] Softmax + TopK: 0.37 ms
[TIME-FLAT] Sparse Expert Forward: 893.80 ms
[TIME-FLAT] Total flat forward: 895.78 ms
[TIME-VEC] _forward_flat: 895.90 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 896.13 ms
[TIME] reward_preds total:  896.22 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  896.39 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1506.48 ms
[TIME] grad clipping:    2.36 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4844.75 ms
[TIME] encode(future):  450.97 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1535.98 ms
[TIME] rollout & consistency (vectorized):  454.38 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.92 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 792.30 ms
[TIME-FLAT] Total flat forward: 794.28 ms
[TIME-VEC] _forward_flat: 794.42 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 794.69 ms
[TIME] reward_preds total:  794.81 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  794.99 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    1.00 ms
[TIME] value loss:    4.10 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1510.33 ms
[TIME] grad clipping:    2.66 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4763.30 ms
[TIME] encode(future):  449.94 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1537.43 ms
[TIME] rollout & consistency (vectorized):  452.54 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.91 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 812.01 ms
[TIME-FLAT] Total flat forward: 814.00 ms
[TIME-VEC] _forward_flat: 814.13 ms
[TIME-VEC] Reshape back: 0.10 ms
[TIME-VEC] Total vector reward: 814.38 ms
[TIME] reward_preds total:  814.49 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  814.66 ms
[TIME] Q preds:    5.21 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    3.77 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1498.82 ms
[TIME] grad clipping:    2.80 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4769.80 ms
[TIME] encode(future):  451.56 ms
[TIME] compute TD targets:    1.08 ms
[TIME] encode(initial): 1619.15 ms
[TIME] rollout & consistency (vectorized):  450.55 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 794.87 ms
[TIME-FLAT] Total flat forward: 796.71 ms
[TIME-VEC] _forward_flat: 796.84 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 797.07 ms
[TIME] reward_preds total:  797.15 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.31 ms
[TIME] Q preds:    5.28 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1494.47 ms
[TIME] grad clipping:    3.20 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4828.30 ms
[TIME] encode(future):  446.02 ms
[TIME] compute TD targets:    1.13 ms
[TIME] encode(initial): 1505.31 ms
[TIME] rollout & consistency (vectorized):  449.87 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 780.67 ms
[TIME-FLAT] Total flat forward: 782.57 ms
[TIME-VEC] _forward_flat: 782.71 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 782.96 ms
[TIME] reward_preds total:  783.06 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  783.24 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1493.69 ms
[TIME] grad clipping:    1.89 ms
[TIME] optim.step:    1.16 ms
[TIME] _update total: 4692.35 ms
[TIME] encode(future):  451.19 ms
[TIME] compute TD targets:    1.15 ms
[TIME] encode(initial): 1611.20 ms
[TIME] rollout & consistency (vectorized):  449.93 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.51 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 788.37 ms
[TIME-FLAT] Total flat forward: 790.29 ms
[TIME-VEC] _forward_flat: 790.43 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 790.69 ms
[TIME] reward_preds total:  790.79 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  790.96 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1488.46 ms
[TIME] grad clipping:    2.02 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4805.60 ms
[TIME] encode(future):  447.32 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1518.18 ms
[TIME] rollout & consistency (vectorized):  454.46 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 790.15 ms
[TIME-FLAT] Total flat forward: 792.04 ms
[TIME-VEC] _forward_flat: 792.18 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 792.44 ms
[TIME] reward_preds total:  792.53 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  792.70 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1491.76 ms
[TIME] grad clipping:    2.06 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4718.26 ms
[TIME] encode(future):  449.99 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1605.58 ms
[TIME] rollout & consistency (vectorized):  450.89 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.52 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 794.03 ms
[TIME-FLAT] Total flat forward: 795.96 ms
[TIME-VEC] _forward_flat: 796.08 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 796.31 ms
[TIME] reward_preds total:  796.40 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  796.57 ms
[TIME] Q preds:    5.23 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.22 ms
[TIME] backward: 1494.64 ms
[TIME] grad clipping:    2.04 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4812.10 ms
[TIME] encode(future):  448.31 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1545.26 ms
[TIME] rollout & consistency (vectorized):  443.67 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 782.48 ms
[TIME-FLAT] Total flat forward: 784.25 ms
[TIME-VEC] _forward_flat: 784.35 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 784.56 ms
[TIME] reward_preds total:  784.64 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  784.79 ms
[TIME] Q preds:    4.97 ms
[TIME] reward loss:    0.77 ms
[TIME] value loss:    2.15 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1470.25 ms
[TIME] grad clipping:    2.54 ms
[TIME] optim.step:    1.06 ms
[TIME] _update total: 4706.27 ms
[TIME] encode(future):  439.94 ms
[TIME] compute TD targets:    0.96 ms
[TIME] encode(initial): 1599.88 ms
[TIME] rollout & consistency (vectorized):  440.78 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 783.41 ms
[TIME-FLAT] Total flat forward: 785.18 ms
[TIME-VEC] _forward_flat: 785.29 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 785.52 ms
[TIME] reward_preds total:  785.62 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  785.78 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.15 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1468.85 ms
[TIME] grad clipping:    2.66 ms
[TIME] optim.step:    1.07 ms
[TIME] _update total: 4749.55 ms
[TIME] encode(future):  442.67 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1503.16 ms
[TIME] rollout & consistency (vectorized):  441.02 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 788.11 ms
[TIME-FLAT] Total flat forward: 790.02 ms
[TIME-VEC] _forward_flat: 790.14 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 790.36 ms
[TIME] reward_preds total:  790.47 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  790.67 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1509.11 ms
[TIME] grad clipping:    2.73 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4700.93 ms
[TIME] encode(future):  453.11 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1517.50 ms
[TIME] rollout & consistency (vectorized):  441.91 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 882.23 ms
[TIME-FLAT] Total flat forward: 884.09 ms
[TIME-VEC] _forward_flat: 884.20 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 884.44 ms
[TIME] reward_preds total:  884.55 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  884.73 ms
[TIME] Q preds:    5.06 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1474.86 ms
[TIME] grad clipping:    2.80 ms
[TIME] optim.step:    1.07 ms
[TIME] _update total: 4786.60 ms
[TIME] encode(future):  439.51 ms
[TIME] compute TD targets:    0.98 ms
[TIME] encode(initial): 1509.48 ms
[TIME] rollout & consistency (vectorized):  442.42 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 795.10 ms
[TIME-FLAT] Total flat forward: 796.92 ms
[TIME-VEC] _forward_flat: 797.02 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 797.23 ms
[TIME] reward_preds total:  797.31 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.49 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1476.70 ms
[TIME] grad clipping:    2.45 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4679.59 ms
[TIME] encode(future):  441.20 ms
[TIME] compute TD targets:    0.97 ms
[TIME] encode(initial): 1510.15 ms
[TIME] rollout & consistency (vectorized):  440.54 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 785.85 ms
[TIME-FLAT] Total flat forward: 787.63 ms
[TIME-VEC] _forward_flat: 787.74 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 787.96 ms
[TIME] reward_preds total:  788.05 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  788.21 ms
[TIME] Q preds:    5.04 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.24 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1472.11 ms
[TIME] grad clipping:    2.74 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4666.65 ms
[TIME] encode(future):  444.41 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1598.39 ms
[TIME] rollout & consistency (vectorized):  440.26 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 781.85 ms
[TIME-FLAT] Total flat forward: 783.72 ms
[TIME-VEC] _forward_flat: 783.84 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 784.09 ms
[TIME] reward_preds total:  784.18 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  784.38 ms
[TIME] Q preds:    5.21 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.01 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1468.71 ms
[TIME] grad clipping:    2.51 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4750.29 ms
[TIME] encode(future):  442.99 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1503.97 ms
[TIME] rollout & consistency (vectorized):  442.98 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 778.07 ms
[TIME-FLAT] Total flat forward: 779.92 ms
[TIME-VEC] _forward_flat: 780.03 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 780.27 ms
[TIME] reward_preds total:  780.38 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  780.57 ms
[TIME] Q preds:    5.01 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    1.99 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1467.71 ms
[TIME] grad clipping:    2.64 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4652.33 ms
[TIME] encode(future):  442.32 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1612.12 ms
[TIME] rollout & consistency (vectorized):  452.62 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 797.61 ms
[TIME-FLAT] Total flat forward: 799.42 ms
[TIME-VEC] _forward_flat: 799.54 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 799.78 ms
[TIME] reward_preds total:  799.90 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.06 ms
[TIME] Q preds:    5.15 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.22 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1508.86 ms
[TIME] grad clipping:    2.72 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4830.67 ms
[TIME] encode(future):  444.90 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1513.47 ms
[TIME] rollout & consistency (vectorized):  447.52 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 788.76 ms
[TIME-FLAT] Total flat forward: 790.65 ms
[TIME-VEC] _forward_flat: 790.77 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 791.03 ms
[TIME] reward_preds total:  791.12 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  791.28 ms
[TIME] Q preds:    5.01 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1449.82 ms
[TIME] grad clipping:    2.69 ms
[TIME] optim.step:    1.24 ms
[TIME] _update total: 4662.14 ms
[TIME] encode(future):  435.86 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1596.63 ms
[TIME] rollout & consistency (vectorized):  439.66 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 782.41 ms
[TIME-FLAT] Total flat forward: 784.34 ms
[TIME-VEC] _forward_flat: 784.46 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 784.67 ms
[TIME] reward_preds total:  784.78 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  784.96 ms
[TIME] Q preds:    5.18 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1459.82 ms
[TIME] grad clipping:    2.00 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4731.02 ms
[TIME] encode(future):  440.27 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1494.69 ms
[TIME] rollout & consistency (vectorized):  438.39 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 791.96 ms
[TIME-FLAT] Total flat forward: 793.85 ms
[TIME-VEC] _forward_flat: 793.97 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 794.19 ms
[TIME] reward_preds total:  794.29 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  794.47 ms
[TIME] Q preds:    5.46 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.02 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1463.36 ms
[TIME] grad clipping:    1.97 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4645.18 ms
[TIME] encode(future):  436.67 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1588.96 ms
[TIME] rollout & consistency (vectorized):  440.16 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.80 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 779.54 ms
[TIME-FLAT] Total flat forward: 781.30 ms
[TIME-VEC] _forward_flat: 781.43 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 781.66 ms
[TIME] reward_preds total:  781.75 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  781.93 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1448.14 ms
[TIME] grad clipping:    2.10 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4710.20 ms
[TIME] encode(future):  445.24 ms
[TIME] compute TD targets:    1.13 ms
[TIME] encode(initial): 1531.64 ms
[TIME] rollout & consistency (vectorized):  441.60 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 787.54 ms
[TIME-FLAT] Total flat forward: 789.34 ms
[TIME-VEC] _forward_flat: 789.50 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 789.75 ms
[TIME] reward_preds total:  789.84 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  790.01 ms
[TIME] Q preds:    5.30 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.20 ms
[TIME] backward: 1486.78 ms
[TIME] grad clipping:    2.00 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4710.07 ms
[TIME] encode(future):  446.83 ms
[TIME] compute TD targets:    1.12 ms
[TIME] encode(initial): 1516.38 ms
[TIME] rollout & consistency (vectorized):  451.37 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.98 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 899.73 ms
[TIME-FLAT] Total flat forward: 901.76 ms
[TIME-VEC] _forward_flat: 901.89 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 902.14 ms
[TIME] reward_preds total:  902.24 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  902.43 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1495.51 ms
[TIME] grad clipping:    2.09 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4826.60 ms
[TIME] encode(future):  446.90 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1517.53 ms
[TIME] rollout & consistency (vectorized):  450.00 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.08 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.53 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 784.48 ms
[TIME-FLAT] Total flat forward: 786.41 ms
[TIME-VEC] _forward_flat: 786.54 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 786.80 ms
[TIME] reward_preds total:  786.89 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  787.11 ms
[TIME] Q preds:    5.20 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1489.06 ms
[TIME] grad clipping:    1.88 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4704.43 ms
[TIME] encode(future):  447.63 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1521.97 ms
[TIME] rollout & consistency (vectorized):  454.84 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 793.86 ms
[TIME-FLAT] Total flat forward: 795.68 ms
[TIME-VEC] _forward_flat: 795.80 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 796.03 ms
[TIME] reward_preds total:  796.85 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.07 ms
[TIME] Q preds:    5.32 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1488.96 ms
[TIME] grad clipping:    2.02 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4724.43 ms
[TIME] encode(future):  446.16 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1590.04 ms
[TIME] rollout & consistency (vectorized):  452.48 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.96 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 786.91 ms
[TIME-FLAT] Total flat forward: 788.89 ms
[TIME-VEC] _forward_flat: 789.02 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 789.26 ms
[TIME] reward_preds total:  789.36 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  789.53 ms
[TIME] Q preds:    5.44 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1483.46 ms
[TIME] grad clipping:    1.99 ms
[TIME] optim.step:    1.20 ms
[TIME] _update total: 4775.75 ms
[TIME] encode(future):  446.57 ms
[TIME] compute TD targets:    1.21 ms
[TIME] encode(initial): 1498.14 ms
[TIME] rollout & consistency (vectorized):  447.93 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 781.77 ms
[TIME-FLAT] Total flat forward: 783.63 ms
[TIME-VEC] _forward_flat: 783.75 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 783.96 ms
[TIME] reward_preds total:  784.05 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  784.23 ms
[TIME] Q preds:    5.19 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1489.91 ms
[TIME] grad clipping:    1.90 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4680.70 ms
[TIME] encode(future):  449.42 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1620.55 ms
[TIME] rollout & consistency (vectorized):  452.84 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 795.30 ms
[TIME-FLAT] Total flat forward: 797.20 ms
[TIME-VEC] _forward_flat: 797.32 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 797.56 ms
[TIME] reward_preds total:  797.68 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.86 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1489.54 ms
[TIME] grad clipping:    2.08 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4823.99 ms
[TIME] encode(future):  446.26 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1514.46 ms
[TIME] rollout & consistency (vectorized):  450.88 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.89 ms
[TIME-FLAT] Softmax + TopK: 0.37 ms
[TIME-FLAT] Sparse Expert Forward: 787.07 ms
[TIME-FLAT] Total flat forward: 789.07 ms
[TIME-VEC] _forward_flat: 789.21 ms
[TIME-VEC] Reshape back: 0.10 ms
[TIME-VEC] Total vector reward: 789.46 ms
[TIME] reward_preds total:  789.56 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  789.77 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1483.59 ms
[TIME] grad clipping:    1.98 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4698.87 ms
[TIME] encode(future):  451.50 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1592.87 ms
[TIME] rollout & consistency (vectorized):  447.48 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 791.17 ms
[TIME-FLAT] Total flat forward: 793.05 ms
[TIME-VEC] _forward_flat: 793.18 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 793.39 ms
[TIME] reward_preds total:  793.48 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  793.66 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1486.86 ms
[TIME] grad clipping:    2.02 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4786.30 ms
[TIME] encode(future):  447.77 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1506.49 ms
[TIME] rollout & consistency (vectorized):  448.32 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 786.60 ms
[TIME-FLAT] Total flat forward: 788.45 ms
[TIME-VEC] _forward_flat: 788.59 ms
[TIME-VEC] Reshape back: 0.10 ms
[TIME-VEC] Total vector reward: 788.83 ms
[TIME] reward_preds total:  788.93 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  789.08 ms
[TIME] Q preds:    5.23 ms
[TIME] reward loss:    0.86 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1496.14 ms
[TIME] grad clipping:    2.55 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4702.30 ms
[TIME] encode(future):  449.05 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1629.33 ms
[TIME] rollout & consistency (vectorized):  447.78 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 795.48 ms
[TIME-FLAT] Total flat forward: 797.36 ms
[TIME-VEC] _forward_flat: 797.49 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 797.72 ms
[TIME] reward_preds total:  797.83 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  798.02 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1509.41 ms
[TIME] grad clipping:    2.63 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4847.96 ms
[TIME] encode(future):  458.79 ms
[TIME] compute TD targets:    1.15 ms
[TIME] encode(initial): 1514.75 ms
[TIME] rollout & consistency (vectorized):  444.91 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 792.71 ms
[TIME-FLAT] Total flat forward: 794.63 ms
[TIME-VEC] _forward_flat: 794.77 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 795.01 ms
[TIME] reward_preds total:  795.12 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.30 ms
[TIME] Q preds:    5.15 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1501.13 ms
[TIME] grad clipping:    2.65 ms
[TIME] optim.step:    1.22 ms
[TIME] _update total: 4729.60 ms
[TIME] encode(future):  449.97 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1618.83 ms
[TIME] rollout & consistency (vectorized):  445.17 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.53 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 795.22 ms
[TIME-FLAT] Total flat forward: 797.16 ms
[TIME-VEC] _forward_flat: 797.30 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 797.53 ms
[TIME] reward_preds total:  797.62 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.81 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    1.00 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1506.69 ms
[TIME] grad clipping:    2.60 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4833.18 ms
[TIME] encode(future):  448.38 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1528.76 ms
[TIME] rollout & consistency (vectorized):  452.54 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 791.02 ms
[TIME-FLAT] Total flat forward: 792.84 ms
[TIME-VEC] _forward_flat: 792.95 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 793.15 ms
[TIME] reward_preds total:  793.24 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  793.41 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    3.96 ms
[TIME] loss computation:    0.28 ms
[TIME] backward: 1493.18 ms
[TIME] grad clipping:    2.83 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4732.98 ms
[TIME] encode(future):  450.56 ms
[TIME] compute TD targets:    1.15 ms
[TIME] encode(initial): 1515.31 ms
[TIME] rollout & consistency (vectorized):  448.01 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.43 ms
[TIME-FLAT] Sparse Expert Forward: 892.82 ms
[TIME-FLAT] Total flat forward: 894.73 ms
[TIME-VEC] _forward_flat: 894.86 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 895.10 ms
[TIME] reward_preds total:  895.19 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  895.36 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.28 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1511.36 ms
[TIME] grad clipping:    2.68 ms
[TIME] optim.step:    1.22 ms
[TIME] _update total: 4835.50 ms
[TIME] encode(future):  448.86 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1530.56 ms
[TIME] rollout & consistency (vectorized):  450.61 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 796.70 ms
[TIME-FLAT] Total flat forward: 798.58 ms
[TIME-VEC] _forward_flat: 798.70 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 798.94 ms
[TIME] reward_preds total:  799.04 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  799.22 ms
[TIME] Q preds:    5.30 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1508.03 ms
[TIME] grad clipping:    3.04 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4752.37 ms
[TIME] encode(future):  453.19 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1564.22 ms
[TIME] rollout & consistency (vectorized):  475.08 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.54 ms
[TIME-FLAT] Gate forward: 0.98 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 859.02 ms
[TIME-FLAT] Total flat forward: 861.13 ms
[TIME-VEC] _forward_flat: 861.24 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 861.46 ms
[TIME] reward_preds total:  861.56 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  861.73 ms
[TIME] Q preds:    5.60 ms
[TIME] reward loss:    1.32 ms
[TIME] value loss:    2.17 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1599.16 ms
[TIME] grad clipping:    3.11 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4969.39 ms
[TIME] encode(future):  467.47 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1725.58 ms
[TIME] rollout & consistency (vectorized):  463.84 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.53 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 835.59 ms
[TIME-FLAT] Total flat forward: 837.53 ms
[TIME-VEC] _forward_flat: 837.65 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 837.87 ms
[TIME] reward_preds total:  837.96 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  838.13 ms
[TIME] Q preds:    5.64 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.60 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1621.72 ms
[TIME] grad clipping:    2.98 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 5132.54 ms
[TIME] encode(future):  494.60 ms
[TIME] compute TD targets:    1.34 ms
[TIME] encode(initial): 1575.68 ms
[TIME] rollout & consistency (vectorized):  481.62 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 790.88 ms
[TIME-FLAT] Total flat forward: 792.72 ms
[TIME-VEC] _forward_flat: 792.85 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 793.10 ms
[TIME] reward_preds total:  793.21 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  793.41 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1482.68 ms
[TIME] grad clipping:    2.86 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4843.28 ms
[TIME] encode(future):  451.61 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1611.60 ms
[TIME] rollout & consistency (vectorized):  449.95 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 813.34 ms
[TIME-FLAT] Total flat forward: 815.19 ms
[TIME-VEC] _forward_flat: 815.30 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 815.54 ms
[TIME] reward_preds total:  815.64 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  815.81 ms
[TIME] Q preds:    5.28 ms
[TIME] reward loss:    1.24 ms
[TIME] value loss:    2.58 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1548.26 ms
[TIME] grad clipping:    2.22 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4892.94 ms
[TIME] encode(future):  467.86 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1669.14 ms
[TIME] rollout & consistency (vectorized):  492.57 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 813.43 ms
[TIME-FLAT] Total flat forward: 816.80 ms
[TIME-VEC] _forward_flat: 816.91 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 817.14 ms
[TIME] reward_preds total:  817.23 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  817.40 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1602.85 ms
[TIME] grad clipping:    2.14 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 5063.62 ms
[TIME] encode(future):  486.85 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1624.51 ms
[TIME] rollout & consistency (vectorized):  508.76 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.53 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 914.17 ms
[TIME-FLAT] Total flat forward: 916.12 ms
[TIME-VEC] _forward_flat: 916.27 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 916.52 ms
[TIME] reward_preds total:  916.65 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  916.86 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1668.46 ms
[TIME] grad clipping:    3.09 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 5220.33 ms
[TIME] encode(future):  516.92 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1622.73 ms
[TIME] rollout & consistency (vectorized):  461.50 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 1.16 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 823.19 ms
[TIME-FLAT] Total flat forward: 825.36 ms
[TIME-VEC] _forward_flat: 825.50 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 825.75 ms
[TIME] reward_preds total:  825.86 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  826.03 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1501.09 ms
[TIME] grad clipping:    2.58 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4942.66 ms
[TIME] encode(future):  455.22 ms
[TIME] compute TD targets:    1.08 ms
[TIME] encode(initial): 1581.11 ms
[TIME] rollout & consistency (vectorized):  490.98 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.66 ms
[TIME-FLAT] Gate forward: 0.90 ms
[TIME-FLAT] Softmax + TopK: 0.47 ms
[TIME-FLAT] Sparse Expert Forward: 910.21 ms
[TIME-FLAT] Total flat forward: 912.43 ms
[TIME-VEC] _forward_flat: 912.57 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 912.78 ms
[TIME] reward_preds total:  912.87 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  913.03 ms
[TIME] Q preds:    5.15 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1540.12 ms
[TIME] grad clipping:    2.49 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 4994.81 ms
[TIME] encode(future):  449.32 ms
[TIME] compute TD targets:    0.98 ms
[TIME] encode(initial): 1600.42 ms
[TIME] rollout & consistency (vectorized):  484.48 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 821.78 ms
[TIME-FLAT] Total flat forward: 823.67 ms
[TIME-VEC] _forward_flat: 823.79 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 824.01 ms
[TIME] reward_preds total:  824.10 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  824.26 ms
[TIME] Q preds:    5.23 ms
[TIME] reward loss:    0.89 ms
[TIME] value loss:    2.15 ms
[TIME] loss computation:    0.21 ms
[TIME] backward: 1611.80 ms
[TIME] grad clipping:    2.44 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4984.76 ms
[TIME] encode(future):  489.37 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1581.73 ms
[TIME] rollout & consistency (vectorized):  471.61 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 1.12 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 834.77 ms
[TIME-FLAT] Total flat forward: 836.91 ms
[TIME-VEC] _forward_flat: 837.02 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 837.24 ms
[TIME] reward_preds total:  837.33 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  837.50 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1596.80 ms
[TIME] grad clipping:    4.05 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4992.98 ms
[TIME] encode(future):  477.72 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1649.58 ms
[TIME] rollout & consistency (vectorized):  448.21 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 832.89 ms
[TIME-FLAT] Total flat forward: 834.70 ms
[TIME-VEC] _forward_flat: 834.81 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 835.04 ms
[TIME] reward_preds total:  835.13 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  835.32 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    1.54 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.31 ms
[TIME] backward: 1562.88 ms
[TIME] grad clipping:    2.73 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4989.11 ms
[TIME] encode(future):  467.07 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1581.55 ms
[TIME] rollout & consistency (vectorized):  450.76 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 790.89 ms
[TIME-FLAT] Total flat forward: 792.69 ms
[TIME-VEC] _forward_flat: 792.80 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 793.02 ms
[TIME] reward_preds total:  793.12 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  793.30 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1575.11 ms
[TIME] grad clipping:    4.03 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4883.57 ms
[TIME] encode(future):  508.21 ms
[TIME] compute TD targets:    2.00 ms
[TIME] encode(initial): 1657.15 ms
[TIME] rollout & consistency (vectorized):  445.12 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 830.01 ms
[TIME-FLAT] Total flat forward: 831.87 ms
[TIME-VEC] _forward_flat: 831.98 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 832.19 ms
[TIME] reward_preds total:  832.29 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  832.46 ms
[TIME] Q preds:    5.21 ms
[TIME] reward loss:    0.88 ms
[TIME] value loss:    4.42 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1627.06 ms
[TIME] grad clipping:    2.86 ms
[TIME] optim.step:    1.18 ms
[TIME] _update total: 5088.26 ms
[TIME] encode(future):  459.63 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1599.02 ms
[TIME] rollout & consistency (vectorized):  473.73 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.52 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 823.24 ms
[TIME-FLAT] Total flat forward: 825.14 ms
[TIME-VEC] _forward_flat: 825.25 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 825.47 ms
[TIME] reward_preds total:  825.56 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  825.74 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.19 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1582.81 ms
[TIME] grad clipping:    2.17 ms
[TIME] optim.step:    1.16 ms
[TIME] _update total: 4955.07 ms
[TIME] encode(future):  449.62 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1662.51 ms
[TIME] rollout & consistency (vectorized):  451.63 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.88 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 797.60 ms
[TIME-FLAT] Total flat forward: 799.47 ms
[TIME-VEC] _forward_flat: 799.61 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 799.82 ms
[TIME] reward_preds total:  799.92 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.12 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1541.67 ms
[TIME] grad clipping:    3.07 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4920.37 ms
[TIME] encode(future):  480.31 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1595.71 ms
[TIME] rollout & consistency (vectorized):  482.31 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.37 ms
[TIME-FLAT] Sparse Expert Forward: 809.21 ms
[TIME-FLAT] Total flat forward: 811.12 ms
[TIME-VEC] _forward_flat: 811.28 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 811.50 ms
[TIME] reward_preds total:  811.59 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  811.77 ms
[TIME] Q preds:    5.15 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1555.26 ms
[TIME] grad clipping:    3.13 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4940.87 ms
[TIME] encode(future):  459.06 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1651.33 ms
[TIME] rollout & consistency (vectorized):  447.49 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 783.06 ms
[TIME-FLAT] Total flat forward: 784.90 ms
[TIME-VEC] _forward_flat: 785.01 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 785.22 ms
[TIME] reward_preds total:  785.31 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  785.47 ms
[TIME] Q preds:    5.32 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1554.21 ms
[TIME] grad clipping:    2.06 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 4912.10 ms
[TIME] encode(future):  450.65 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1512.62 ms
[TIME] rollout & consistency (vectorized):  444.76 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 793.63 ms
[TIME-FLAT] Total flat forward: 795.45 ms
[TIME-VEC] _forward_flat: 795.60 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 795.83 ms
[TIME] reward_preds total:  795.92 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  796.09 ms
[TIME] Q preds:    5.21 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1508.49 ms
[TIME] grad clipping:    1.96 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 4726.37 ms
[TIME] encode(future):  437.57 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1586.06 ms
[TIME] rollout & consistency (vectorized):  475.73 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.51 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 901.21 ms
[TIME-FLAT] Total flat forward: 903.15 ms
[TIME-VEC] _forward_flat: 903.28 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 903.52 ms
[TIME] reward_preds total:  903.61 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  903.80 ms
[TIME] Q preds:    5.04 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1588.52 ms
[TIME] grad clipping:    2.23 ms
[TIME] optim.step:    1.54 ms
[TIME] _update total: 5005.89 ms
[TIME] encode(future):  473.76 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1559.33 ms
[TIME] rollout & consistency (vectorized):  480.49 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 814.27 ms
[TIME-FLAT] Total flat forward: 816.15 ms
[TIME-VEC] _forward_flat: 816.27 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 816.54 ms
[TIME] reward_preds total:  816.63 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  816.87 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1549.16 ms
[TIME] grad clipping:    3.71 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4895.10 ms
[TIME] encode(future):  484.54 ms
[TIME] compute TD targets:    1.48 ms
[TIME] encode(initial): 1554.11 ms
[TIME] rollout & consistency (vectorized):  450.12 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 897.46 ms
[TIME-FLAT] Total flat forward: 899.39 ms
[TIME-VEC] _forward_flat: 899.51 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 899.74 ms
[TIME] reward_preds total:  899.85 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  900.04 ms
[TIME] Q preds:    5.26 ms
[TIME] reward loss:    0.88 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1620.47 ms
[TIME] grad clipping:    2.14 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 5024.18 ms
[TIME] encode(future):  475.86 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1599.66 ms
[TIME] rollout & consistency (vectorized):  508.55 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 1.02 ms
[TIME-FLAT] Softmax + TopK: 0.61 ms
[TIME-FLAT] Sparse Expert Forward: 849.24 ms
[TIME-FLAT] Total flat forward: 851.59 ms
[TIME-VEC] _forward_flat: 851.71 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 851.93 ms
[TIME] reward_preds total:  852.02 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  852.20 ms
[TIME] Q preds:    5.42 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    1.14 ms
[TIME] backward: 1550.98 ms
[TIME] grad clipping:    1.97 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 5003.87 ms
[TIME] encode(future):  450.47 ms
[TIME] compute TD targets:    0.98 ms
[TIME] encode(initial): 1526.40 ms
[TIME] rollout & consistency (vectorized):  451.97 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 790.02 ms
[TIME-FLAT] Total flat forward: 791.90 ms
[TIME-VEC] _forward_flat: 792.01 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 792.23 ms
[TIME] reward_preds total:  792.33 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  792.50 ms
[TIME] Q preds:    5.19 ms
[TIME] reward loss:    0.77 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1497.74 ms
[TIME] grad clipping:    2.26 ms
[TIME] optim.step:    1.19 ms
[TIME] _update total: 4733.10 ms
[TIME] encode(future):  436.98 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1603.50 ms
[TIME] rollout & consistency (vectorized):  453.26 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.37 ms
[TIME-FLAT] Sparse Expert Forward: 800.40 ms
[TIME-FLAT] Total flat forward: 802.35 ms
[TIME-VEC] _forward_flat: 802.48 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 802.69 ms
[TIME] reward_preds total:  802.80 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  802.99 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.23 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1495.41 ms
[TIME] grad clipping:    1.87 ms
[TIME] optim.step:    1.19 ms
[TIME] _update total: 4805.89 ms
[TIME] encode(future):  454.10 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1537.36 ms
[TIME] rollout & consistency (vectorized):  450.63 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.53 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 796.54 ms
[TIME-FLAT] Total flat forward: 798.50 ms
[TIME-VEC] _forward_flat: 798.62 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 798.87 ms
[TIME] reward_preds total:  798.99 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  799.17 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.25 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1499.29 ms
[TIME] grad clipping:    2.65 ms
[TIME] optim.step:    1.16 ms
[TIME] _update total: 4755.24 ms
[TIME] encode(future):  452.48 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1635.89 ms
[TIME] rollout & consistency (vectorized):  454.58 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 801.62 ms
[TIME-FLAT] Total flat forward: 803.43 ms
[TIME-VEC] _forward_flat: 803.57 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 803.81 ms
[TIME] reward_preds total:  803.97 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  804.17 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1513.48 ms
[TIME] grad clipping:    2.70 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4875.07 ms
[TIME] encode(future):  452.60 ms
[TIME] compute TD targets:    1.08 ms
[TIME] encode(initial): 1541.53 ms
[TIME] rollout & consistency (vectorized):  449.68 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 795.60 ms
[TIME-FLAT] Total flat forward: 797.45 ms
[TIME-VEC] _forward_flat: 797.58 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 797.79 ms
[TIME] reward_preds total:  797.87 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  798.04 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.27 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1510.65 ms
[TIME] grad clipping:    2.65 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4767.17 ms
[TIME] encode(future):  455.15 ms
[TIME] compute TD targets:    0.98 ms
[TIME] encode(initial): 1621.60 ms
[TIME] rollout & consistency (vectorized):  451.41 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 793.52 ms
[TIME-FLAT] Total flat forward: 795.47 ms
[TIME-VEC] _forward_flat: 795.60 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 795.85 ms
[TIME] reward_preds total:  795.95 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  796.14 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    2.27 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1516.76 ms
[TIME] grad clipping:    2.83 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4855.93 ms
[TIME] encode(future):  467.57 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1563.75 ms
[TIME] rollout & consistency (vectorized):  471.66 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 840.38 ms
[TIME-FLAT] Total flat forward: 842.32 ms
[TIME-VEC] _forward_flat: 842.44 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 842.66 ms
[TIME] reward_preds total:  842.77 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  842.96 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.18 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1700.31 ms
[TIME] grad clipping:    2.82 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 5061.13 ms
[TIME] encode(future):  464.47 ms
[TIME] compute TD targets:    1.09 ms
[TIME] encode(initial): 1603.63 ms
[TIME] rollout & consistency (vectorized):  470.24 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.08 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 906.79 ms
[TIME-FLAT] Total flat forward: 908.66 ms
[TIME-VEC] _forward_flat: 908.79 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 909.04 ms
[TIME] reward_preds total:  909.14 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  909.30 ms
[TIME] Q preds:    5.19 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.18 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1598.21 ms
[TIME] grad clipping:    2.60 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 5060.55 ms
[TIME] encode(future):  477.75 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1594.59 ms
[TIME] rollout & consistency (vectorized):  498.59 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 853.49 ms
[TIME-FLAT] Total flat forward: 855.37 ms
[TIME-VEC] _forward_flat: 855.50 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 855.71 ms
[TIME] reward_preds total:  855.81 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  855.97 ms
[TIME] Q preds:    5.25 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.78 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1635.24 ms
[TIME] grad clipping:    3.43 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 5078.27 ms
[TIME] encode(future):  479.12 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1617.27 ms
[TIME] rollout & consistency (vectorized):  498.43 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 842.90 ms
[TIME-FLAT] Total flat forward: 844.76 ms
[TIME-VEC] _forward_flat: 844.88 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 845.09 ms
[TIME] reward_preds total:  845.19 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  845.35 ms
[TIME] Q preds:    5.22 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1522.40 ms
[TIME] grad clipping:    2.51 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4977.07 ms
[TIME] encode(future):  478.83 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1634.15 ms
[TIME] rollout & consistency (vectorized):  471.25 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 822.20 ms
[TIME-FLAT] Total flat forward: 824.06 ms
[TIME-VEC] _forward_flat: 824.64 ms
[TIME-VEC] Reshape back: 0.29 ms
[TIME-VEC] Total vector reward: 825.10 ms
[TIME] reward_preds total:  825.20 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  825.38 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.27 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1620.61 ms
[TIME] grad clipping:    3.47 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 5045.75 ms
[TIME] encode(future):  495.03 ms
[TIME] compute TD targets:    1.17 ms
[TIME] encode(initial): 1615.62 ms
[TIME] rollout & consistency (vectorized):  472.98 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 796.37 ms
[TIME-FLAT] Total flat forward: 798.21 ms
[TIME-VEC] _forward_flat: 798.32 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 798.52 ms
[TIME] reward_preds total:  798.61 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  798.77 ms
[TIME] Q preds:    4.96 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.01 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1687.76 ms
[TIME] grad clipping:    1.93 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 5084.03 ms
[TIME] encode(future):  482.36 ms
[TIME] compute TD targets:    1.52 ms
[TIME] encode(initial): 1673.86 ms
[TIME] rollout & consistency (vectorized):  491.93 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.09 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 818.46 ms
[TIME-FLAT] Total flat forward: 820.41 ms
[TIME-VEC] _forward_flat: 820.54 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 820.80 ms
[TIME] reward_preds total:  820.90 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  821.09 ms
[TIME] Q preds:    5.19 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1526.69 ms
[TIME] grad clipping:    2.07 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 5010.33 ms
[TIME] encode(future):  446.35 ms
[TIME] compute TD targets:    1.08 ms
[TIME] encode(initial): 1564.21 ms
[TIME] rollout & consistency (vectorized):  459.02 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 820.24 ms
[TIME-FLAT] Total flat forward: 822.09 ms
[TIME-VEC] _forward_flat: 822.22 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 822.46 ms
[TIME] reward_preds total:  822.55 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  822.72 ms
[TIME] Q preds:    6.11 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1619.31 ms
[TIME] grad clipping:    2.13 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4926.60 ms
[TIME] encode(future):  462.64 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1661.16 ms
[TIME] rollout & consistency (vectorized):  490.27 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 811.30 ms
[TIME-FLAT] Total flat forward: 813.21 ms
[TIME-VEC] _forward_flat: 813.33 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 813.56 ms
[TIME] reward_preds total:  813.66 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  813.85 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    3.19 ms
[TIME] loss computation:    0.35 ms
[TIME] backward: 1629.14 ms
[TIME] grad clipping:    1.88 ms
[TIME] optim.step:    1.26 ms
[TIME] _update total: 5072.21 ms
[TIME] encode(future):  454.66 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1578.54 ms
[TIME] rollout & consistency (vectorized):  458.40 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.37 ms
[TIME-FLAT] Sparse Expert Forward: 832.89 ms
[TIME-FLAT] Total flat forward: 834.85 ms
[TIME-VEC] _forward_flat: 835.02 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 835.29 ms
[TIME] reward_preds total:  835.44 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  835.62 ms
[TIME] Q preds:    6.09 ms
[TIME] reward loss:    1.12 ms
[TIME] value loss:    2.36 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1555.67 ms
[TIME] grad clipping:    2.31 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4900.23 ms
[TIME] encode(future):  454.23 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1712.53 ms
[TIME] rollout & consistency (vectorized):  499.86 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 833.18 ms
[TIME-FLAT] Total flat forward: 835.05 ms
[TIME-VEC] _forward_flat: 835.17 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 835.38 ms
[TIME] reward_preds total:  835.48 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  835.65 ms
[TIME] Q preds:    5.23 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1556.25 ms
[TIME] grad clipping:    3.65 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 5074.24 ms
[TIME] encode(future):  452.22 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1555.55 ms
[TIME] rollout & consistency (vectorized):  459.14 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 805.58 ms
[TIME-FLAT] Total flat forward: 807.46 ms
[TIME-VEC] _forward_flat: 807.58 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 807.82 ms
[TIME] reward_preds total:  807.91 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  808.09 ms
[TIME] Q preds:    5.15 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1535.47 ms
[TIME] grad clipping:    2.70 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4825.08 ms
[TIME] encode(future):  491.97 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1629.56 ms
[TIME] rollout & consistency (vectorized):  484.35 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 922.26 ms
[TIME-FLAT] Total flat forward: 924.10 ms
[TIME-VEC] _forward_flat: 924.25 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 924.47 ms
[TIME] reward_preds total:  924.58 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  924.74 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.11 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1572.75 ms
[TIME] grad clipping:    2.75 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 5118.00 ms
[TIME] encode(future):  456.93 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1555.08 ms
[TIME] rollout & consistency (vectorized):  449.70 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 800.91 ms
[TIME-FLAT] Total flat forward: 802.80 ms
[TIME-VEC] _forward_flat: 802.93 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 803.14 ms
[TIME] reward_preds total:  803.23 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  803.40 ms
[TIME] Q preds:    5.42 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1519.88 ms
[TIME] grad clipping:    2.04 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4799.59 ms
[TIME] encode(future):  456.76 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1524.87 ms
[TIME] rollout & consistency (vectorized):  448.47 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 793.16 ms
[TIME-FLAT] Total flat forward: 794.95 ms
[TIME-VEC] _forward_flat: 795.07 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 795.31 ms
[TIME] reward_preds total:  795.40 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.56 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1600.06 ms
[TIME] grad clipping:    1.92 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4839.35 ms
[TIME] encode(future):  500.45 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1628.20 ms
[TIME] rollout & consistency (vectorized):  469.14 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 825.59 ms
[TIME-FLAT] Total flat forward: 827.43 ms
[TIME-VEC] _forward_flat: 827.56 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 827.79 ms
[TIME] reward_preds total:  827.88 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  828.06 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.23 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1557.16 ms
[TIME] grad clipping:    2.23 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4997.20 ms
[TIME] encode(future):  473.17 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1552.70 ms
[TIME] rollout & consistency (vectorized):  491.37 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 807.60 ms
[TIME-FLAT] Total flat forward: 809.42 ms
[TIME-VEC] _forward_flat: 809.55 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 809.77 ms
[TIME] reward_preds total:  809.86 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  810.03 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1543.86 ms
[TIME] grad clipping:    1.84 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4884.93 ms
[TIME] encode(future):  447.48 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1646.84 ms
[TIME] rollout & consistency (vectorized):  491.63 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 826.61 ms
[TIME-FLAT] Total flat forward: 828.46 ms
[TIME-VEC] _forward_flat: 828.58 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 828.80 ms
[TIME] reward_preds total:  828.89 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  829.06 ms
[TIME] Q preds:    5.86 ms
[TIME] reward loss:    1.42 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1585.74 ms
[TIME] grad clipping:    2.04 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 5015.91 ms
[TIME] encode(future):  454.50 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1568.29 ms
[TIME] rollout & consistency (vectorized):  459.45 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 819.81 ms
[TIME-FLAT] Total flat forward: 821.63 ms
[TIME-VEC] _forward_flat: 821.77 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 821.98 ms
[TIME] reward_preds total:  822.16 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  822.32 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.15 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1500.44 ms
[TIME] grad clipping:    2.83 ms
[TIME] optim.step:    1.26 ms
[TIME] _update total: 4819.87 ms
[TIME] encode(future):  458.66 ms
[TIME] compute TD targets:    1.32 ms
[TIME] encode(initial): 1626.74 ms
[TIME] rollout & consistency (vectorized):  454.64 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 797.55 ms
[TIME-FLAT] Total flat forward: 799.48 ms
[TIME-VEC] _forward_flat: 799.62 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 799.84 ms
[TIME] reward_preds total:  799.94 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.18 ms
[TIME] Q preds:    5.20 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1486.21 ms
[TIME] grad clipping:    1.92 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4840.47 ms
[TIME] encode(future):  454.39 ms
[TIME] compute TD targets:    1.14 ms
[TIME] encode(initial): 1552.58 ms
[TIME] rollout & consistency (vectorized):  455.15 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 818.86 ms
[TIME-FLAT] Total flat forward: 820.74 ms
[TIME-VEC] _forward_flat: 820.85 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 821.06 ms
[TIME] reward_preds total:  821.15 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  821.32 ms
[TIME] Q preds:    5.19 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1582.04 ms
[TIME] grad clipping:    2.53 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4879.92 ms
[TIME] encode(future):  487.94 ms
[TIME] compute TD targets:    1.15 ms
[TIME] encode(initial): 1618.72 ms
[TIME] rollout & consistency (vectorized):  451.77 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 793.27 ms
[TIME-FLAT] Total flat forward: 795.13 ms
[TIME-VEC] _forward_flat: 795.23 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 795.45 ms
[TIME] reward_preds total:  795.57 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.73 ms
[TIME] Q preds:    5.02 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.11 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1514.41 ms
[TIME] grad clipping:    1.91 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4882.91 ms
[TIME] encode(future):  474.59 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1515.83 ms
[TIME] rollout & consistency (vectorized):  459.88 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 790.68 ms
[TIME-FLAT] Total flat forward: 792.57 ms
[TIME-VEC] _forward_flat: 792.68 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 792.90 ms
[TIME] reward_preds total:  793.00 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  793.18 ms
[TIME] Q preds:    5.04 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.21 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1501.64 ms
[TIME] grad clipping:    1.96 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4758.92 ms
[TIME] encode(future):  457.48 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1537.76 ms
[TIME] rollout & consistency (vectorized):  464.56 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 899.32 ms
[TIME-FLAT] Total flat forward: 901.15 ms
[TIME-VEC] _forward_flat: 901.28 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 901.50 ms
[TIME] reward_preds total:  901.59 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  901.77 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1489.53 ms
[TIME] grad clipping:    2.03 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4864.82 ms
[TIME] encode(future):  451.79 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1557.62 ms
[TIME] rollout & consistency (vectorized):  462.12 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 797.70 ms
[TIME-FLAT] Total flat forward: 799.53 ms
[TIME-VEC] _forward_flat: 799.65 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 799.86 ms
[TIME] reward_preds total:  799.95 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.10 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1519.05 ms
[TIME] grad clipping:    2.73 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4805.03 ms
[TIME] encode(future):  453.00 ms
[TIME] compute TD targets:    1.13 ms
[TIME] encode(initial): 1527.30 ms
[TIME] rollout & consistency (vectorized):  452.59 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.08 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 798.71 ms
[TIME-FLAT] Total flat forward: 800.58 ms
[TIME-VEC] _forward_flat: 800.72 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 800.94 ms
[TIME] reward_preds total:  801.03 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  801.19 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.42 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1503.84 ms
[TIME] grad clipping:    1.95 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4752.07 ms
[TIME] encode(future):  449.16 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1610.82 ms
[TIME] rollout & consistency (vectorized):  469.51 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 804.69 ms
[TIME-FLAT] Total flat forward: 806.51 ms
[TIME-VEC] _forward_flat: 806.62 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 806.86 ms
[TIME] reward_preds total:  806.95 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  807.86 ms
[TIME] Q preds:    5.52 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1491.62 ms
[TIME] grad clipping:    2.07 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4843.18 ms
[TIME] encode(future):  443.65 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1515.00 ms
[TIME] rollout & consistency (vectorized):  462.28 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 794.78 ms
[TIME-FLAT] Total flat forward: 796.68 ms
[TIME-VEC] _forward_flat: 796.79 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 797.00 ms
[TIME] reward_preds total:  797.09 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.27 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1523.18 ms
[TIME] grad clipping:    2.06 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4755.09 ms
[TIME] encode(future):  470.04 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1644.08 ms
[TIME] rollout & consistency (vectorized):  451.38 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 800.53 ms
[TIME-FLAT] Total flat forward: 802.44 ms
[TIME-VEC] _forward_flat: 802.55 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 802.76 ms
[TIME] reward_preds total:  802.85 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  803.03 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.18 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1516.90 ms
[TIME] grad clipping:    1.99 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4899.76 ms
[TIME] encode(future):  474.92 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1536.56 ms
[TIME] rollout & consistency (vectorized):  447.64 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.89 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 799.39 ms
[TIME-FLAT] Total flat forward: 801.35 ms
[TIME-VEC] _forward_flat: 801.50 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 801.73 ms
[TIME] reward_preds total:  801.82 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  802.03 ms
[TIME] Q preds:    5.28 ms
[TIME] reward loss:    0.91 ms
[TIME] value loss:    2.22 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1465.06 ms
[TIME] grad clipping:    1.86 ms
[TIME] optim.step:    1.07 ms
[TIME] _update total: 4740.37 ms
[TIME] encode(future):  442.59 ms
[TIME] compute TD targets:    0.97 ms
[TIME] encode(initial): 1667.10 ms
[TIME] rollout & consistency (vectorized):  489.26 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 816.55 ms
[TIME-FLAT] Total flat forward: 818.45 ms
[TIME-VEC] _forward_flat: 818.57 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 818.78 ms
[TIME] reward_preds total:  818.87 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  819.05 ms
[TIME] Q preds:    5.30 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.11 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1663.56 ms
[TIME] grad clipping:    2.75 ms
[TIME] optim.step:    1.32 ms
[TIME] _update total: 5096.55 ms
[TIME] encode(future):  457.16 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1586.04 ms
[TIME] rollout & consistency (vectorized):  495.91 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 1.01 ms
[TIME-FLAT] Gate forward: 1.03 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 817.99 ms
[TIME-FLAT] Total flat forward: 820.66 ms
[TIME-VEC] _forward_flat: 820.77 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 820.98 ms
[TIME] reward_preds total:  821.08 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  821.25 ms
[TIME] Q preds:    5.78 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1637.63 ms
[TIME] grad clipping:    2.85 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 5013.48 ms
[TIME] encode(future):  486.63 ms
[TIME] compute TD targets:    1.12 ms
[TIME] encode(initial): 1710.55 ms
[TIME] rollout & consistency (vectorized):  452.71 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 823.90 ms
[TIME-FLAT] Total flat forward: 825.83 ms
[TIME-VEC] _forward_flat: 825.96 ms
[TIME-VEC] Reshape back: 0.10 ms
[TIME-VEC] Total vector reward: 826.22 ms
[TIME] reward_preds total:  826.32 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  826.49 ms
[TIME] Q preds:    5.31 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.19 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1548.04 ms
[TIME] grad clipping:    2.43 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 5039.13 ms
[TIME] encode(future):  469.94 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1606.47 ms
[TIME] rollout & consistency (vectorized):  483.89 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 821.69 ms
[TIME-FLAT] Total flat forward: 823.61 ms
[TIME-VEC] _forward_flat: 823.73 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 823.95 ms
[TIME] reward_preds total:  824.05 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  824.27 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    3.78 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1551.38 ms
[TIME] grad clipping:    2.93 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4952.52 ms
[TIME] encode(future):  450.64 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1635.51 ms
[TIME] rollout & consistency (vectorized):  447.52 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.70 ms
[TIME-FLAT] Gate forward: 0.95 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 839.29 ms
[TIME-FLAT] Total flat forward: 841.50 ms
[TIME-VEC] _forward_flat: 841.63 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 841.86 ms
[TIME] reward_preds total:  842.05 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  842.24 ms
[TIME] Q preds:    5.23 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1539.43 ms
[TIME] grad clipping:    2.62 ms
[TIME] optim.step:    1.22 ms
[TIME] _update total: 4930.02 ms
[TIME] encode(future):  456.73 ms
[TIME] compute TD targets:    1.09 ms
[TIME] encode(initial): 1559.19 ms
[TIME] rollout & consistency (vectorized):  456.86 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 808.37 ms
[TIME-FLAT] Total flat forward: 810.21 ms
[TIME-VEC] _forward_flat: 810.32 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 810.53 ms
[TIME] reward_preds total:  810.62 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  810.79 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    3.58 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1523.21 ms
[TIME] grad clipping:    2.83 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4822.90 ms
[TIME] encode(future):  466.42 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1558.82 ms
[TIME] rollout & consistency (vectorized):  452.97 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.08 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 924.87 ms
[TIME-FLAT] Total flat forward: 926.85 ms
[TIME-VEC] _forward_flat: 926.97 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 927.21 ms
[TIME] reward_preds total:  927.31 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  927.50 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1513.64 ms
[TIME] grad clipping:    2.48 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4933.79 ms
[TIME] encode(future):  449.87 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1573.23 ms
[TIME] rollout & consistency (vectorized):  449.56 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 798.07 ms
[TIME-FLAT] Total flat forward: 799.90 ms
[TIME-VEC] _forward_flat: 800.02 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 800.25 ms
[TIME] reward_preds total:  800.34 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.52 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.88 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1544.16 ms
[TIME] grad clipping:    3.13 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4832.25 ms
[TIME] encode(future):  465.39 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1525.97 ms
[TIME] rollout & consistency (vectorized):  451.08 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.39 ms
[TIME-FLAT] Sparse Expert Forward: 792.33 ms
[TIME-FLAT] Total flat forward: 794.24 ms
[TIME-VEC] _forward_flat: 794.38 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 794.59 ms
[TIME] reward_preds total:  794.69 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  794.85 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1556.37 ms
[TIME] grad clipping:    3.06 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4808.39 ms
[TIME] encode(future):  487.52 ms
[TIME] compute TD targets:    1.42 ms
[TIME] encode(initial): 1639.90 ms
[TIME] rollout & consistency (vectorized):  466.33 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 842.30 ms
[TIME-FLAT] Total flat forward: 844.18 ms
[TIME-VEC] _forward_flat: 844.29 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 844.50 ms
[TIME] reward_preds total:  844.59 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  844.76 ms
[TIME] Q preds:    5.01 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.11 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1490.99 ms
[TIME] grad clipping:    2.09 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4943.98 ms
[TIME] encode(future):  448.79 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1530.93 ms
[TIME] rollout & consistency (vectorized):  450.39 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 792.51 ms
[TIME-FLAT] Total flat forward: 794.40 ms
[TIME-VEC] _forward_flat: 794.53 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 794.76 ms
[TIME] reward_preds total:  794.86 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.03 ms
[TIME] Q preds:    5.24 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1491.40 ms
[TIME] grad clipping:    2.03 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4730.53 ms
[TIME] encode(future):  452.62 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1620.86 ms
[TIME] rollout & consistency (vectorized):  447.69 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 796.87 ms
[TIME-FLAT] Total flat forward: 798.70 ms
[TIME-VEC] _forward_flat: 798.82 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 799.05 ms
[TIME] reward_preds total:  799.14 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  799.30 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1539.64 ms
[TIME] grad clipping:    2.09 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4874.03 ms
[TIME] encode(future):  450.53 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1563.65 ms
[TIME] rollout & consistency (vectorized):  453.95 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 784.92 ms
[TIME-FLAT] Total flat forward: 786.74 ms
[TIME-VEC] _forward_flat: 786.85 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 787.06 ms
[TIME] reward_preds total:  787.15 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  787.30 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1508.52 ms
[TIME] grad clipping:    2.83 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4778.69 ms
[TIME] encode(future):  462.33 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1548.17 ms
[TIME] rollout & consistency (vectorized):  453.53 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.89 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 891.26 ms
[TIME-FLAT] Total flat forward: 897.26 ms
[TIME-VEC] _forward_flat: 897.41 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 897.63 ms
[TIME] reward_preds total:  897.72 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  897.89 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1518.23 ms
[TIME] grad clipping:    2.22 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4894.20 ms
[TIME] encode(future):  458.77 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1572.44 ms
[TIME] rollout & consistency (vectorized):  465.43 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 784.51 ms
[TIME-FLAT] Total flat forward: 786.31 ms
[TIME-VEC] _forward_flat: 787.03 ms
[TIME-VEC] Reshape back: 0.17 ms
[TIME-VEC] Total vector reward: 787.36 ms
[TIME] reward_preds total:  787.46 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  787.63 ms
[TIME] Q preds:    5.04 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1527.74 ms
[TIME] grad clipping:    1.89 ms
[TIME] optim.step:    1.18 ms
[TIME] _update total: 4825.63 ms
[TIME] encode(future):  460.03 ms
[TIME] compute TD targets:    1.47 ms
[TIME] encode(initial): 1574.37 ms
[TIME] rollout & consistency (vectorized):  457.75 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 896.21 ms
[TIME-FLAT] Total flat forward: 898.07 ms
[TIME-VEC] _forward_flat: 898.17 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 898.41 ms
[TIME] reward_preds total:  898.51 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  898.67 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1492.52 ms
[TIME] grad clipping:    1.82 ms
[TIME] optim.step:    1.26 ms
[TIME] _update total: 4897.74 ms
[TIME] encode(future):  449.76 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1526.23 ms
[TIME] rollout & consistency (vectorized):  449.32 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 792.11 ms
[TIME-FLAT] Total flat forward: 793.96 ms
[TIME-VEC] _forward_flat: 794.08 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 794.30 ms
[TIME] reward_preds total:  794.40 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  794.56 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1494.39 ms
[TIME] grad clipping:    2.12 ms
[TIME] optim.step:    1.20 ms
[TIME] _update total: 4728.29 ms
[TIME] encode(future):  452.43 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1570.09 ms
[TIME] rollout & consistency (vectorized):  448.58 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 811.38 ms
[TIME-FLAT] Total flat forward: 813.21 ms
[TIME-VEC] _forward_flat: 813.32 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 813.53 ms
[TIME] reward_preds total:  813.62 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  813.77 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1516.61 ms
[TIME] grad clipping:    2.61 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4815.70 ms
[TIME] encode(future):  458.21 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1640.95 ms
[TIME] rollout & consistency (vectorized):  454.29 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 795.36 ms
[TIME-FLAT] Total flat forward: 797.15 ms
[TIME-VEC] _forward_flat: 797.26 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 797.47 ms
[TIME] reward_preds total:  797.56 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.73 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1497.21 ms
[TIME] grad clipping:    2.60 ms
[TIME] optim.step:    1.20 ms
[TIME] _update total: 4862.83 ms
[TIME] encode(future):  465.57 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1550.51 ms
[TIME] rollout & consistency (vectorized):  454.56 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 791.95 ms
[TIME-FLAT] Total flat forward: 793.86 ms
[TIME-VEC] _forward_flat: 794.00 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 794.21 ms
[TIME] reward_preds total:  794.34 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  794.50 ms
[TIME] Q preds:    5.04 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1547.06 ms
[TIME] grad clipping:    2.07 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4826.44 ms
[TIME] encode(future):  449.41 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1675.88 ms
[TIME] rollout & consistency (vectorized):  449.73 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 794.57 ms
[TIME-FLAT] Total flat forward: 796.47 ms
[TIME-VEC] _forward_flat: 796.59 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 796.81 ms
[TIME] reward_preds total:  796.92 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.10 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.17 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1511.45 ms
[TIME] grad clipping:    2.13 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4897.62 ms
[TIME] encode(future):  475.84 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1540.71 ms
[TIME] rollout & consistency (vectorized):  458.56 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 826.92 ms
[TIME-FLAT] Total flat forward: 828.83 ms
[TIME-VEC] _forward_flat: 828.94 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 829.16 ms
[TIME] reward_preds total:  829.25 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  829.46 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1683.77 ms
[TIME] grad clipping:    2.93 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 5003.32 ms
[TIME] encode(future):  467.07 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1699.95 ms
[TIME] rollout & consistency (vectorized):  502.22 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 825.59 ms
[TIME-FLAT] Total flat forward: 827.47 ms
[TIME-VEC] _forward_flat: 827.58 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 827.80 ms
[TIME] reward_preds total:  827.89 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  828.06 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    3.70 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1555.68 ms
[TIME] grad clipping:    3.44 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 5069.88 ms
[TIME] encode(future):  481.47 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1558.35 ms
[TIME] rollout & consistency (vectorized):  454.85 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 799.07 ms
[TIME-FLAT] Total flat forward: 800.94 ms
[TIME-VEC] _forward_flat: 801.07 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 801.27 ms
[TIME] reward_preds total:  801.36 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  801.56 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    3.80 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1536.15 ms
[TIME] grad clipping:    3.46 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4849.41 ms
[TIME] encode(future):  467.95 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1546.14 ms
[TIME] rollout & consistency (vectorized):  454.39 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.45 ms
[TIME-FLAT] Sparse Expert Forward: 903.03 ms
[TIME-FLAT] Total flat forward: 905.04 ms
[TIME-VEC] _forward_flat: 905.15 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 905.36 ms
[TIME] reward_preds total:  905.46 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  905.65 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1497.77 ms
[TIME] grad clipping:    1.95 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4885.53 ms
[TIME] encode(future):  454.16 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1532.11 ms
[TIME] rollout & consistency (vectorized):  454.74 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 801.67 ms
[TIME-FLAT] Total flat forward: 803.58 ms
[TIME-VEC] _forward_flat: 803.70 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 803.95 ms
[TIME] reward_preds total:  804.05 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  804.26 ms
[TIME] Q preds:    5.18 ms
[TIME] reward loss:    0.99 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1556.00 ms
[TIME] grad clipping:    2.54 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4816.36 ms
[TIME] encode(future):  479.20 ms
[TIME] compute TD targets:    1.20 ms
[TIME] encode(initial): 1539.18 ms
[TIME] rollout & consistency (vectorized):  455.51 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 938.79 ms
[TIME-FLAT] Total flat forward: 940.70 ms
[TIME-VEC] _forward_flat: 940.82 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 941.05 ms
[TIME] reward_preds total:  941.14 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  941.65 ms
[TIME] Q preds:    5.25 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1507.85 ms
[TIME] grad clipping:    1.94 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4937.31 ms
[TIME] encode(future):  453.03 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1564.90 ms
[TIME] rollout & consistency (vectorized):  465.30 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.60 ms
[TIME-FLAT] Sparse Expert Forward: 819.15 ms
[TIME-FLAT] Total flat forward: 821.30 ms
[TIME-VEC] _forward_flat: 821.42 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 821.63 ms
[TIME] reward_preds total:  821.72 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  821.89 ms
[TIME] Q preds:    5.15 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.29 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1539.42 ms
[TIME] grad clipping:    2.14 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4858.75 ms
[TIME] encode(future):  458.48 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1551.82 ms
[TIME] rollout & consistency (vectorized):  449.79 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 812.52 ms
[TIME-FLAT] Total flat forward: 814.42 ms
[TIME-VEC] _forward_flat: 814.54 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 814.79 ms
[TIME] reward_preds total:  814.88 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  815.04 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1516.97 ms
[TIME] grad clipping:    2.27 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4806.19 ms
[TIME] encode(future):  462.48 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1658.04 ms
[TIME] rollout & consistency (vectorized):  459.77 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 809.25 ms
[TIME-FLAT] Total flat forward: 811.11 ms
[TIME-VEC] _forward_flat: 811.24 ms
[TIME-VEC] Reshape back: 0.13 ms
[TIME-VEC] Total vector reward: 811.52 ms
[TIME] reward_preds total:  811.61 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  811.79 ms
[TIME] Q preds:    5.40 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1522.42 ms
[TIME] grad clipping:    2.65 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4929.28 ms
[TIME] encode(future):  480.85 ms
[TIME] compute TD targets:    1.12 ms
[TIME] encode(initial): 1551.01 ms
[TIME] rollout & consistency (vectorized):  457.25 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 807.65 ms
[TIME-FLAT] Total flat forward: 809.52 ms
[TIME-VEC] _forward_flat: 809.68 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 809.90 ms
[TIME] reward_preds total:  810.02 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  810.30 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1497.30 ms
[TIME] grad clipping:    1.92 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4812.17 ms
[TIME] encode(future):  453.79 ms
[TIME] compute TD targets:    1.09 ms
[TIME] encode(initial): 1640.70 ms
[TIME] rollout & consistency (vectorized):  457.52 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 805.19 ms
[TIME-FLAT] Total flat forward: 807.02 ms
[TIME-VEC] _forward_flat: 807.18 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 807.43 ms
[TIME] reward_preds total:  807.52 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  807.68 ms
[TIME] Q preds:    5.33 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.20 ms
[TIME] backward: 1512.10 ms
[TIME] grad clipping:    2.05 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 4886.06 ms
[TIME] encode(future):  454.15 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1545.40 ms
[TIME] rollout & consistency (vectorized):  453.04 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 802.16 ms
[TIME-FLAT] Total flat forward: 804.04 ms
[TIME-VEC] _forward_flat: 804.18 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 804.41 ms
[TIME] reward_preds total:  804.50 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  804.68 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.19 ms
[TIME] backward: 1519.53 ms
[TIME] grad clipping:    1.92 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4790.68 ms
[TIME] encode(future):  458.46 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1649.64 ms
[TIME] rollout & consistency (vectorized):  451.34 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 796.44 ms
[TIME-FLAT] Total flat forward: 798.31 ms
[TIME-VEC] _forward_flat: 798.45 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 798.68 ms
[TIME] reward_preds total:  798.77 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  798.94 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.89 ms
[TIME] value loss:    2.31 ms
[TIME] loss computation:    0.19 ms
[TIME] backward: 1648.91 ms
[TIME] grad clipping:    2.21 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 5021.83 ms
[TIME] encode(future):  486.60 ms
[TIME] compute TD targets:    1.11 ms
[TIME] encode(initial): 1528.04 ms
[TIME] rollout & consistency (vectorized):  490.50 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.95 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 816.02 ms
[TIME-FLAT] Total flat forward: 817.99 ms
[TIME-VEC] _forward_flat: 818.10 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 818.32 ms
[TIME] reward_preds total:  818.41 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  818.61 ms
[TIME] Q preds:    5.76 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.11 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1650.12 ms
[TIME] grad clipping:    2.02 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4989.13 ms
[TIME] encode(future):  455.60 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1548.92 ms
[TIME] rollout & consistency (vectorized):  472.75 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 1.55 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 942.46 ms
[TIME-FLAT] Total flat forward: 945.08 ms
[TIME-VEC] _forward_flat: 945.19 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 945.42 ms
[TIME] reward_preds total:  945.51 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  945.68 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    1.01 ms
[TIME] value loss:    2.11 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1632.75 ms
[TIME] grad clipping:    2.38 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 5070.86 ms
[TIME] encode(future):  452.81 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1535.10 ms
[TIME] rollout & consistency (vectorized):  463.11 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 826.89 ms
[TIME-FLAT] Total flat forward: 828.76 ms
[TIME-VEC] _forward_flat: 828.90 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 829.12 ms
[TIME] reward_preds total:  829.22 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  829.41 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.18 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1534.12 ms
[TIME] grad clipping:    2.03 ms
[TIME] optim.step:    1.16 ms
[TIME] _update total: 4829.04 ms
[TIME] encode(future):  459.23 ms
[TIME] compute TD targets:    2.24 ms
[TIME] encode(initial): 1587.43 ms
[TIME] rollout & consistency (vectorized):  453.25 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.90 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 826.80 ms
[TIME-FLAT] Total flat forward: 828.81 ms
[TIME-VEC] _forward_flat: 828.92 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 829.13 ms
[TIME] reward_preds total:  829.23 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  829.40 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1622.56 ms
[TIME] grad clipping:    2.55 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4967.50 ms
[TIME] encode(future):  450.14 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1672.75 ms
[TIME] rollout & consistency (vectorized):  453.61 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.44 ms
[TIME-FLAT] Sparse Expert Forward: 811.30 ms
[TIME-FLAT] Total flat forward: 813.27 ms
[TIME-VEC] _forward_flat: 813.39 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 813.62 ms
[TIME] reward_preds total:  813.71 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  813.90 ms
[TIME] Q preds:    5.21 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    3.23 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1636.38 ms
[TIME] grad clipping:    3.35 ms
[TIME] optim.step:    2.40 ms
[TIME] _update total: 5044.60 ms
[TIME] encode(future):  463.58 ms
[TIME] compute TD targets:    1.08 ms
[TIME] encode(initial): 1597.77 ms
[TIME] rollout & consistency (vectorized):  485.63 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 801.21 ms
[TIME-FLAT] Total flat forward: 803.14 ms
[TIME-VEC] _forward_flat: 803.26 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 803.49 ms
[TIME] reward_preds total:  803.60 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  803.77 ms
[TIME] Q preds:    5.56 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    3.93 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1513.53 ms
[TIME] grad clipping:    2.50 ms
[TIME] optim.step:    1.53 ms
[TIME] _update total: 4881.36 ms
[TIME] encode(future):  453.68 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1627.47 ms
[TIME] rollout & consistency (vectorized):  452.42 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 833.55 ms
[TIME-FLAT] Total flat forward: 835.46 ms
[TIME-VEC] _forward_flat: 835.57 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 835.78 ms
[TIME] reward_preds total:  835.87 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  836.04 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1557.86 ms
[TIME] grad clipping:    3.03 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4942.42 ms
[TIME] encode(future):  455.94 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1532.51 ms
[TIME] rollout & consistency (vectorized):  450.67 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 827.16 ms
[TIME-FLAT] Total flat forward: 829.05 ms
[TIME-VEC] _forward_flat: 829.17 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 829.38 ms
[TIME] reward_preds total:  829.47 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  829.64 ms
[TIME] Q preds:    6.07 ms
[TIME] reward loss:    1.07 ms
[TIME] value loss:    3.99 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1531.05 ms
[TIME] grad clipping:    2.49 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4817.36 ms
[TIME] encode(future):  484.64 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1667.94 ms
[TIME] rollout & consistency (vectorized):  457.52 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 799.70 ms
[TIME-FLAT] Total flat forward: 801.62 ms
[TIME-VEC] _forward_flat: 801.73 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 801.94 ms
[TIME] reward_preds total:  802.04 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  802.25 ms
[TIME] Q preds:    5.06 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.24 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1543.01 ms
[TIME] grad clipping:    2.88 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4970.21 ms
[TIME] encode(future):  473.87 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1558.10 ms
[TIME] rollout & consistency (vectorized):  495.32 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.61 ms
[TIME-FLAT] Sparse Expert Forward: 835.68 ms
[TIME-FLAT] Total flat forward: 837.84 ms
[TIME-VEC] _forward_flat: 837.95 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 838.17 ms
[TIME] reward_preds total:  838.27 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  838.44 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.86 ms
[TIME] value loss:    3.63 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1534.90 ms
[TIME] grad clipping:    3.12 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4917.20 ms
[TIME] encode(future):  463.82 ms
[TIME] compute TD targets:    1.23 ms
[TIME] encode(initial): 1654.47 ms
[TIME] rollout & consistency (vectorized):  459.26 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 809.92 ms
[TIME-FLAT] Total flat forward: 811.77 ms
[TIME-VEC] _forward_flat: 811.91 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 812.12 ms
[TIME] reward_preds total:  812.28 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  812.83 ms
[TIME] Q preds:    5.25 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1526.07 ms
[TIME] grad clipping:    2.82 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4931.52 ms
[TIME] encode(future):  455.62 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1539.77 ms
[TIME] rollout & consistency (vectorized):  450.69 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 797.61 ms
[TIME-FLAT] Total flat forward: 799.52 ms
[TIME-VEC] _forward_flat: 799.65 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 799.89 ms
[TIME] reward_preds total:  799.99 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.17 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1521.13 ms
[TIME] grad clipping:    2.68 ms
[TIME] optim.step:    1.19 ms
[TIME] _update total: 4781.82 ms
[TIME] encode(future):  452.93 ms
[TIME] compute TD targets:    1.15 ms
[TIME] encode(initial): 1530.03 ms
[TIME] rollout & consistency (vectorized):  452.95 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 888.53 ms
[TIME-FLAT] Total flat forward: 890.36 ms
[TIME-VEC] _forward_flat: 890.49 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 890.72 ms
[TIME] reward_preds total:  890.84 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  890.99 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.11 ms
[TIME] loss computation:    0.20 ms
[TIME] backward: 1480.58 ms
[TIME] grad clipping:    2.31 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4821.95 ms
[TIME] encode(future):  451.20 ms
[TIME] compute TD targets:    0.96 ms
[TIME] encode(initial): 1526.72 ms
[TIME] rollout & consistency (vectorized):  457.09 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 1.15 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 791.86 ms
[TIME-FLAT] Total flat forward: 794.07 ms
[TIME-VEC] _forward_flat: 794.20 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 794.42 ms
[TIME] reward_preds total:  794.51 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  794.67 ms
[TIME] Q preds:    5.22 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1490.56 ms
[TIME] grad clipping:    2.00 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4734.09 ms
[TIME] encode(future):  453.73 ms
[TIME] compute TD targets:    0.98 ms
[TIME] encode(initial): 1530.69 ms
[TIME] rollout & consistency (vectorized):  453.46 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 797.28 ms
[TIME-FLAT] Total flat forward: 799.18 ms
[TIME-VEC] _forward_flat: 799.31 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 799.55 ms
[TIME] reward_preds total:  799.64 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  799.83 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.38 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1499.58 ms
[TIME] grad clipping:    2.00 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4751.42 ms
[TIME] encode(future):  455.11 ms
[TIME] compute TD targets:    1.13 ms
[TIME] encode(initial): 1613.97 ms
[TIME] rollout & consistency (vectorized):  457.43 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 790.05 ms
[TIME-FLAT] Total flat forward: 791.92 ms
[TIME-VEC] _forward_flat: 792.05 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 792.29 ms
[TIME] reward_preds total:  792.40 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  792.57 ms
[TIME] Q preds:    5.03 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    2.19 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1500.90 ms
[TIME] grad clipping:    2.03 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4834.06 ms
[TIME] encode(future):  450.09 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1526.78 ms
[TIME] rollout & consistency (vectorized):  458.00 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 797.07 ms
[TIME-FLAT] Total flat forward: 798.98 ms
[TIME-VEC] _forward_flat: 799.09 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 799.30 ms
[TIME] reward_preds total:  799.39 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  799.55 ms
[TIME] Q preds:    5.24 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.24 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1506.11 ms
[TIME] grad clipping:    2.00 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4754.70 ms
[TIME] encode(future):  449.84 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1615.76 ms
[TIME] rollout & consistency (vectorized):  446.40 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 784.52 ms
[TIME-FLAT] Total flat forward: 786.31 ms
[TIME-VEC] _forward_flat: 786.42 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 786.63 ms
[TIME] reward_preds total:  786.72 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  786.91 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.01 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1464.39 ms
[TIME] grad clipping:    1.95 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4776.81 ms
[TIME] encode(future):  440.08 ms
[TIME] compute TD targets:    1.15 ms
[TIME] encode(initial): 1526.36 ms
[TIME] rollout & consistency (vectorized):  444.95 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 786.04 ms
[TIME-FLAT] Total flat forward: 787.96 ms
[TIME-VEC] _forward_flat: 788.07 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 788.32 ms
[TIME] reward_preds total:  788.41 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  788.59 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.91 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1498.32 ms
[TIME] grad clipping:    2.15 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4712.46 ms
[TIME] encode(future):  456.79 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1642.60 ms
[TIME] rollout & consistency (vectorized):  452.16 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 798.14 ms
[TIME-FLAT] Total flat forward: 800.00 ms
[TIME-VEC] _forward_flat: 800.16 ms
[TIME-VEC] Reshape back: 0.10 ms
[TIME-VEC] Total vector reward: 800.41 ms
[TIME] reward_preds total:  800.51 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.68 ms
[TIME] Q preds:    5.28 ms
[TIME] reward loss:    1.03 ms
[TIME] value loss:    2.16 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1513.22 ms
[TIME] grad clipping:    2.27 ms
[TIME] optim.step:    1.19 ms
[TIME] _update total: 4880.23 ms
[TIME] encode(future):  453.37 ms
[TIME] compute TD targets:    1.08 ms
[TIME] encode(initial): 1518.58 ms
[TIME] rollout & consistency (vectorized):  442.18 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 791.58 ms
[TIME-FLAT] Total flat forward: 793.50 ms
[TIME-VEC] _forward_flat: 793.65 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 793.88 ms
[TIME] reward_preds total:  793.98 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  794.19 ms
[TIME] Q preds:    5.21 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1469.83 ms
[TIME] grad clipping:    1.92 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4692.62 ms
[TIME] encode(future):  440.76 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1607.22 ms
[TIME] rollout & consistency (vectorized):  439.59 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 786.37 ms
[TIME-FLAT] Total flat forward: 788.26 ms
[TIME-VEC] _forward_flat: 788.38 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 788.63 ms
[TIME] reward_preds total:  788.73 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  788.92 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.95 ms
[TIME] value loss:    2.02 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1461.07 ms
[TIME] grad clipping:    1.94 ms
[TIME] optim.step:    1.26 ms
[TIME] _update total: 4751.41 ms
[TIME] encode(future):  449.39 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1545.04 ms
[TIME] rollout & consistency (vectorized):  458.85 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 797.35 ms
[TIME-FLAT] Total flat forward: 799.23 ms
[TIME-VEC] _forward_flat: 799.34 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 799.56 ms
[TIME] reward_preds total:  799.66 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  799.85 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1518.04 ms
[TIME] grad clipping:    2.54 ms
[TIME] optim.step:    1.26 ms
[TIME] _update total: 4785.64 ms
[TIME] encode(future):  454.36 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1630.64 ms
[TIME] rollout & consistency (vectorized):  457.29 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.08 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 801.93 ms
[TIME-FLAT] Total flat forward: 803.70 ms
[TIME-VEC] _forward_flat: 803.84 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 804.08 ms
[TIME] reward_preds total:  804.18 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  804.36 ms
[TIME] Q preds:    5.27 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1509.44 ms
[TIME] grad clipping:    2.46 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4870.59 ms
[TIME] encode(future):  456.95 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1534.51 ms
[TIME] rollout & consistency (vectorized):  456.32 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 795.42 ms
[TIME-FLAT] Total flat forward: 797.33 ms
[TIME-VEC] _forward_flat: 797.45 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 797.66 ms
[TIME] reward_preds total:  797.75 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.90 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1517.52 ms
[TIME] grad clipping:    2.69 ms
[TIME] optim.step:    1.16 ms
[TIME] _update total: 4777.54 ms
[TIME] encode(future):  454.85 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1639.70 ms
[TIME] rollout & consistency (vectorized):  456.49 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 801.76 ms
[TIME-FLAT] Total flat forward: 803.63 ms
[TIME-VEC] _forward_flat: 803.75 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 804.00 ms
[TIME] reward_preds total:  804.11 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  804.28 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.26 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1521.04 ms
[TIME] grad clipping:    2.58 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 4891.00 ms
[TIME] encode(future):  448.98 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1522.04 ms
[TIME] rollout & consistency (vectorized):  451.63 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 851.24 ms
[TIME-FLAT] Total flat forward: 853.14 ms
[TIME-VEC] _forward_flat: 853.27 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 853.51 ms
[TIME] reward_preds total:  853.62 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  853.79 ms
[TIME] Q preds:    5.83 ms
[TIME] reward loss:    1.24 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1672.32 ms
[TIME] grad clipping:    3.16 ms
[TIME] optim.step:    1.75 ms
[TIME] _update total: 4966.20 ms
[TIME] encode(future):  508.65 ms
[TIME] compute TD targets:    1.49 ms
[TIME] encode(initial): 1596.03 ms
[TIME] rollout & consistency (vectorized):  493.05 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.67 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 884.38 ms
[TIME-FLAT] Total flat forward: 886.45 ms
[TIME-VEC] _forward_flat: 886.57 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 886.79 ms
[TIME] reward_preds total:  886.89 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  887.04 ms
[TIME] Q preds:    5.02 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1472.81 ms
[TIME] grad clipping:    1.83 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4971.57 ms
[TIME] encode(future):  442.15 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1607.73 ms
[TIME] rollout & consistency (vectorized):  479.18 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 824.77 ms
[TIME-FLAT] Total flat forward: 826.59 ms
[TIME-VEC] _forward_flat: 826.72 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 826.96 ms
[TIME] reward_preds total:  827.06 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  827.22 ms
[TIME] Q preds:    4.99 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1594.54 ms
[TIME] grad clipping:    2.19 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4964.71 ms
[TIME] encode(future):  509.07 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1591.19 ms
[TIME] rollout & consistency (vectorized):  502.86 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 812.88 ms
[TIME-FLAT] Total flat forward: 814.78 ms
[TIME-VEC] _forward_flat: 814.90 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 815.12 ms
[TIME] reward_preds total:  815.22 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  815.39 ms
[TIME] Q preds:    5.80 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1641.62 ms
[TIME] grad clipping:    2.67 ms
[TIME] optim.step:    1.23 ms
[TIME] _update total: 5077.42 ms
[TIME] encode(future):  504.47 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1649.32 ms
[TIME] rollout & consistency (vectorized):  512.59 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 843.83 ms
[TIME-FLAT] Total flat forward: 845.70 ms
[TIME-VEC] _forward_flat: 845.82 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 846.03 ms
[TIME] reward_preds total:  846.13 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  846.29 ms
[TIME] Q preds:    5.04 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1645.56 ms
[TIME] grad clipping:    2.83 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 5172.78 ms
[TIME] encode(future):  508.15 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1631.29 ms
[TIME] rollout & consistency (vectorized):  455.81 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.92 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 806.76 ms
[TIME-FLAT] Total flat forward: 808.71 ms
[TIME-VEC] _forward_flat: 808.84 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 809.06 ms
[TIME] reward_preds total:  809.16 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  809.33 ms
[TIME] Q preds:    5.82 ms
[TIME] reward loss:    0.86 ms
[TIME] value loss:    3.14 ms
[TIME] loss computation:    0.26 ms
[TIME] backward: 1631.82 ms
[TIME] grad clipping:    3.15 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 5053.25 ms
[TIME] encode(future):  483.32 ms
[TIME] compute TD targets:    1.39 ms
[TIME] encode(initial): 1747.52 ms
[TIME] rollout & consistency (vectorized):  514.32 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 833.83 ms
[TIME-FLAT] Total flat forward: 835.69 ms
[TIME-VEC] _forward_flat: 835.81 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 836.04 ms
[TIME] reward_preds total:  836.13 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  836.29 ms
[TIME] Q preds:    5.18 ms
[TIME] reward loss:    1.20 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1666.96 ms
[TIME] grad clipping:    2.86 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 5263.89 ms
[TIME] encode(future):  493.16 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1574.86 ms
[TIME] rollout & consistency (vectorized):  470.77 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.58 ms
[TIME-FLAT] Gate forward: 1.11 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 868.79 ms
[TIME-FLAT] Total flat forward: 871.05 ms
[TIME-VEC] _forward_flat: 871.17 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 871.39 ms
[TIME] reward_preds total:  871.48 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  871.65 ms
[TIME] Q preds:    5.72 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.70 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1651.31 ms
[TIME] grad clipping:    2.65 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 5077.37 ms
[TIME] encode(future):  467.08 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1713.00 ms
[TIME] rollout & consistency (vectorized):  497.80 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 803.95 ms
[TIME-FLAT] Total flat forward: 805.83 ms
[TIME-VEC] _forward_flat: 805.93 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 806.15 ms
[TIME] reward_preds total:  806.24 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  806.42 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1627.00 ms
[TIME] grad clipping:    3.40 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 5126.41 ms
[TIME] encode(future):  506.96 ms
[TIME] compute TD targets:    1.84 ms
[TIME] encode(initial): 1638.66 ms
[TIME] rollout & consistency (vectorized):  496.62 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.97 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 838.28 ms
[TIME-FLAT] Total flat forward: 840.28 ms
[TIME-VEC] _forward_flat: 840.40 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 840.62 ms
[TIME] reward_preds total:  840.72 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  840.89 ms
[TIME] Q preds:    5.50 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.35 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1622.83 ms
[TIME] grad clipping:    2.93 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 5122.13 ms
[TIME] encode(future):  478.78 ms
[TIME] compute TD targets:    1.28 ms
[TIME] encode(initial): 1732.56 ms
[TIME] rollout & consistency (vectorized):  486.75 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 814.23 ms
[TIME-FLAT] Total flat forward: 816.08 ms
[TIME-VEC] _forward_flat: 816.22 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 816.44 ms
[TIME] reward_preds total:  816.53 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  816.69 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.15 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1546.64 ms
[TIME] grad clipping:    2.75 ms
[TIME] optim.step:    1.63 ms
[TIME] _update total: 5076.88 ms
[TIME] encode(future):  487.46 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1547.99 ms
[TIME] rollout & consistency (vectorized):  500.73 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 814.06 ms
[TIME-FLAT] Total flat forward: 815.89 ms
[TIME-VEC] _forward_flat: 816.01 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 816.24 ms
[TIME] reward_preds total:  816.34 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  816.67 ms
[TIME] Q preds:    5.19 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1561.37 ms
[TIME] grad clipping:    2.74 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4928.83 ms
[TIME] encode(future):  452.99 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1650.39 ms
[TIME] rollout & consistency (vectorized):  460.89 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.75 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 814.44 ms
[TIME-FLAT] Total flat forward: 816.61 ms
[TIME-VEC] _forward_flat: 816.73 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 816.96 ms
[TIME] reward_preds total:  817.06 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  817.22 ms
[TIME] Q preds:    5.40 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.15 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1498.00 ms
[TIME] grad clipping:    3.04 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4895.42 ms
[TIME] encode(future):  451.58 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1573.05 ms
[TIME] rollout & consistency (vectorized):  456.05 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 796.50 ms
[TIME-FLAT] Total flat forward: 798.42 ms
[TIME-VEC] _forward_flat: 798.54 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 798.75 ms
[TIME] reward_preds total:  798.84 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  799.02 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.90 ms
[TIME] value loss:    2.11 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1541.80 ms
[TIME] grad clipping:    2.13 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4835.47 ms
[TIME] encode(future):  447.12 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1625.78 ms
[TIME] rollout & consistency (vectorized):  479.50 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 826.78 ms
[TIME-FLAT] Total flat forward: 828.67 ms
[TIME-VEC] _forward_flat: 828.78 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 829.03 ms
[TIME] reward_preds total:  829.27 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  829.53 ms
[TIME] Q preds:    5.47 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1558.47 ms
[TIME] grad clipping:    2.00 ms
[TIME] optim.step:    1.26 ms
[TIME] _update total: 4955.10 ms
[TIME] encode(future):  450.31 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1580.66 ms
[TIME] rollout & consistency (vectorized):  487.83 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 1.12 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 816.98 ms
[TIME-FLAT] Total flat forward: 819.16 ms
[TIME-VEC] _forward_flat: 819.27 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 819.51 ms
[TIME] reward_preds total:  819.60 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  819.77 ms
[TIME] Q preds:    5.87 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    3.11 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1606.98 ms
[TIME] grad clipping:    1.96 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4961.13 ms
[TIME] encode(future):  454.16 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1650.52 ms
[TIME] rollout & consistency (vectorized):  492.45 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 1.16 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 833.63 ms
[TIME-FLAT] Total flat forward: 835.82 ms
[TIME-VEC] _forward_flat: 835.93 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 836.14 ms
[TIME] reward_preds total:  836.23 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  836.41 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.01 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1539.63 ms
[TIME] grad clipping:    2.05 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4987.38 ms
[TIME] encode(future):  451.83 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1592.69 ms
[TIME] rollout & consistency (vectorized):  470.09 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.90 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 834.44 ms
[TIME-FLAT] Total flat forward: 836.38 ms
[TIME-VEC] _forward_flat: 836.49 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 836.70 ms
[TIME] reward_preds total:  836.79 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  836.95 ms
[TIME] Q preds:    5.04 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1542.84 ms
[TIME] grad clipping:    2.04 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4908.08 ms
[TIME] encode(future):  449.33 ms
[TIME] compute TD targets:    1.52 ms
[TIME] encode(initial): 1590.49 ms
[TIME] rollout & consistency (vectorized):  480.00 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 918.15 ms
[TIME-FLAT] Total flat forward: 920.01 ms
[TIME-VEC] _forward_flat: 920.12 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 920.35 ms
[TIME] reward_preds total:  920.43 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  920.61 ms
[TIME] Q preds:    5.31 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1569.37 ms
[TIME] grad clipping:    2.23 ms
[TIME] optim.step:    1.73 ms
[TIME] _update total: 5025.58 ms
[TIME] encode(future):  462.24 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1592.35 ms
[TIME] rollout & consistency (vectorized):  489.49 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.44 ms
[TIME-FLAT] Sparse Expert Forward: 816.31 ms
[TIME-FLAT] Total flat forward: 818.30 ms
[TIME-VEC] _forward_flat: 818.42 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 818.64 ms
[TIME] reward_preds total:  818.73 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  818.89 ms
[TIME] Q preds:    5.20 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1572.02 ms
[TIME] grad clipping:    2.62 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4949.57 ms
[TIME] encode(future):  452.20 ms
[TIME] compute TD targets:    1.09 ms
[TIME] encode(initial): 1567.98 ms
[TIME] rollout & consistency (vectorized):  456.87 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 927.36 ms
[TIME-FLAT] Total flat forward: 933.03 ms
[TIME-VEC] _forward_flat: 933.16 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 933.39 ms
[TIME] reward_preds total:  933.80 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  933.98 ms
[TIME] Q preds:    5.18 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1558.97 ms
[TIME] grad clipping:    2.37 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4984.36 ms
[TIME] encode(future):  450.70 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1554.85 ms
[TIME] rollout & consistency (vectorized):  456.04 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 798.15 ms
[TIME-FLAT] Total flat forward: 800.00 ms
[TIME-VEC] _forward_flat: 800.11 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 800.32 ms
[TIME] reward_preds total:  800.41 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.58 ms
[TIME] Q preds:    5.01 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1496.99 ms
[TIME] grad clipping:    1.76 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4772.61 ms
[TIME] encode(future):  456.39 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1553.95 ms
[TIME] rollout & consistency (vectorized):  463.60 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 816.75 ms
[TIME-FLAT] Total flat forward: 818.63 ms
[TIME-VEC] _forward_flat: 818.76 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 818.99 ms
[TIME] reward_preds total:  819.08 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  819.28 ms
[TIME] Q preds:    5.37 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    3.31 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1547.57 ms
[TIME] grad clipping:    3.04 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4857.19 ms
[TIME] encode(future):  451.22 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1629.26 ms
[TIME] rollout & consistency (vectorized):  491.00 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.65 ms
[TIME-FLAT] Sparse Expert Forward: 793.95 ms
[TIME-FLAT] Total flat forward: 796.14 ms
[TIME-VEC] _forward_flat: 796.25 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 796.46 ms
[TIME] reward_preds total:  796.55 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  796.73 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.17 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1556.57 ms
[TIME] grad clipping:    2.17 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4938.62 ms
[TIME] encode(future):  492.07 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1565.48 ms
[TIME] rollout & consistency (vectorized):  474.18 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 1.14 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 830.79 ms
[TIME-FLAT] Total flat forward: 832.96 ms
[TIME-VEC] _forward_flat: 833.07 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 833.28 ms
[TIME] reward_preds total:  833.37 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  833.53 ms
[TIME] Q preds:    5.23 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1555.03 ms
[TIME] grad clipping:    2.52 ms
[TIME] optim.step:    1.21 ms
[TIME] _update total: 4935.35 ms
[TIME] encode(future):  459.56 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1676.97 ms
[TIME] rollout & consistency (vectorized):  452.28 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.37 ms
[TIME-FLAT] Sparse Expert Forward: 789.06 ms
[TIME-FLAT] Total flat forward: 790.99 ms
[TIME-VEC] _forward_flat: 791.11 ms
[TIME-VEC] Reshape back: 0.10 ms
[TIME-VEC] Total vector reward: 791.36 ms
[TIME] reward_preds total:  791.46 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  791.67 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.00 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1564.18 ms
[TIME] grad clipping:    2.07 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4958.32 ms
[TIME] encode(future):  475.21 ms
[TIME] compute TD targets:    1.09 ms
[TIME] encode(initial): 1576.97 ms
[TIME] rollout & consistency (vectorized):  452.35 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 1.00 ms
[TIME-FLAT] Softmax + TopK: 0.68 ms
[TIME-FLAT] Sparse Expert Forward: 818.98 ms
[TIME-FLAT] Total flat forward: 821.40 ms
[TIME-VEC] _forward_flat: 821.53 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 821.75 ms
[TIME] reward_preds total:  821.85 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  822.03 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.11 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1578.62 ms
[TIME] grad clipping:    2.43 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4919.47 ms
[TIME] encode(future):  477.97 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1676.67 ms
[TIME] rollout & consistency (vectorized):  487.34 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.61 ms
[TIME-FLAT] Sparse Expert Forward: 807.09 ms
[TIME-FLAT] Total flat forward: 809.23 ms
[TIME-VEC] _forward_flat: 809.34 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 809.54 ms
[TIME] reward_preds total:  809.63 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  809.81 ms
[TIME] Q preds:    5.04 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1526.15 ms
[TIME] grad clipping:    2.58 ms
[TIME] optim.step:    1.19 ms
[TIME] _update total: 4992.24 ms
[TIME] encode(future):  459.80 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1549.25 ms
[TIME] rollout & consistency (vectorized):  454.49 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.89 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 811.31 ms
[TIME-FLAT] Total flat forward: 813.30 ms
[TIME-VEC] _forward_flat: 813.45 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 813.67 ms
[TIME] reward_preds total:  813.76 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  813.95 ms
[TIME] Q preds:    7.01 ms
[TIME] reward loss:    0.90 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1560.98 ms
[TIME] grad clipping:    2.68 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4855.04 ms
[TIME] encode(future):  450.15 ms
[TIME] compute TD targets:    1.16 ms
[TIME] encode(initial): 1538.27 ms
[TIME] rollout & consistency (vectorized):  453.24 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 897.63 ms
[TIME-FLAT] Total flat forward: 899.46 ms
[TIME-VEC] _forward_flat: 899.58 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 899.82 ms
[TIME] reward_preds total:  899.92 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  900.10 ms
[TIME] Q preds:    4.99 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    3.78 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1523.01 ms
[TIME] grad clipping:    2.77 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4880.96 ms
[TIME] encode(future):  458.40 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1518.09 ms
[TIME] rollout & consistency (vectorized):  444.54 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 796.59 ms
[TIME-FLAT] Total flat forward: 798.37 ms
[TIME-VEC] _forward_flat: 798.49 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 798.72 ms
[TIME] reward_preds total:  798.83 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  798.99 ms
[TIME] Q preds:    5.22 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.15 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1497.89 ms
[TIME] grad clipping:    2.24 ms
[TIME] optim.step:    1.16 ms
[TIME] _update total: 4732.12 ms
[TIME] encode(future):  451.86 ms
[TIME] compute TD targets:    1.12 ms
[TIME] encode(initial): 1526.69 ms
[TIME] rollout & consistency (vectorized):  452.77 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 780.55 ms
[TIME-FLAT] Total flat forward: 782.42 ms
[TIME-VEC] _forward_flat: 782.54 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 782.77 ms
[TIME] reward_preds total:  782.89 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  783.06 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.11 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1490.85 ms
[TIME] grad clipping:    2.19 ms
[TIME] optim.step:    1.16 ms
[TIME] _update total: 4719.99 ms
[TIME] encode(future):  466.72 ms
[TIME] compute TD targets:    1.09 ms
[TIME] encode(initial): 1605.77 ms
[TIME] rollout & consistency (vectorized):  451.77 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 787.23 ms
[TIME-FLAT] Total flat forward: 789.05 ms
[TIME-VEC] _forward_flat: 789.17 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 789.40 ms
[TIME] reward_preds total:  789.48 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  789.68 ms
[TIME] Q preds:    5.23 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1490.68 ms
[TIME] grad clipping:    1.97 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4818.45 ms
[TIME] encode(future):  444.23 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1505.18 ms
[TIME] rollout & consistency (vectorized):  451.52 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.88 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 787.30 ms
[TIME-FLAT] Total flat forward: 789.22 ms
[TIME-VEC] _forward_flat: 789.35 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 789.58 ms
[TIME] reward_preds total:  789.67 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  789.83 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.26 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1495.48 ms
[TIME] grad clipping:    2.16 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4700.35 ms
[TIME] encode(future):  445.82 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1612.70 ms
[TIME] rollout & consistency (vectorized):  451.68 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.92 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 795.12 ms
[TIME-FLAT] Total flat forward: 797.05 ms
[TIME-VEC] _forward_flat: 797.19 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 797.41 ms
[TIME] reward_preds total:  797.53 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.70 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1498.49 ms
[TIME] grad clipping:    2.13 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4820.26 ms
[TIME] encode(future):  454.92 ms
[TIME] compute TD targets:    1.11 ms
[TIME] encode(initial): 1546.78 ms
[TIME] rollout & consistency (vectorized):  454.69 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.52 ms
[TIME-FLAT] Gate forward: 0.98 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 802.45 ms
[TIME-FLAT] Total flat forward: 804.50 ms
[TIME-VEC] _forward_flat: 804.64 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 804.87 ms
[TIME] reward_preds total:  804.96 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  805.12 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1506.92 ms
[TIME] grad clipping:    2.61 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4782.82 ms
[TIME] encode(future):  451.75 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1631.58 ms
[TIME] rollout & consistency (vectorized):  448.12 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 797.84 ms
[TIME-FLAT] Total flat forward: 799.69 ms
[TIME-VEC] _forward_flat: 799.81 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 800.05 ms
[TIME] reward_preds total:  800.14 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.32 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1509.23 ms
[TIME] grad clipping:    2.49 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4855.37 ms
[TIME] encode(future):  448.60 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1542.48 ms
[TIME] rollout & consistency (vectorized):  453.45 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.35 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.72 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 800.30 ms
[TIME-FLAT] Total flat forward: 802.51 ms
[TIME-VEC] _forward_flat: 802.66 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 803.22 ms
[TIME] reward_preds total:  803.63 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  808.17 ms
[TIME] Q preds:    6.21 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1520.28 ms
[TIME] grad clipping:    2.38 ms
[TIME] optim.step:    1.28 ms
[TIME] _update total: 4788.69 ms
[TIME] encode(future):  451.53 ms
[TIME] compute TD targets:    1.13 ms
[TIME] encode(initial): 1618.97 ms
[TIME] rollout & consistency (vectorized):  450.31 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.88 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 787.40 ms
[TIME-FLAT] Total flat forward: 789.37 ms
[TIME-VEC] _forward_flat: 789.49 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 789.75 ms
[TIME] reward_preds total:  789.85 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  790.06 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.22 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1496.85 ms
[TIME] grad clipping:    1.93 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4822.15 ms
[TIME] encode(future):  448.77 ms
[TIME] compute TD targets:    1.12 ms
[TIME] encode(initial): 1525.80 ms
[TIME] rollout & consistency (vectorized):  452.23 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 787.99 ms
[TIME-FLAT] Total flat forward: 789.79 ms
[TIME-VEC] _forward_flat: 789.97 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 790.17 ms
[TIME] reward_preds total:  790.28 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  790.44 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1491.53 ms
[TIME] grad clipping:    2.02 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4722.94 ms
[TIME] encode(future):  454.65 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1528.04 ms
[TIME] rollout & consistency (vectorized):  452.09 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 886.28 ms
[TIME-FLAT] Total flat forward: 888.10 ms
[TIME-VEC] _forward_flat: 888.21 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 888.43 ms
[TIME] reward_preds total:  888.54 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  888.70 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.19 ms
[TIME] backward: 1491.75 ms
[TIME] grad clipping:    2.02 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4829.10 ms
[TIME] encode(future):  454.78 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1541.21 ms
[TIME] rollout & consistency (vectorized):  451.57 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.51 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 788.70 ms
[TIME-FLAT] Total flat forward: 790.65 ms
[TIME-VEC] _forward_flat: 790.77 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 791.02 ms
[TIME] reward_preds total:  791.15 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  791.34 ms
[TIME] Q preds:    5.06 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1505.71 ms
[TIME] grad clipping:    2.67 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4758.97 ms
[TIME] encode(future):  453.22 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1518.01 ms
[TIME] rollout & consistency (vectorized):  449.65 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.95 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 789.58 ms
[TIME-FLAT] Total flat forward: 791.60 ms
[TIME-VEC] _forward_flat: 791.70 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 791.91 ms
[TIME] reward_preds total:  792.02 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  792.18 ms
[TIME] Q preds:    5.02 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1504.83 ms
[TIME] grad clipping:    2.66 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4732.37 ms
[TIME] encode(future):  451.19 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1633.45 ms
[TIME] rollout & consistency (vectorized):  451.59 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 796.45 ms
[TIME-FLAT] Total flat forward: 798.40 ms
[TIME-VEC] _forward_flat: 798.51 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 798.76 ms
[TIME] reward_preds total:  798.86 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  799.03 ms
[TIME] Q preds:    5.32 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1511.10 ms
[TIME] grad clipping:    2.87 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4861.15 ms
[TIME] encode(future):  455.21 ms
[TIME] compute TD targets:    1.22 ms
[TIME] encode(initial): 1576.77 ms
[TIME] rollout & consistency (vectorized):  451.35 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 793.48 ms
[TIME-FLAT] Total flat forward: 795.33 ms
[TIME-VEC] _forward_flat: 795.46 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 795.68 ms
[TIME] reward_preds total:  795.78 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.95 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.23 ms
[TIME] backward: 1502.02 ms
[TIME] grad clipping:    2.73 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4796.08 ms
[TIME] encode(future):  456.73 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1648.72 ms
[TIME] rollout & consistency (vectorized):  453.03 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 790.45 ms
[TIME-FLAT] Total flat forward: 792.32 ms
[TIME-VEC] _forward_flat: 792.44 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 792.67 ms
[TIME] reward_preds total:  792.76 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  792.91 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.27 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1509.38 ms
[TIME] grad clipping:    2.76 ms
[TIME] optim.step:    1.19 ms
[TIME] _update total: 4875.60 ms
[TIME] encode(future):  457.82 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1517.29 ms
[TIME] rollout & consistency (vectorized):  452.74 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 791.20 ms
[TIME-FLAT] Total flat forward: 793.00 ms
[TIME-VEC] _forward_flat: 793.14 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 793.38 ms
[TIME] reward_preds total:  793.50 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  793.65 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1509.40 ms
[TIME] grad clipping:    2.89 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4745.50 ms
[TIME] encode(future):  453.58 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1631.67 ms
[TIME] rollout & consistency (vectorized):  454.11 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 799.99 ms
[TIME-FLAT] Total flat forward: 801.86 ms
[TIME-VEC] _forward_flat: 801.97 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 802.18 ms
[TIME] reward_preds total:  802.28 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  802.45 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    3.67 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1500.93 ms
[TIME] grad clipping:    2.88 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4858.94 ms
[TIME] encode(future):  452.73 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1531.90 ms
[TIME] rollout & consistency (vectorized):  452.29 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 797.58 ms
[TIME-FLAT] Total flat forward: 799.52 ms
[TIME-VEC] _forward_flat: 799.65 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 799.87 ms
[TIME] reward_preds total:  799.98 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.17 ms
[TIME] Q preds:    5.24 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1522.70 ms
[TIME] grad clipping:    2.86 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4774.41 ms
[TIME] encode(future):  449.50 ms
[TIME] compute TD targets:    1.36 ms
[TIME] encode(initial): 1610.24 ms
[TIME] rollout & consistency (vectorized):  450.53 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.52 ms
[TIME-FLAT] Gate forward: 0.92 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 792.30 ms
[TIME-FLAT] Total flat forward: 794.32 ms
[TIME-VEC] _forward_flat: 794.44 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 794.68 ms
[TIME] reward_preds total:  795.06 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.26 ms
[TIME] Q preds:    5.28 ms
[TIME] reward loss:    0.92 ms
[TIME] value loss:    2.15 ms
[TIME] loss computation:    0.19 ms
[TIME] backward: 1505.30 ms
[TIME] grad clipping:    2.72 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4826.04 ms
[TIME] encode(future):  450.44 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1524.35 ms
[TIME] rollout & consistency (vectorized):  448.65 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 779.69 ms
[TIME-FLAT] Total flat forward: 781.55 ms
[TIME-VEC] _forward_flat: 781.66 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 781.89 ms
[TIME] reward_preds total:  782.01 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  782.18 ms
[TIME] Q preds:    5.02 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.15 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1484.42 ms
[TIME] grad clipping:    2.11 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4704.45 ms
[TIME] encode(future):  454.10 ms
[TIME] compute TD targets:    1.19 ms
[TIME] encode(initial): 1539.75 ms
[TIME] rollout & consistency (vectorized):  457.85 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.91 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 904.11 ms
[TIME-FLAT] Total flat forward: 905.97 ms
[TIME-VEC] _forward_flat: 906.10 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 906.33 ms
[TIME] reward_preds total:  906.45 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  906.61 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1487.58 ms
[TIME] grad clipping:    2.49 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4860.14 ms
[TIME] encode(future):  443.59 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1502.39 ms
[TIME] rollout & consistency (vectorized):  446.86 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 807.38 ms
[TIME-FLAT] Total flat forward: 809.28 ms
[TIME-VEC] _forward_flat: 809.39 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 809.61 ms
[TIME] reward_preds total:  809.70 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  809.87 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1474.63 ms
[TIME] grad clipping:    1.79 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4690.82 ms
[TIME] encode(future):  453.36 ms
[TIME] compute TD targets:    1.08 ms
[TIME] encode(initial): 1539.80 ms
[TIME] rollout & consistency (vectorized):  445.61 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 785.35 ms
[TIME-FLAT] Total flat forward: 787.22 ms
[TIME-VEC] _forward_flat: 787.34 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 787.57 ms
[TIME] reward_preds total:  787.66 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  787.84 ms
[TIME] Q preds:    5.18 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1567.46 ms
[TIME] grad clipping:    2.01 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4807.95 ms
[TIME] encode(future):  453.27 ms
[TIME] compute TD targets:    1.17 ms
[TIME] encode(initial): 1615.32 ms
[TIME] rollout & consistency (vectorized):  461.89 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 2.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 787.30 ms
[TIME-FLAT] Total flat forward: 791.16 ms
[TIME-VEC] _forward_flat: 791.27 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 791.48 ms
[TIME] reward_preds total:  791.58 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  791.75 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.20 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1459.20 ms
[TIME] grad clipping:    1.83 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4798.08 ms
[TIME] encode(future):  461.86 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1521.48 ms
[TIME] rollout & consistency (vectorized):  440.82 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 779.00 ms
[TIME-FLAT] Total flat forward: 780.81 ms
[TIME-VEC] _forward_flat: 780.92 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 781.13 ms
[TIME] reward_preds total:  781.22 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  781.39 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.18 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1528.68 ms
[TIME] grad clipping:    1.88 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4747.93 ms
[TIME] encode(future):  436.85 ms
[TIME] compute TD targets:    0.98 ms
[TIME] encode(initial): 1711.58 ms
[TIME] rollout & consistency (vectorized):  486.96 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 832.68 ms
[TIME-FLAT] Total flat forward: 834.51 ms
[TIME-VEC] _forward_flat: 834.63 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 834.84 ms
[TIME] reward_preds total:  834.93 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  835.09 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.95 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1555.80 ms
[TIME] grad clipping:    1.91 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 5040.80 ms
[TIME] encode(future):  472.59 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1590.32 ms
[TIME] rollout & consistency (vectorized):  465.78 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 820.42 ms
[TIME-FLAT] Total flat forward: 822.22 ms
[TIME-VEC] _forward_flat: 822.34 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 822.56 ms
[TIME] reward_preds total:  822.65 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  822.83 ms
[TIME] Q preds:    5.52 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1560.89 ms
[TIME] grad clipping:    1.95 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4926.56 ms
[TIME] encode(future):  473.35 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1665.46 ms
[TIME] rollout & consistency (vectorized):  484.18 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.08 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.55 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 817.50 ms
[TIME-FLAT] Total flat forward: 819.46 ms
[TIME-VEC] _forward_flat: 819.57 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 819.80 ms
[TIME] reward_preds total:  819.92 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  820.09 ms
[TIME] Q preds:    5.20 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.16 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1580.55 ms
[TIME] grad clipping:    1.99 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 5037.49 ms
[TIME] encode(future):  465.88 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1582.38 ms
[TIME] rollout & consistency (vectorized):  454.42 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 802.73 ms
[TIME-FLAT] Total flat forward: 804.58 ms
[TIME-VEC] _forward_flat: 804.69 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 804.90 ms
[TIME] reward_preds total:  804.99 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  805.17 ms
[TIME] Q preds:    5.44 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1609.09 ms
[TIME] grad clipping:    4.41 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4933.65 ms
[TIME] encode(future):  467.25 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1715.74 ms
[TIME] rollout & consistency (vectorized):  483.15 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 809.38 ms
[TIME-FLAT] Total flat forward: 811.24 ms
[TIME-VEC] _forward_flat: 811.37 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 811.59 ms
[TIME] reward_preds total:  811.69 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  811.86 ms
[TIME] Q preds:    5.02 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.16 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1545.70 ms
[TIME] grad clipping:    2.71 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 5038.12 ms
[TIME] encode(future):  472.76 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1550.95 ms
[TIME] rollout & consistency (vectorized):  447.16 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 819.41 ms
[TIME-FLAT] Total flat forward: 821.21 ms
[TIME-VEC] _forward_flat: 821.33 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 821.56 ms
[TIME] reward_preds total:  821.65 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  821.81 ms
[TIME] Q preds:    5.15 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    3.78 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1633.06 ms
[TIME] grad clipping:    6.76 ms
[TIME] optim.step:    1.21 ms
[TIME] _update total: 4946.18 ms
[TIME] encode(future):  503.29 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1727.71 ms
[TIME] rollout & consistency (vectorized):  490.20 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 1.05 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 822.86 ms
[TIME-FLAT] Total flat forward: 825.00 ms
[TIME-VEC] _forward_flat: 825.11 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 825.33 ms
[TIME] reward_preds total:  825.43 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  825.60 ms
[TIME] Q preds:    5.19 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1551.76 ms
[TIME] grad clipping:    4.07 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 5114.62 ms
[TIME] encode(future):  455.88 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1570.92 ms
[TIME] rollout & consistency (vectorized):  451.83 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 793.52 ms
[TIME-FLAT] Total flat forward: 795.40 ms
[TIME-VEC] _forward_flat: 795.55 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 795.77 ms
[TIME] reward_preds total:  795.86 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  796.03 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.31 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1500.88 ms
[TIME] grad clipping:    2.46 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4790.02 ms
[TIME] encode(future):  447.63 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1535.47 ms
[TIME] rollout & consistency (vectorized):  451.46 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 908.85 ms
[TIME-FLAT] Total flat forward: 910.63 ms
[TIME-VEC] _forward_flat: 910.75 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 910.96 ms
[TIME] reward_preds total:  911.05 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  911.21 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    3.57 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1482.93 ms
[TIME] grad clipping:    2.89 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4844.84 ms
[TIME] encode(future):  445.40 ms
[TIME] compute TD targets:    0.97 ms
[TIME] encode(initial): 1567.39 ms
[TIME] rollout & consistency (vectorized):  456.07 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 804.34 ms
[TIME-FLAT] Total flat forward: 806.24 ms
[TIME-VEC] _forward_flat: 806.37 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 806.59 ms
[TIME] reward_preds total:  806.68 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  806.84 ms
[TIME] Q preds:    5.22 ms
[TIME] reward loss:    0.97 ms
[TIME] value loss:    2.15 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1555.45 ms
[TIME] grad clipping:    2.85 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4846.10 ms
[TIME] encode(future):  458.21 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1537.09 ms
[TIME] rollout & consistency (vectorized):  448.94 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 820.63 ms
[TIME-FLAT] Total flat forward: 822.53 ms
[TIME-VEC] _forward_flat: 822.63 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 822.85 ms
[TIME] reward_preds total:  822.95 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  823.13 ms
[TIME] Q preds:    5.22 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1516.13 ms
[TIME] grad clipping:    2.47 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4797.80 ms
[TIME] encode(future):  449.59 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1649.09 ms
[TIME] rollout & consistency (vectorized):  449.87 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 801.57 ms
[TIME-FLAT] Total flat forward: 803.50 ms
[TIME-VEC] _forward_flat: 803.62 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 803.85 ms
[TIME] reward_preds total:  803.96 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  804.14 ms
[TIME] Q preds:    5.26 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.18 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1494.84 ms
[TIME] grad clipping:    2.94 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4862.43 ms
[TIME] encode(future):  448.66 ms
[TIME] compute TD targets:    1.09 ms
[TIME] encode(initial): 1540.28 ms
[TIME] rollout & consistency (vectorized):  455.57 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 824.21 ms
[TIME-FLAT] Total flat forward: 826.03 ms
[TIME-VEC] _forward_flat: 826.13 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 826.34 ms
[TIME] reward_preds total:  826.44 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  826.62 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1518.37 ms
[TIME] grad clipping:    2.78 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4804.12 ms
[TIME] encode(future):  451.63 ms
[TIME] compute TD targets:    1.08 ms
[TIME] encode(initial): 1648.10 ms
[TIME] rollout & consistency (vectorized):  451.05 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 790.54 ms
[TIME-FLAT] Total flat forward: 792.35 ms
[TIME-VEC] _forward_flat: 792.48 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 792.70 ms
[TIME] reward_preds total:  792.79 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  792.98 ms
[TIME] Q preds:    5.34 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1486.46 ms
[TIME] grad clipping:    2.87 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4845.44 ms
[TIME] encode(future):  440.57 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1561.77 ms
[TIME] rollout & consistency (vectorized):  446.85 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.95 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 813.20 ms
[TIME-FLAT] Total flat forward: 815.14 ms
[TIME-VEC] _forward_flat: 815.25 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 815.47 ms
[TIME] reward_preds total:  815.55 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  815.71 ms
[TIME] Q preds:    6.17 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.26 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1501.07 ms
[TIME] grad clipping:    1.89 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4781.38 ms
[TIME] encode(future):  457.60 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1549.46 ms
[TIME] rollout & consistency (vectorized):  448.67 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 907.38 ms
[TIME-FLAT] Total flat forward: 909.22 ms
[TIME-VEC] _forward_flat: 909.33 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 909.54 ms
[TIME] reward_preds total:  909.62 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  909.78 ms
[TIME] Q preds:    6.05 ms
[TIME] reward loss:    1.05 ms
[TIME] value loss:    2.17 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1513.88 ms
[TIME] grad clipping:    2.02 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 4894.60 ms
[TIME] encode(future):  466.69 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1549.05 ms
[TIME] rollout & consistency (vectorized):  455.30 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 794.01 ms
[TIME-FLAT] Total flat forward: 795.84 ms
[TIME-VEC] _forward_flat: 795.97 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 796.21 ms
[TIME] reward_preds total:  796.30 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  796.46 ms
[TIME] Q preds:    5.19 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1490.98 ms
[TIME] grad clipping:    2.16 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4772.93 ms
[TIME] encode(future):  452.58 ms
[TIME] compute TD targets:    1.09 ms
[TIME] encode(initial): 1564.14 ms
[TIME] rollout & consistency (vectorized):  453.95 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 907.84 ms
[TIME-FLAT] Total flat forward: 909.74 ms
[TIME-VEC] _forward_flat: 909.87 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 910.13 ms
[TIME] reward_preds total:  910.24 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  910.42 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.89 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1620.10 ms
[TIME] grad clipping:    2.96 ms
[TIME] optim.step:    1.18 ms
[TIME] _update total: 5016.36 ms
[TIME] encode(future):  449.57 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1539.82 ms
[TIME] rollout & consistency (vectorized):  452.01 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 797.19 ms
[TIME-FLAT] Total flat forward: 799.07 ms
[TIME-VEC] _forward_flat: 799.23 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 799.45 ms
[TIME] reward_preds total:  799.55 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  799.72 ms
[TIME] Q preds:    5.27 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.19 ms
[TIME] backward: 1533.29 ms
[TIME] grad clipping:    2.46 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 4789.07 ms
[TIME] encode(future):  453.82 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1555.80 ms
[TIME] rollout & consistency (vectorized):  459.82 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.80 ms
[TIME-FLAT] Sparse Expert Forward: 805.70 ms
[TIME-FLAT] Total flat forward: 808.03 ms
[TIME-VEC] _forward_flat: 808.15 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 808.38 ms
[TIME] reward_preds total:  808.50 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  808.68 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1507.83 ms
[TIME] grad clipping:    2.49 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4800.11 ms
[TIME] encode(future):  473.30 ms
[TIME] compute TD targets:    2.09 ms
[TIME] encode(initial): 1637.73 ms
[TIME] rollout & consistency (vectorized):  450.91 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 800.33 ms
[TIME-FLAT] Total flat forward: 802.24 ms
[TIME-VEC] _forward_flat: 802.40 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 802.62 ms
[TIME] reward_preds total:  802.71 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  802.89 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.99 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.19 ms
[TIME] backward: 1543.12 ms
[TIME] grad clipping:    2.83 ms
[TIME] optim.step:    1.22 ms
[TIME] _update total: 4924.04 ms
[TIME] encode(future):  454.02 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1569.10 ms
[TIME] rollout & consistency (vectorized):  477.04 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.52 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 800.90 ms
[TIME-FLAT] Total flat forward: 802.81 ms
[TIME-VEC] _forward_flat: 802.96 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 803.17 ms
[TIME] reward_preds total:  803.27 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  803.43 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1534.56 ms
[TIME] grad clipping:    2.63 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4852.47 ms
[TIME] encode(future):  483.37 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1667.78 ms
[TIME] rollout & consistency (vectorized):  449.43 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 820.10 ms
[TIME-FLAT] Total flat forward: 821.97 ms
[TIME-VEC] _forward_flat: 822.08 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 822.31 ms
[TIME] reward_preds total:  822.40 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  822.60 ms
[TIME] Q preds:    5.37 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1574.61 ms
[TIME] grad clipping:    2.63 ms
[TIME] optim.step:    1.21 ms
[TIME] _update total: 5012.64 ms
[TIME] encode(future):  465.88 ms
[TIME] compute TD targets:    1.09 ms
[TIME] encode(initial): 1549.51 ms
[TIME] rollout & consistency (vectorized):  483.23 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 803.05 ms
[TIME-FLAT] Total flat forward: 804.91 ms
[TIME-VEC] _forward_flat: 805.04 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 805.28 ms
[TIME] reward_preds total:  805.37 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  805.54 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    2.54 ms
[TIME] loss computation:    0.19 ms
[TIME] backward: 1530.85 ms
[TIME] grad clipping:    3.69 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4851.19 ms
[TIME] encode(future):  453.53 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1694.31 ms
[TIME] rollout & consistency (vectorized):  466.57 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 810.87 ms
[TIME-FLAT] Total flat forward: 812.74 ms
[TIME-VEC] _forward_flat: 812.86 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 813.08 ms
[TIME] reward_preds total:  813.18 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  813.35 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1536.43 ms
[TIME] grad clipping:    2.98 ms
[TIME] optim.step:    1.25 ms
[TIME] _update total: 4979.02 ms
[TIME] encode(future):  485.77 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1529.93 ms
[TIME] rollout & consistency (vectorized):  449.89 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 794.58 ms
[TIME-FLAT] Total flat forward: 796.41 ms
[TIME-VEC] _forward_flat: 796.52 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 796.73 ms
[TIME] reward_preds total:  797.27 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.46 ms
[TIME] Q preds:    5.20 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1500.79 ms
[TIME] grad clipping:    2.67 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4778.30 ms
[TIME] encode(future):  452.98 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1631.33 ms
[TIME] rollout & consistency (vectorized):  453.88 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 797.37 ms
[TIME-FLAT] Total flat forward: 799.21 ms
[TIME-VEC] _forward_flat: 799.33 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 799.53 ms
[TIME] reward_preds total:  799.63 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  799.81 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.19 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1572.03 ms
[TIME] grad clipping:    2.16 ms
[TIME] optim.step:    1.18 ms
[TIME] _update total: 4924.54 ms
[TIME] encode(future):  449.79 ms
[TIME] compute TD targets:    1.12 ms
[TIME] encode(initial): 1536.65 ms
[TIME] rollout & consistency (vectorized):  452.49 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 785.32 ms
[TIME-FLAT] Total flat forward: 787.20 ms
[TIME-VEC] _forward_flat: 787.32 ms
[TIME-VEC] Reshape back: 0.10 ms
[TIME-VEC] Total vector reward: 787.58 ms
[TIME] reward_preds total:  787.67 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  787.87 ms
[TIME] Q preds:    5.96 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.35 ms
[TIME] loss computation:    0.30 ms
[TIME] backward: 1502.50 ms
[TIME] grad clipping:    2.12 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4744.52 ms
[TIME] encode(future):  446.62 ms
[TIME] compute TD targets:    1.34 ms
[TIME] encode(initial): 1538.93 ms
[TIME] rollout & consistency (vectorized):  452.16 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 892.90 ms
[TIME-FLAT] Total flat forward: 894.81 ms
[TIME-VEC] _forward_flat: 894.94 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 895.15 ms
[TIME] reward_preds total:  895.26 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  895.46 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1486.03 ms
[TIME] grad clipping:    1.95 ms
[TIME] optim.step:    1.34 ms
[TIME] _update total: 4834.05 ms
[TIME] encode(future):  454.35 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1528.82 ms
[TIME] rollout & consistency (vectorized):  452.38 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.67 ms
[TIME-FLAT] Gate forward: 1.60 ms
[TIME-FLAT] Softmax + TopK: 0.63 ms
[TIME-FLAT] Sparse Expert Forward: 794.21 ms
[TIME-FLAT] Total flat forward: 797.46 ms
[TIME-VEC] _forward_flat: 797.63 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 797.86 ms
[TIME] reward_preds total:  797.95 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  798.12 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1509.72 ms
[TIME] grad clipping:    2.67 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4757.95 ms
[TIME] encode(future):  453.98 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1550.29 ms
[TIME] rollout & consistency (vectorized):  456.24 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 899.00 ms
[TIME-FLAT] Total flat forward: 900.86 ms
[TIME-VEC] _forward_flat: 900.98 ms
[TIME-VEC] Reshape back: 0.10 ms
[TIME-VEC] Total vector reward: 901.24 ms
[TIME] reward_preds total:  901.34 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  901.52 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1509.59 ms
[TIME] grad clipping:    2.68 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4886.15 ms
[TIME] encode(future):  454.09 ms
[TIME] compute TD targets:    1.08 ms
[TIME] encode(initial): 1529.42 ms
[TIME] rollout & consistency (vectorized):  455.86 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 790.66 ms
[TIME-FLAT] Total flat forward: 792.54 ms
[TIME-VEC] _forward_flat: 792.68 ms
[TIME-VEC] Reshape back: 0.14 ms
[TIME-VEC] Total vector reward: 793.62 ms
[TIME] reward_preds total:  793.74 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  793.90 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1492.19 ms
[TIME] grad clipping:    2.50 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4739.84 ms
[TIME] encode(future):  448.48 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1526.01 ms
[TIME] rollout & consistency (vectorized):  467.86 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 1.02 ms
[TIME-FLAT] Gate forward: 1.00 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 797.62 ms
[TIME-FLAT] Total flat forward: 800.17 ms
[TIME-VEC] _forward_flat: 800.29 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 800.50 ms
[TIME] reward_preds total:  800.75 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.92 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1486.55 ms
[TIME] grad clipping:    1.90 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4743.51 ms
[TIME] encode(future):  447.45 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1629.20 ms
[TIME] rollout & consistency (vectorized):  451.71 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 807.47 ms
[TIME-FLAT] Total flat forward: 809.31 ms
[TIME-VEC] _forward_flat: 809.43 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 809.67 ms
[TIME] reward_preds total:  809.76 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  809.92 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1493.60 ms
[TIME] grad clipping:    1.93 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4845.49 ms
[TIME] encode(future):  460.55 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1572.11 ms
[TIME] rollout & consistency (vectorized):  451.42 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 788.63 ms
[TIME-FLAT] Total flat forward: 790.47 ms
[TIME-VEC] _forward_flat: 790.60 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 790.85 ms
[TIME] reward_preds total:  790.96 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  791.12 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1488.21 ms
[TIME] grad clipping:    1.93 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4777.54 ms
[TIME] encode(future):  446.10 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1616.99 ms
[TIME] rollout & consistency (vectorized):  455.40 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.01 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 779.92 ms
[TIME-FLAT] Total flat forward: 781.82 ms
[TIME-VEC] _forward_flat: 781.93 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 782.14 ms
[TIME] reward_preds total:  782.28 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  782.47 ms
[TIME] Q preds:    5.04 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.00 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1513.65 ms
[TIME] grad clipping:    1.74 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4827.91 ms
[TIME] encode(future):  462.33 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1551.25 ms
[TIME] rollout & consistency (vectorized):  449.53 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.61 ms
[TIME-FLAT] Gate forward: 0.88 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 797.60 ms
[TIME-FLAT] Total flat forward: 799.69 ms
[TIME-VEC] _forward_flat: 799.84 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 800.08 ms
[TIME] reward_preds total:  800.18 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.35 ms
[TIME] Q preds:    5.67 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.34 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1504.14 ms
[TIME] grad clipping:    1.86 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4781.98 ms
[TIME] encode(future):  453.27 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1647.84 ms
[TIME] rollout & consistency (vectorized):  456.80 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 811.89 ms
[TIME-FLAT] Total flat forward: 813.80 ms
[TIME-VEC] _forward_flat: 813.93 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 814.17 ms
[TIME] reward_preds total:  814.28 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  814.48 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1493.79 ms
[TIME] grad clipping:    2.74 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4880.78 ms
[TIME] encode(future):  464.26 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1558.92 ms
[TIME] rollout & consistency (vectorized):  466.77 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 787.24 ms
[TIME-FLAT] Total flat forward: 789.09 ms
[TIME-VEC] _forward_flat: 789.22 ms
[TIME-VEC] Reshape back: 0.10 ms
[TIME-VEC] Total vector reward: 789.49 ms
[TIME] reward_preds total:  789.63 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  789.83 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1564.44 ms
[TIME] grad clipping:    1.91 ms
[TIME] optim.step:    1.36 ms
[TIME] _update total: 4858.18 ms
[TIME] encode(future):  455.93 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1585.34 ms
[TIME] rollout & consistency (vectorized):  489.58 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 2.02 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 914.11 ms
[TIME-FLAT] Total flat forward: 917.12 ms
[TIME-VEC] _forward_flat: 917.23 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 917.44 ms
[TIME] reward_preds total:  917.54 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  917.69 ms
[TIME] Q preds:    6.18 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1578.69 ms
[TIME] grad clipping:    2.14 ms
[TIME] optim.step:    1.46 ms
[TIME] _update total: 5042.51 ms
[TIME] encode(future):  471.65 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1599.05 ms
[TIME] rollout & consistency (vectorized):  474.11 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 830.29 ms
[TIME-FLAT] Total flat forward: 832.11 ms
[TIME-VEC] _forward_flat: 832.22 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 832.44 ms
[TIME] reward_preds total:  832.53 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  832.69 ms
[TIME] Q preds:    5.61 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1537.77 ms
[TIME] grad clipping:    1.90 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4929.43 ms
[TIME] encode(future):  455.52 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1549.49 ms
[TIME] rollout & consistency (vectorized):  451.10 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 803.46 ms
[TIME-FLAT] Total flat forward: 805.27 ms
[TIME-VEC] _forward_flat: 805.40 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 805.64 ms
[TIME] reward_preds total:  805.73 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  805.89 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.25 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1513.62 ms
[TIME] grad clipping:    2.71 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4790.37 ms
[TIME] encode(future):  452.48 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1626.01 ms
[TIME] rollout & consistency (vectorized):  447.68 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 798.35 ms
[TIME-FLAT] Total flat forward: 800.19 ms
[TIME-VEC] _forward_flat: 800.31 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 800.54 ms
[TIME] reward_preds total:  800.63 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.79 ms
[TIME] Q preds:    5.20 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1494.95 ms
[TIME] grad clipping:    2.71 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4836.51 ms
[TIME] encode(future):  451.65 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1530.87 ms
[TIME] rollout & consistency (vectorized):  452.82 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 816.06 ms
[TIME-FLAT] Total flat forward: 817.93 ms
[TIME-VEC] _forward_flat: 818.04 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 818.28 ms
[TIME] reward_preds total:  818.39 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  818.60 ms
[TIME] Q preds:    5.86 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.40 ms
[TIME] backward: 1493.45 ms
[TIME] grad clipping:    2.62 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4762.86 ms
[TIME] encode(future):  444.03 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1614.27 ms
[TIME] rollout & consistency (vectorized):  461.54 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 810.57 ms
[TIME-FLAT] Total flat forward: 812.46 ms
[TIME-VEC] _forward_flat: 812.58 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 812.80 ms
[TIME] reward_preds total:  812.90 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  813.08 ms
[TIME] Q preds:    5.15 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    3.81 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1504.96 ms
[TIME] grad clipping:    2.64 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4854.20 ms
[TIME] encode(future):  468.41 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1538.29 ms
[TIME] rollout & consistency (vectorized):  449.74 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 801.32 ms
[TIME-FLAT] Total flat forward: 803.23 ms
[TIME-VEC] _forward_flat: 803.35 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 803.59 ms
[TIME] reward_preds total:  803.69 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  803.85 ms
[TIME] Q preds:    5.26 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1521.31 ms
[TIME] grad clipping:    3.05 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4796.58 ms
[TIME] encode(future):  452.10 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1630.37 ms
[TIME] rollout & consistency (vectorized):  453.60 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 793.73 ms
[TIME-FLAT] Total flat forward: 795.62 ms
[TIME-VEC] _forward_flat: 795.74 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 795.95 ms
[TIME] reward_preds total:  796.04 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  796.22 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    3.74 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1502.47 ms
[TIME] grad clipping:    3.02 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4851.32 ms
[TIME] encode(future):  448.75 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1562.48 ms
[TIME] rollout & consistency (vectorized):  446.58 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 796.49 ms
[TIME-FLAT] Total flat forward: 798.31 ms
[TIME-VEC] _forward_flat: 798.42 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 798.65 ms
[TIME] reward_preds total:  798.75 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  798.90 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.86 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1515.78 ms
[TIME] grad clipping:    2.59 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4786.84 ms
[TIME] encode(future):  439.22 ms
[TIME] compute TD targets:    0.96 ms
[TIME] encode(initial): 1620.60 ms
[TIME] rollout & consistency (vectorized):  449.71 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 1.04 ms
[TIME-FLAT] Gate forward: 1.11 ms
[TIME-FLAT] Softmax + TopK: 0.43 ms
[TIME-FLAT] Sparse Expert Forward: 809.53 ms
[TIME-FLAT] Total flat forward: 812.40 ms
[TIME-VEC] _forward_flat: 812.55 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 816.61 ms
[TIME] reward_preds total:  816.71 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  816.88 ms
[TIME] Q preds:    5.15 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1497.21 ms
[TIME] grad clipping:    2.63 ms
[TIME] optim.step:    1.23 ms
[TIME] _update total: 4837.96 ms
[TIME] encode(future):  453.78 ms
[TIME] compute TD targets:    1.47 ms
[TIME] encode(initial): 1522.72 ms
[TIME] rollout & consistency (vectorized):  450.78 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.80 ms
[TIME-FLAT] Softmax + TopK: 0.37 ms
[TIME-FLAT] Sparse Expert Forward: 787.28 ms
[TIME-FLAT] Total flat forward: 789.20 ms
[TIME-VEC] _forward_flat: 789.32 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 789.54 ms
[TIME] reward_preds total:  789.63 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  789.79 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1488.73 ms
[TIME] grad clipping:    2.98 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4721.48 ms
[TIME] encode(future):  438.98 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1515.24 ms
[TIME] rollout & consistency (vectorized):  443.61 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 882.86 ms
[TIME-FLAT] Total flat forward: 884.70 ms
[TIME-VEC] _forward_flat: 884.82 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 885.06 ms
[TIME] reward_preds total:  885.15 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  885.32 ms
[TIME] Q preds:    5.60 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.11 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1496.60 ms
[TIME] grad clipping:    1.87 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4794.42 ms
[TIME] encode(future):  446.97 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1527.10 ms
[TIME] rollout & consistency (vectorized):  450.81 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 801.53 ms
[TIME-FLAT] Total flat forward: 803.39 ms
[TIME-VEC] _forward_flat: 803.53 ms
[TIME-VEC] Reshape back: 0.10 ms
[TIME-VEC] Total vector reward: 803.81 ms
[TIME] reward_preds total:  803.90 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  804.08 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    1.81 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1505.60 ms
[TIME] grad clipping:    1.95 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4749.50 ms
[TIME] encode(future):  445.04 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1540.21 ms
[TIME] rollout & consistency (vectorized):  448.89 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.37 ms
[TIME-FLAT] Sparse Expert Forward: 793.11 ms
[TIME-FLAT] Total flat forward: 794.98 ms
[TIME-VEC] _forward_flat: 795.12 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 795.34 ms
[TIME] reward_preds total:  795.43 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.61 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1496.35 ms
[TIME] grad clipping:    2.33 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 4740.79 ms
[TIME] encode(future):  465.58 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1613.11 ms
[TIME] rollout & consistency (vectorized):  449.12 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 789.63 ms
[TIME-FLAT] Total flat forward: 791.48 ms
[TIME-VEC] _forward_flat: 791.59 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 791.80 ms
[TIME] reward_preds total:  791.88 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  792.05 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.77 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1486.50 ms
[TIME] grad clipping:    1.89 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4819.83 ms
[TIME] encode(future):  446.87 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1557.55 ms
[TIME] rollout & consistency (vectorized):  476.72 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 811.62 ms
[TIME-FLAT] Total flat forward: 813.51 ms
[TIME-VEC] _forward_flat: 813.62 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 813.84 ms
[TIME] reward_preds total:  813.93 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  814.09 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.97 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1496.67 ms
[TIME] grad clipping:    1.91 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4807.07 ms
[TIME] encode(future):  506.41 ms
[TIME] compute TD targets:    1.18 ms
[TIME] encode(initial): 1632.27 ms
[TIME] rollout & consistency (vectorized):  457.21 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 795.65 ms
[TIME-FLAT] Total flat forward: 797.47 ms
[TIME-VEC] _forward_flat: 797.63 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 797.86 ms
[TIME] reward_preds total:  797.96 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  798.13 ms
[TIME] Q preds:    5.04 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1511.98 ms
[TIME] grad clipping:    2.32 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4920.37 ms
[TIME] encode(future):  453.70 ms
[TIME] compute TD targets:    1.23 ms
[TIME] encode(initial): 1516.37 ms
[TIME] rollout & consistency (vectorized):  453.50 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.53 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 789.31 ms
[TIME-FLAT] Total flat forward: 791.26 ms
[TIME-VEC] _forward_flat: 791.39 ms
[TIME-VEC] Reshape back: 0.11 ms
[TIME-VEC] Total vector reward: 791.65 ms
[TIME] reward_preds total:  791.76 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  791.95 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1507.91 ms
[TIME] grad clipping:    2.09 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4738.14 ms
[TIME] encode(future):  457.67 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1645.22 ms
[TIME] rollout & consistency (vectorized):  457.29 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.52 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 794.34 ms
[TIME-FLAT] Total flat forward: 796.28 ms
[TIME-VEC] _forward_flat: 796.45 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 796.69 ms
[TIME] reward_preds total:  796.79 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  796.99 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.19 ms
[TIME] backward: 1517.51 ms
[TIME] grad clipping:    2.64 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4889.23 ms
[TIME] encode(future):  453.65 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1542.66 ms
[TIME] rollout & consistency (vectorized):  455.99 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 795.88 ms
[TIME-FLAT] Total flat forward: 797.75 ms
[TIME-VEC] _forward_flat: 797.88 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 798.11 ms
[TIME] reward_preds total:  798.21 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  798.37 ms
[TIME] Q preds:    5.03 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.23 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1521.57 ms
[TIME] grad clipping:    2.56 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4786.68 ms
[TIME] encode(future):  453.05 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1634.31 ms
[TIME] rollout & consistency (vectorized):  451.45 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.91 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 798.43 ms
[TIME-FLAT] Total flat forward: 800.44 ms
[TIME-VEC] _forward_flat: 800.56 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 800.78 ms
[TIME] reward_preds total:  800.88 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  801.06 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1503.94 ms
[TIME] grad clipping:    3.05 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4858.42 ms
[TIME] encode(future):  454.08 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1527.65 ms
[TIME] rollout & consistency (vectorized):  453.19 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 797.33 ms
[TIME-FLAT] Total flat forward: 799.20 ms
[TIME-VEC] _forward_flat: 799.31 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 799.54 ms
[TIME] reward_preds total:  799.66 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  799.82 ms
[TIME] Q preds:    5.18 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1527.68 ms
[TIME] grad clipping:    2.15 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4776.50 ms
[TIME] encode(future):  459.37 ms
[TIME] compute TD targets:    0.96 ms
[TIME] encode(initial): 1540.12 ms
[TIME] rollout & consistency (vectorized):  455.60 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 903.61 ms
[TIME-FLAT] Total flat forward: 905.37 ms
[TIME-VEC] _forward_flat: 905.49 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 905.69 ms
[TIME] reward_preds total:  905.79 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  905.94 ms
[TIME] Q preds:    5.21 ms
[TIME] reward loss:    0.99 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1520.58 ms
[TIME] grad clipping:    2.60 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4896.15 ms
[TIME] encode(future):  454.33 ms
[TIME] compute TD targets:    1.16 ms
[TIME] encode(initial): 1538.20 ms
[TIME] rollout & consistency (vectorized):  456.10 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 800.85 ms
[TIME-FLAT] Total flat forward: 802.68 ms
[TIME-VEC] _forward_flat: 802.79 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 803.04 ms
[TIME] reward_preds total:  803.14 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  803.31 ms
[TIME] Q preds:    5.25 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    3.89 ms
[TIME] loss computation:    0.19 ms
[TIME] backward: 1515.08 ms
[TIME] grad clipping:    2.70 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4783.60 ms
[TIME] encode(future):  452.37 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1545.20 ms
[TIME] rollout & consistency (vectorized):  459.34 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 796.42 ms
[TIME-FLAT] Total flat forward: 798.28 ms
[TIME-VEC] _forward_flat: 798.39 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 798.62 ms
[TIME] reward_preds total:  798.73 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  798.92 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.16 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1516.74 ms
[TIME] grad clipping:    2.70 ms
[TIME] optim.step:    1.26 ms
[TIME] _update total: 4787.43 ms
[TIME] encode(future):  453.97 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1636.75 ms
[TIME] rollout & consistency (vectorized):  451.14 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 794.57 ms
[TIME-FLAT] Total flat forward: 796.51 ms
[TIME-VEC] _forward_flat: 796.63 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 796.86 ms
[TIME] reward_preds total:  796.96 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.14 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    3.52 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1510.16 ms
[TIME] grad clipping:    3.18 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4865.64 ms
[TIME] encode(future):  453.38 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1525.01 ms
[TIME] rollout & consistency (vectorized):  454.16 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 800.93 ms
[TIME-FLAT] Total flat forward: 802.80 ms
[TIME-VEC] _forward_flat: 802.94 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 803.16 ms
[TIME] reward_preds total:  803.26 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  803.42 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.11 ms
[TIME] loss computation:    0.22 ms
[TIME] backward: 1510.64 ms
[TIME] grad clipping:    2.27 ms
[TIME] optim.step:    1.22 ms
[TIME] _update total: 4764.59 ms
[TIME] encode(future):  449.25 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1612.39 ms
[TIME] rollout & consistency (vectorized):  440.07 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.92 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 781.29 ms
[TIME-FLAT] Total flat forward: 783.17 ms
[TIME-VEC] _forward_flat: 783.32 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 783.54 ms
[TIME] reward_preds total:  783.64 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  783.80 ms
[TIME] Q preds:    5.26 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1484.10 ms
[TIME] grad clipping:    1.93 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4783.94 ms
[TIME] encode(future):  449.19 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1497.68 ms
[TIME] rollout & consistency (vectorized):  443.92 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 788.85 ms
[TIME-FLAT] Total flat forward: 790.67 ms
[TIME-VEC] _forward_flat: 790.78 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 791.01 ms
[TIME] reward_preds total:  791.10 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  791.25 ms
[TIME] Q preds:    5.21 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.11 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1460.73 ms
[TIME] grad clipping:    2.10 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4656.81 ms
[TIME] encode(future):  451.56 ms
[TIME] compute TD targets:    1.08 ms
[TIME] encode(initial): 1613.92 ms
[TIME] rollout & consistency (vectorized):  453.96 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.59 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 788.44 ms
[TIME-FLAT] Total flat forward: 790.46 ms
[TIME-VEC] _forward_flat: 790.58 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 790.80 ms
[TIME] reward_preds total:  790.91 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  791.12 ms
[TIME] Q preds:    4.97 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.01 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1495.29 ms
[TIME] grad clipping:    1.99 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 4820.21 ms
[TIME] encode(future):  450.07 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1516.43 ms
[TIME] rollout & consistency (vectorized):  452.70 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 786.69 ms
[TIME-FLAT] Total flat forward: 788.55 ms
[TIME-VEC] _forward_flat: 788.65 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 788.87 ms
[TIME] reward_preds total:  788.96 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  789.13 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1502.15 ms
[TIME] grad clipping:    2.30 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4724.65 ms
[TIME] encode(future):  448.44 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1609.26 ms
[TIME] rollout & consistency (vectorized):  452.20 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 794.45 ms
[TIME-FLAT] Total flat forward: 796.38 ms
[TIME-VEC] _forward_flat: 796.50 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 796.71 ms
[TIME] reward_preds total:  796.80 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  796.97 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1502.29 ms
[TIME] grad clipping:    2.09 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4823.12 ms
[TIME] encode(future):  449.44 ms
[TIME] compute TD targets:    0.95 ms
[TIME] encode(initial): 1526.22 ms
[TIME] rollout & consistency (vectorized):  451.69 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 790.60 ms
[TIME-FLAT] Total flat forward: 792.46 ms
[TIME-VEC] _forward_flat: 792.58 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 792.83 ms
[TIME] reward_preds total:  792.94 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  793.11 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.01 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1509.11 ms
[TIME] grad clipping:    2.16 ms
[TIME] optim.step:    1.16 ms
[TIME] _update total: 4743.36 ms
[TIME] encode(future):  453.97 ms
[TIME] compute TD targets:    1.08 ms
[TIME] encode(initial): 1619.34 ms
[TIME] rollout & consistency (vectorized):  453.96 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 793.53 ms
[TIME-FLAT] Total flat forward: 795.40 ms
[TIME-VEC] _forward_flat: 795.55 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 795.79 ms
[TIME] reward_preds total:  795.93 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  796.08 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1496.04 ms
[TIME] grad clipping:    2.02 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4833.29 ms
[TIME] encode(future):  450.78 ms
[TIME] compute TD targets:    1.21 ms
[TIME] encode(initial): 1525.07 ms
[TIME] rollout & consistency (vectorized):  451.70 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 787.15 ms
[TIME-FLAT] Total flat forward: 789.02 ms
[TIME-VEC] _forward_flat: 789.14 ms
[TIME-VEC] Reshape back: 0.10 ms
[TIME-VEC] Total vector reward: 789.39 ms
[TIME] reward_preds total:  789.49 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  789.66 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1502.23 ms
[TIME] grad clipping:    2.44 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4733.93 ms
[TIME] encode(future):  450.46 ms
[TIME] compute TD targets:    1.18 ms
[TIME] encode(initial): 1537.31 ms
[TIME] rollout & consistency (vectorized):  450.13 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.38 ms
[TIME-FLAT] Sparse Expert Forward: 905.27 ms
[TIME-FLAT] Total flat forward: 907.25 ms
[TIME-VEC] _forward_flat: 907.41 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 907.63 ms
[TIME] reward_preds total:  907.74 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  907.91 ms
[TIME] Q preds:    5.03 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1504.64 ms
[TIME] grad clipping:    2.71 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4865.16 ms
[TIME] encode(future):  450.64 ms
[TIME] compute TD targets:    1.09 ms
[TIME] encode(initial): 1522.72 ms
[TIME] rollout & consistency (vectorized):  451.42 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 794.65 ms
[TIME-FLAT] Total flat forward: 796.53 ms
[TIME-VEC] _forward_flat: 796.67 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 796.93 ms
[TIME] reward_preds total:  797.04 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.25 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1490.72 ms
[TIME] grad clipping:    2.06 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4727.46 ms
[TIME] encode(future):  448.55 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1549.97 ms
[TIME] rollout & consistency (vectorized):  474.51 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 1.11 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 803.02 ms
[TIME-FLAT] Total flat forward: 805.17 ms
[TIME-VEC] _forward_flat: 805.28 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 805.48 ms
[TIME] reward_preds total:  805.57 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  805.72 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1494.83 ms
[TIME] grad clipping:    2.01 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4787.38 ms
[TIME] encode(future):  442.68 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1604.72 ms
[TIME] rollout & consistency (vectorized):  451.70 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.93 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 815.00 ms
[TIME-FLAT] Total flat forward: 816.94 ms
[TIME-VEC] _forward_flat: 817.06 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 817.29 ms
[TIME] reward_preds total:  817.39 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  817.56 ms
[TIME] Q preds:    6.21 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.63 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1548.49 ms
[TIME] grad clipping:    1.98 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4880.52 ms
[TIME] encode(future):  450.15 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1521.19 ms
[TIME] rollout & consistency (vectorized):  455.35 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 786.72 ms
[TIME-FLAT] Total flat forward: 788.60 ms
[TIME-VEC] _forward_flat: 788.71 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 788.96 ms
[TIME] reward_preds total:  789.05 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  789.22 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1510.40 ms
[TIME] grad clipping:    2.72 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4740.93 ms
[TIME] encode(future):  452.84 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1636.69 ms
[TIME] rollout & consistency (vectorized):  453.44 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 798.73 ms
[TIME-FLAT] Total flat forward: 800.57 ms
[TIME-VEC] _forward_flat: 800.70 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 800.92 ms
[TIME] reward_preds total:  801.03 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  801.21 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1503.17 ms
[TIME] grad clipping:    2.66 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4861.75 ms
[TIME] encode(future):  452.07 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1522.39 ms
[TIME] rollout & consistency (vectorized):  452.20 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 798.92 ms
[TIME-FLAT] Total flat forward: 800.76 ms
[TIME-VEC] _forward_flat: 800.90 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 801.12 ms
[TIME] reward_preds total:  801.23 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  801.42 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1509.53 ms
[TIME] grad clipping:    2.90 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4752.35 ms
[TIME] encode(future):  450.72 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1520.58 ms
[TIME] rollout & consistency (vectorized):  445.20 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 876.39 ms
[TIME-FLAT] Total flat forward: 878.28 ms
[TIME-VEC] _forward_flat: 878.41 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 878.63 ms
[TIME] reward_preds total:  878.74 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  878.94 ms
[TIME] Q preds:    5.03 ms
[TIME] reward loss:    0.77 ms
[TIME] value loss:    2.01 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1467.95 ms
[TIME] grad clipping:    2.78 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4777.86 ms
[TIME] encode(future):  439.08 ms
[TIME] compute TD targets:    0.98 ms
[TIME] encode(initial): 1522.72 ms
[TIME] rollout & consistency (vectorized):  442.40 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.08 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 784.04 ms
[TIME-FLAT] Total flat forward: 785.89 ms
[TIME-VEC] _forward_flat: 786.00 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 786.23 ms
[TIME] reward_preds total:  786.34 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  786.54 ms
[TIME] Q preds:    6.62 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1477.03 ms
[TIME] grad clipping:    2.73 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4683.61 ms
[TIME] encode(future):  442.13 ms
[TIME] compute TD targets:    1.13 ms
[TIME] encode(initial): 1506.14 ms
[TIME] rollout & consistency (vectorized):  446.45 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 888.71 ms
[TIME-FLAT] Total flat forward: 890.52 ms
[TIME-VEC] _forward_flat: 890.65 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 890.86 ms
[TIME] reward_preds total:  890.95 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  891.10 ms
[TIME] Q preds:    5.19 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    3.54 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1481.34 ms
[TIME] grad clipping:    2.88 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4783.35 ms
[TIME] encode(future):  476.33 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1610.96 ms
[TIME] rollout & consistency (vectorized):  472.98 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 1.89 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 843.21 ms
[TIME-FLAT] Total flat forward: 846.07 ms
[TIME-VEC] _forward_flat: 846.18 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 846.39 ms
[TIME] reward_preds total:  846.49 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  846.64 ms
[TIME] Q preds:    5.26 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1585.45 ms
[TIME] grad clipping:    2.65 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 5006.88 ms
[TIME] encode(future):  475.57 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1611.07 ms
[TIME] rollout & consistency (vectorized):  474.80 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.47 ms
[TIME-FLAT] Sparse Expert Forward: 829.79 ms
[TIME-FLAT] Total flat forward: 831.77 ms
[TIME-VEC] _forward_flat: 831.90 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 832.14 ms
[TIME] reward_preds total:  832.24 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  832.39 ms
[TIME] Q preds:    5.35 ms
[TIME] reward loss:    2.03 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1588.30 ms
[TIME] grad clipping:    5.09 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 5000.65 ms
[TIME] encode(future):  480.25 ms
[TIME] compute TD targets:    0.97 ms
[TIME] encode(initial): 1699.40 ms
[TIME] rollout & consistency (vectorized):  450.29 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 805.27 ms
[TIME-FLAT] Total flat forward: 807.11 ms
[TIME-VEC] _forward_flat: 807.24 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 807.49 ms
[TIME] reward_preds total:  807.58 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  807.76 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.86 ms
[TIME] value loss:    2.32 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1504.94 ms
[TIME] grad clipping:    2.79 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 4957.55 ms
[TIME] encode(future):  451.02 ms
[TIME] compute TD targets:    1.16 ms
[TIME] encode(initial): 1507.83 ms
[TIME] rollout & consistency (vectorized):  460.70 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 787.73 ms
[TIME-FLAT] Total flat forward: 789.58 ms
[TIME-VEC] _forward_flat: 789.71 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 789.93 ms
[TIME] reward_preds total:  790.04 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  790.20 ms
[TIME] Q preds:    5.18 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1487.29 ms
[TIME] grad clipping:    1.94 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4711.30 ms
[TIME] encode(future):  449.12 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1647.15 ms
[TIME] rollout & consistency (vectorized):  449.56 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 784.91 ms
[TIME-FLAT] Total flat forward: 786.82 ms
[TIME-VEC] _forward_flat: 786.94 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 787.16 ms
[TIME] reward_preds total:  787.27 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  787.45 ms
[TIME] Q preds:    5.19 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1492.43 ms
[TIME] grad clipping:    2.06 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4840.22 ms
[TIME] encode(future):  449.82 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1522.09 ms
[TIME] rollout & consistency (vectorized):  450.36 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 794.84 ms
[TIME-FLAT] Total flat forward: 796.73 ms
[TIME-VEC] _forward_flat: 796.88 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 797.09 ms
[TIME] reward_preds total:  797.18 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.36 ms
[TIME] Q preds:    5.01 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.22 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1509.44 ms
[TIME] grad clipping:    2.53 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4743.43 ms
[TIME] encode(future):  452.57 ms
[TIME] compute TD targets:    1.15 ms
[TIME] encode(initial): 1631.28 ms
[TIME] rollout & consistency (vectorized):  448.34 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.41 ms
[TIME-FLAT] Sparse Expert Forward: 778.68 ms
[TIME-FLAT] Total flat forward: 780.62 ms
[TIME-VEC] _forward_flat: 780.74 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 780.96 ms
[TIME] reward_preds total:  781.05 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  781.24 ms
[TIME] Q preds:    5.04 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.01 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1469.76 ms
[TIME] grad clipping:    2.25 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4797.11 ms
[TIME] encode(future):  444.59 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1522.44 ms
[TIME] rollout & consistency (vectorized):  453.82 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 790.35 ms
[TIME-FLAT] Total flat forward: 792.18 ms
[TIME-VEC] _forward_flat: 792.31 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 792.53 ms
[TIME] reward_preds total:  792.62 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  792.77 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1507.27 ms
[TIME] grad clipping:    2.54 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4735.18 ms
[TIME] encode(future):  445.39 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1650.92 ms
[TIME] rollout & consistency (vectorized):  461.54 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 802.74 ms
[TIME-FLAT] Total flat forward: 804.54 ms
[TIME-VEC] _forward_flat: 804.65 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 804.87 ms
[TIME] reward_preds total:  804.99 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  805.16 ms
[TIME] Q preds:    5.21 ms
[TIME] reward loss:    0.86 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1512.58 ms
[TIME] grad clipping:    2.46 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4890.16 ms
[TIME] encode(future):  453.61 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1532.62 ms
[TIME] rollout & consistency (vectorized):  453.98 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.51 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 786.00 ms
[TIME-FLAT] Total flat forward: 788.00 ms
[TIME-VEC] _forward_flat: 788.11 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 788.32 ms
[TIME] reward_preds total:  788.42 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  788.60 ms
[TIME] Q preds:    5.02 ms
[TIME] reward loss:    0.77 ms
[TIME] value loss:    2.02 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1479.08 ms
[TIME] grad clipping:    2.67 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4722.16 ms
[TIME] encode(future):  439.64 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1519.37 ms
[TIME] rollout & consistency (vectorized):  439.83 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.53 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 886.25 ms
[TIME-FLAT] Total flat forward: 888.16 ms
[TIME-VEC] _forward_flat: 888.27 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 888.50 ms
[TIME] reward_preds total:  888.59 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  888.75 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.77 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1541.56 ms
[TIME] grad clipping:    2.92 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4843.60 ms
[TIME] encode(future):  466.03 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1525.30 ms
[TIME] rollout & consistency (vectorized):  451.56 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 792.67 ms
[TIME-FLAT] Total flat forward: 794.54 ms
[TIME-VEC] _forward_flat: 794.66 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 794.90 ms
[TIME] reward_preds total:  794.99 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.17 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    3.56 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1519.17 ms
[TIME] grad clipping:    2.58 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4773.07 ms
[TIME] encode(future):  456.74 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1547.32 ms
[TIME] rollout & consistency (vectorized):  456.75 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 2.65 ms
[TIME-FLAT] Gate forward: 1.49 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 900.59 ms
[TIME-FLAT] Total flat forward: 905.30 ms
[TIME-VEC] _forward_flat: 905.42 ms
[TIME-VEC] Reshape back: 0.10 ms
[TIME-VEC] Total vector reward: 905.67 ms
[TIME] reward_preds total:  905.77 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  905.96 ms
[TIME] Q preds:    5.18 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1542.03 ms
[TIME] grad clipping:    2.51 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4923.18 ms
[TIME] encode(future):  453.78 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1557.68 ms
[TIME] rollout & consistency (vectorized):  455.16 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 792.68 ms
[TIME-FLAT] Total flat forward: 794.48 ms
[TIME-VEC] _forward_flat: 794.64 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 794.88 ms
[TIME] reward_preds total:  794.99 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.16 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1536.37 ms
[TIME] grad clipping:    2.79 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 4812.98 ms
[TIME] encode(future):  456.75 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1537.60 ms
[TIME] rollout & consistency (vectorized):  446.32 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 800.37 ms
[TIME-FLAT] Total flat forward: 802.21 ms
[TIME-VEC] _forward_flat: 802.35 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 802.59 ms
[TIME] reward_preds total:  802.68 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  802.84 ms
[TIME] Q preds:    5.04 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1493.33 ms
[TIME] grad clipping:    2.06 ms
[TIME] optim.step:    1.16 ms
[TIME] _update total: 4751.25 ms
[TIME] encode(future):  447.66 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1611.20 ms
[TIME] rollout & consistency (vectorized):  452.67 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.90 ms
[TIME-FLAT] Softmax + TopK: 0.38 ms
[TIME-FLAT] Sparse Expert Forward: 790.24 ms
[TIME-FLAT] Total flat forward: 792.35 ms
[TIME-VEC] _forward_flat: 792.47 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 792.68 ms
[TIME] reward_preds total:  792.78 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  792.95 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1489.04 ms
[TIME] grad clipping:    2.19 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4807.95 ms
[TIME] encode(future):  452.53 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1529.46 ms
[TIME] rollout & consistency (vectorized):  448.43 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 795.12 ms
[TIME-FLAT] Total flat forward: 796.92 ms
[TIME-VEC] _forward_flat: 797.03 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 797.24 ms
[TIME] reward_preds total:  797.33 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.49 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1499.87 ms
[TIME] grad clipping:    1.97 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4741.45 ms
[TIME] encode(future):  450.60 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1630.62 ms
[TIME] rollout & consistency (vectorized):  453.31 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.51 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 791.66 ms
[TIME-FLAT] Total flat forward: 793.54 ms
[TIME-VEC] _forward_flat: 793.65 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 793.89 ms
[TIME] reward_preds total:  793.98 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  794.18 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.91 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1486.30 ms
[TIME] grad clipping:    2.28 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4829.11 ms
[TIME] encode(future):  449.77 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1523.15 ms
[TIME] rollout & consistency (vectorized):  450.46 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 791.42 ms
[TIME-FLAT] Total flat forward: 793.22 ms
[TIME-VEC] _forward_flat: 793.34 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 793.58 ms
[TIME] reward_preds total:  793.67 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  793.85 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.01 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1486.97 ms
[TIME] grad clipping:    2.18 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4718.07 ms
[TIME] encode(future):  448.75 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1618.69 ms
[TIME] rollout & consistency (vectorized):  449.64 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.91 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 790.93 ms
[TIME-FLAT] Total flat forward: 792.89 ms
[TIME-VEC] _forward_flat: 793.03 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 793.27 ms
[TIME] reward_preds total:  793.38 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  793.55 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.22 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1486.46 ms
[TIME] grad clipping:    1.85 ms
[TIME] optim.step:    1.06 ms
[TIME] _update total: 4811.22 ms
[TIME] encode(future):  449.71 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1535.21 ms
[TIME] rollout & consistency (vectorized):  448.71 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.08 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 804.75 ms
[TIME-FLAT] Total flat forward: 806.64 ms
[TIME-VEC] _forward_flat: 806.78 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 807.03 ms
[TIME] reward_preds total:  807.14 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  807.34 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1609.18 ms
[TIME] grad clipping:    3.34 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4866.69 ms
[TIME] encode(future):  498.60 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1609.56 ms
[TIME] rollout & consistency (vectorized):  475.06 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 948.82 ms
[TIME-FLAT] Total flat forward: 950.71 ms
[TIME-VEC] _forward_flat: 950.83 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 951.04 ms
[TIME] reward_preds total:  951.13 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  951.31 ms
[TIME] Q preds:    5.73 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1544.18 ms
[TIME] grad clipping:    2.47 ms
[TIME] optim.step:    1.27 ms
[TIME] _update total: 5093.95 ms
[TIME] encode(future):  448.38 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1557.61 ms
[TIME] rollout & consistency (vectorized):  452.78 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.91 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 799.23 ms
[TIME-FLAT] Total flat forward: 801.20 ms
[TIME-VEC] _forward_flat: 801.36 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 801.60 ms
[TIME] reward_preds total:  801.72 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  801.88 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.21 ms
[TIME] backward: 1493.46 ms
[TIME] grad clipping:    2.74 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4768.80 ms
[TIME] encode(future):  479.84 ms
[TIME] compute TD targets:    1.39 ms
[TIME] encode(initial): 1580.48 ms
[TIME] rollout & consistency (vectorized):  456.52 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 801.07 ms
[TIME-FLAT] Total flat forward: 802.98 ms
[TIME-VEC] _forward_flat: 803.13 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 803.38 ms
[TIME] reward_preds total:  803.49 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  803.67 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.91 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1594.34 ms
[TIME] grad clipping:    2.62 ms
[TIME] optim.step:    1.37 ms
[TIME] _update total: 4930.13 ms
[TIME] encode(future):  463.03 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1616.62 ms
[TIME] rollout & consistency (vectorized):  450.59 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 846.66 ms
[TIME-FLAT] Total flat forward: 848.54 ms
[TIME-VEC] _forward_flat: 848.69 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 848.91 ms
[TIME] reward_preds total:  849.03 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  849.20 ms
[TIME] Q preds:    5.22 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1525.20 ms
[TIME] grad clipping:    2.47 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4919.16 ms
[TIME] encode(future):  474.92 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1536.61 ms
[TIME] rollout & consistency (vectorized):  472.09 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 797.22 ms
[TIME-FLAT] Total flat forward: 799.12 ms
[TIME-VEC] _forward_flat: 799.26 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 799.50 ms
[TIME] reward_preds total:  799.59 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  799.79 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1509.38 ms
[TIME] grad clipping:    2.79 ms
[TIME] optim.step:    1.16 ms
[TIME] _update total: 4807.51 ms
[TIME] encode(future):  451.96 ms
[TIME] compute TD targets:    1.22 ms
[TIME] encode(initial): 1650.14 ms
[TIME] rollout & consistency (vectorized):  446.12 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 789.15 ms
[TIME-FLAT] Total flat forward: 791.00 ms
[TIME-VEC] _forward_flat: 791.11 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 791.32 ms
[TIME] reward_preds total:  791.43 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  791.60 ms
[TIME] Q preds:    5.02 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1525.39 ms
[TIME] grad clipping:    2.57 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4879.65 ms
[TIME] encode(future):  463.58 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1562.64 ms
[TIME] rollout & consistency (vectorized):  452.07 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.89 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 798.92 ms
[TIME-FLAT] Total flat forward: 800.86 ms
[TIME-VEC] _forward_flat: 800.98 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 801.22 ms
[TIME] reward_preds total:  801.31 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  801.48 ms
[TIME] Q preds:    5.06 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.24 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1500.53 ms
[TIME] grad clipping:    2.89 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4795.09 ms
[TIME] encode(future):  456.00 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1639.98 ms
[TIME] rollout & consistency (vectorized):  448.66 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 803.10 ms
[TIME-FLAT] Total flat forward: 804.92 ms
[TIME-VEC] _forward_flat: 805.04 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 805.27 ms
[TIME] reward_preds total:  805.36 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  805.53 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.22 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1539.96 ms
[TIME] grad clipping:    2.47 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4904.57 ms
[TIME] encode(future):  447.32 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1543.77 ms
[TIME] rollout & consistency (vectorized):  446.03 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 848.44 ms
[TIME-FLAT] Total flat forward: 850.31 ms
[TIME-VEC] _forward_flat: 850.42 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 850.64 ms
[TIME] reward_preds total:  850.73 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  850.90 ms
[TIME] Q preds:    5.48 ms
[TIME] reward loss:    0.94 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1534.21 ms
[TIME] grad clipping:    2.31 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4836.83 ms
[TIME] encode(future):  457.54 ms
[TIME] compute TD targets:    1.09 ms
[TIME] encode(initial): 1654.02 ms
[TIME] rollout & consistency (vectorized):  456.72 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.80 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 792.88 ms
[TIME-FLAT] Total flat forward: 794.63 ms
[TIME-VEC] _forward_flat: 794.77 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 794.98 ms
[TIME] reward_preds total:  795.10 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.26 ms
[TIME] Q preds:    5.19 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.18 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1510.08 ms
[TIME] grad clipping:    2.72 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4888.39 ms
[TIME] encode(future):  448.87 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1523.18 ms
[TIME] rollout & consistency (vectorized):  453.23 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 789.06 ms
[TIME-FLAT] Total flat forward: 790.98 ms
[TIME-VEC] _forward_flat: 791.89 ms
[TIME-VEC] Reshape back: 0.21 ms
[TIME-VEC] Total vector reward: 792.29 ms
[TIME] reward_preds total:  792.38 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  792.53 ms
[TIME] Q preds:    5.22 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.11 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1494.23 ms
[TIME] grad clipping:    2.82 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4726.69 ms
[TIME] encode(future):  451.34 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1522.02 ms
[TIME] rollout & consistency (vectorized):  450.69 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.08 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 896.88 ms
[TIME-FLAT] Total flat forward: 898.81 ms
[TIME-VEC] _forward_flat: 898.92 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 899.16 ms
[TIME] reward_preds total:  899.26 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  899.44 ms
[TIME] Q preds:    5.32 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1504.86 ms
[TIME] grad clipping:    2.38 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4842.83 ms
[TIME] encode(future):  447.57 ms
[TIME] compute TD targets:    1.22 ms
[TIME] encode(initial): 1521.32 ms
[TIME] rollout & consistency (vectorized):  453.58 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 795.99 ms
[TIME-FLAT] Total flat forward: 797.88 ms
[TIME-VEC] _forward_flat: 798.00 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 798.23 ms
[TIME] reward_preds total:  798.32 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  798.48 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.11 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1498.27 ms
[TIME] grad clipping:    3.01 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4734.24 ms
[TIME] encode(future):  441.40 ms
[TIME] compute TD targets:    1.16 ms
[TIME] encode(initial): 1529.59 ms
[TIME] rollout & consistency (vectorized):  448.93 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 787.86 ms
[TIME-FLAT] Total flat forward: 789.74 ms
[TIME-VEC] _forward_flat: 789.86 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 790.09 ms
[TIME] reward_preds total:  790.21 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  790.37 ms
[TIME] Q preds:    5.00 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1500.43 ms
[TIME] grad clipping:    2.10 ms
[TIME] optim.step:    1.16 ms
[TIME] _update total: 4724.56 ms
[TIME] encode(future):  452.71 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1633.67 ms
[TIME] rollout & consistency (vectorized):  448.90 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 795.30 ms
[TIME-FLAT] Total flat forward: 797.13 ms
[TIME-VEC] _forward_flat: 797.24 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 797.46 ms
[TIME] reward_preds total:  797.55 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.74 ms
[TIME] Q preds:    5.15 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1482.48 ms
[TIME] grad clipping:    2.45 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4829.76 ms
[TIME] encode(future):  440.78 ms
[TIME] compute TD targets:    0.96 ms
[TIME] encode(initial): 1517.53 ms
[TIME] rollout & consistency (vectorized):  448.06 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 791.75 ms
[TIME-FLAT] Total flat forward: 793.62 ms
[TIME-VEC] _forward_flat: 793.73 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 793.94 ms
[TIME] reward_preds total:  794.03 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  794.20 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1484.72 ms
[TIME] grad clipping:    2.47 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4699.46 ms
[TIME] encode(future):  444.48 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1600.38 ms
[TIME] rollout & consistency (vectorized):  441.36 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 786.54 ms
[TIME-FLAT] Total flat forward: 788.38 ms
[TIME-VEC] _forward_flat: 788.49 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 788.71 ms
[TIME] reward_preds total:  788.81 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  788.98 ms
[TIME] Q preds:    4.96 ms
[TIME] reward loss:    0.77 ms
[TIME] value loss:    2.16 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1505.30 ms
[TIME] grad clipping:    2.53 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4794.57 ms
[TIME] encode(future):  453.99 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1527.52 ms
[TIME] rollout & consistency (vectorized):  450.73 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.51 ms
[TIME-FLAT] Gate forward: 0.88 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 791.10 ms
[TIME-FLAT] Total flat forward: 793.07 ms
[TIME-VEC] _forward_flat: 793.19 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 793.43 ms
[TIME] reward_preds total:  793.53 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  793.71 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1511.51 ms
[TIME] grad clipping:    2.82 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4752.15 ms
[TIME] encode(future):  456.06 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1633.95 ms
[TIME] rollout & consistency (vectorized):  450.46 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 796.47 ms
[TIME-FLAT] Total flat forward: 798.38 ms
[TIME-VEC] _forward_flat: 798.49 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 798.72 ms
[TIME] reward_preds total:  798.81 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  798.98 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1508.12 ms
[TIME] grad clipping:    2.88 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4862.28 ms
[TIME] encode(future):  450.20 ms
[TIME] compute TD targets:    0.97 ms
[TIME] encode(initial): 1530.36 ms
[TIME] rollout & consistency (vectorized):  453.34 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.65 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 799.93 ms
[TIME-FLAT] Total flat forward: 801.97 ms
[TIME-VEC] _forward_flat: 802.08 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 802.30 ms
[TIME] reward_preds total:  802.40 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  802.58 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1489.51 ms
[TIME] grad clipping:    2.58 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4740.79 ms
[TIME] encode(future):  454.82 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1613.66 ms
[TIME] rollout & consistency (vectorized):  451.04 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 784.47 ms
[TIME-FLAT] Total flat forward: 786.31 ms
[TIME-VEC] _forward_flat: 786.42 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 786.67 ms
[TIME] reward_preds total:  786.79 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  786.96 ms
[TIME] Q preds:    5.18 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1496.90 ms
[TIME] grad clipping:    2.92 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4818.50 ms
[TIME] encode(future):  450.39 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1519.65 ms
[TIME] rollout & consistency (vectorized):  449.38 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.92 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 793.84 ms
[TIME-FLAT] Total flat forward: 795.81 ms
[TIME-VEC] _forward_flat: 795.95 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 796.18 ms
[TIME] reward_preds total:  796.29 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  796.47 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.19 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1490.24 ms
[TIME] grad clipping:    1.96 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4720.47 ms
[TIME] encode(future):  447.41 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1509.69 ms
[TIME] rollout & consistency (vectorized):  452.60 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 898.62 ms
[TIME-FLAT] Total flat forward: 900.51 ms
[TIME-VEC] _forward_flat: 900.67 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 900.90 ms
[TIME] reward_preds total:  901.02 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  901.20 ms
[TIME] Q preds:    5.06 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.28 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1493.05 ms
[TIME] grad clipping:    1.84 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4817.72 ms
[TIME] encode(future):  453.01 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1525.22 ms
[TIME] rollout & consistency (vectorized):  449.61 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 787.85 ms
[TIME-FLAT] Total flat forward: 789.70 ms
[TIME-VEC] _forward_flat: 789.83 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 790.04 ms
[TIME] reward_preds total:  790.16 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  790.33 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1511.19 ms
[TIME] grad clipping:    2.56 ms
[TIME] optim.step:    1.16 ms
[TIME] _update total: 4743.89 ms
[TIME] encode(future):  454.57 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1536.63 ms
[TIME] rollout & consistency (vectorized):  452.61 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 797.91 ms
[TIME-FLAT] Total flat forward: 799.74 ms
[TIME-VEC] _forward_flat: 799.86 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 800.07 ms
[TIME] reward_preds total:  800.16 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.32 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1505.33 ms
[TIME] grad clipping:    2.38 ms
[TIME] optim.step:    1.16 ms
[TIME] _update total: 4763.73 ms
[TIME] encode(future):  453.02 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1613.82 ms
[TIME] rollout & consistency (vectorized):  453.12 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 803.48 ms
[TIME-FLAT] Total flat forward: 805.34 ms
[TIME-VEC] _forward_flat: 805.47 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 805.70 ms
[TIME] reward_preds total:  805.81 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  805.97 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1515.43 ms
[TIME] grad clipping:    2.60 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4855.84 ms
[TIME] encode(future):  452.26 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1530.41 ms
[TIME] rollout & consistency (vectorized):  456.87 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 792.07 ms
[TIME-FLAT] Total flat forward: 793.99 ms
[TIME-VEC] _forward_flat: 794.13 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 794.35 ms
[TIME] reward_preds total:  794.44 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  794.59 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1519.01 ms
[TIME] grad clipping:    2.62 ms
[TIME] optim.step:    1.20 ms
[TIME] _update total: 4767.57 ms
[TIME] encode(future):  451.09 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1617.64 ms
[TIME] rollout & consistency (vectorized):  454.43 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.98 ms
[TIME-FLAT] Gate forward: 1.22 ms
[TIME-FLAT] Softmax + TopK: 0.38 ms
[TIME-FLAT] Sparse Expert Forward: 797.21 ms
[TIME-FLAT] Total flat forward: 804.25 ms
[TIME-VEC] _forward_flat: 804.38 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 804.59 ms
[TIME] reward_preds total:  804.68 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  804.85 ms
[TIME] Q preds:    5.27 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1511.35 ms
[TIME] grad clipping:    2.63 ms
[TIME] optim.step:    1.18 ms
[TIME] _update total: 4853.99 ms
[TIME] encode(future):  453.48 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1517.29 ms
[TIME] rollout & consistency (vectorized):  453.23 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.88 ms
[TIME-FLAT] Softmax + TopK: 0.37 ms
[TIME-FLAT] Sparse Expert Forward: 792.22 ms
[TIME-FLAT] Total flat forward: 794.19 ms
[TIME-VEC] _forward_flat: 794.34 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 794.58 ms
[TIME] reward_preds total:  795.15 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.35 ms
[TIME] Q preds:    5.34 ms
[TIME] reward loss:    0.86 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1494.37 ms
[TIME] grad clipping:    2.67 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4728.40 ms
[TIME] encode(future):  450.02 ms
[TIME] compute TD targets:    0.98 ms
[TIME] encode(initial): 1602.07 ms
[TIME] rollout & consistency (vectorized):  451.32 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 792.16 ms
[TIME-FLAT] Total flat forward: 794.10 ms
[TIME-VEC] _forward_flat: 794.22 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 794.48 ms
[TIME] reward_preds total:  794.61 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  794.82 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1502.32 ms
[TIME] grad clipping:    2.17 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4814.96 ms
[TIME] encode(future):  453.95 ms
[TIME] compute TD targets:    4.96 ms
[TIME] encode(initial): 1520.15 ms
[TIME] rollout & consistency (vectorized):  453.40 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 787.87 ms
[TIME-FLAT] Total flat forward: 789.69 ms
[TIME-VEC] _forward_flat: 789.81 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 790.03 ms
[TIME] reward_preds total:  790.12 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  790.28 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.20 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1493.90 ms
[TIME] grad clipping:    1.79 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4729.64 ms
[TIME] encode(future):  451.24 ms
[TIME] compute TD targets:    0.97 ms
[TIME] encode(initial): 1611.76 ms
[TIME] rollout & consistency (vectorized):  454.23 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 799.45 ms
[TIME-FLAT] Total flat forward: 801.33 ms
[TIME-VEC] _forward_flat: 801.44 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 801.69 ms
[TIME] reward_preds total:  801.79 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  801.97 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.13 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1499.37 ms
[TIME] grad clipping:    2.01 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4832.30 ms
[TIME] encode(future):  457.54 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1530.97 ms
[TIME] rollout & consistency (vectorized):  456.09 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 794.60 ms
[TIME-FLAT] Total flat forward: 796.49 ms
[TIME-VEC] _forward_flat: 796.62 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 796.87 ms
[TIME] reward_preds total:  796.96 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.15 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1509.23 ms
[TIME] grad clipping:    2.65 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4765.43 ms
[TIME] encode(future):  454.93 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1617.09 ms
[TIME] rollout & consistency (vectorized):  451.27 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.01 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.51 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.38 ms
[TIME-FLAT] Sparse Expert Forward: 791.81 ms
[TIME-FLAT] Total flat forward: 793.82 ms
[TIME-VEC] _forward_flat: 793.95 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 794.17 ms
[TIME] reward_preds total:  794.28 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  794.47 ms
[TIME] Q preds:    5.22 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1507.76 ms
[TIME] grad clipping:    2.62 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4840.13 ms
[TIME] encode(future):  449.52 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1526.35 ms
[TIME] rollout & consistency (vectorized):  451.45 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.08 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.94 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 798.15 ms
[TIME-FLAT] Total flat forward: 800.15 ms
[TIME-VEC] _forward_flat: 800.31 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 800.56 ms
[TIME] reward_preds total:  800.66 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.85 ms
[TIME] Q preds:    5.24 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.03 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1515.03 ms
[TIME] grad clipping:    2.57 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4757.66 ms
[TIME] encode(future):  453.04 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1522.10 ms
[TIME] rollout & consistency (vectorized):  453.77 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 900.87 ms
[TIME-FLAT] Total flat forward: 902.72 ms
[TIME-VEC] _forward_flat: 902.83 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 903.08 ms
[TIME] reward_preds total:  903.18 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  903.38 ms
[TIME] Q preds:    5.03 ms
[TIME] reward loss:    0.89 ms
[TIME] value loss:    3.99 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1512.26 ms
[TIME] grad clipping:    2.77 ms
[TIME] optim.step:    1.16 ms
[TIME] _update total: 4861.17 ms
[TIME] encode(future):  450.00 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1531.18 ms
[TIME] rollout & consistency (vectorized):  454.07 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.87 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 796.90 ms
[TIME-FLAT] Total flat forward: 798.80 ms
[TIME-VEC] _forward_flat: 798.92 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 799.20 ms
[TIME] reward_preds total:  799.30 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  799.48 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.23 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1509.12 ms
[TIME] grad clipping:    2.88 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4758.62 ms
[TIME] encode(future):  449.60 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1546.32 ms
[TIME] rollout & consistency (vectorized):  452.97 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 798.57 ms
[TIME-FLAT] Total flat forward: 800.41 ms
[TIME-VEC] _forward_flat: 800.54 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 800.77 ms
[TIME] reward_preds total:  800.95 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  801.10 ms
[TIME] Q preds:    5.21 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1504.63 ms
[TIME] grad clipping:    3.06 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4769.52 ms
[TIME] encode(future):  449.33 ms
[TIME] compute TD targets:    1.02 ms
[TIME] encode(initial): 1632.49 ms
[TIME] rollout & consistency (vectorized):  448.33 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.81 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 788.10 ms
[TIME-FLAT] Total flat forward: 789.93 ms
[TIME-VEC] _forward_flat: 790.04 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 790.26 ms
[TIME] reward_preds total:  790.35 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  790.50 ms
[TIME] Q preds:    5.04 ms
[TIME] reward loss:    0.77 ms
[TIME] value loss:    2.20 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1504.39 ms
[TIME] grad clipping:    2.41 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4839.28 ms
[TIME] encode(future):  451.28 ms
[TIME] compute TD targets:    0.98 ms
[TIME] encode(initial): 1532.85 ms
[TIME] rollout & consistency (vectorized):  451.35 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.08 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 793.72 ms
[TIME-FLAT] Total flat forward: 795.56 ms
[TIME-VEC] _forward_flat: 795.69 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 795.93 ms
[TIME] reward_preds total:  796.04 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  796.21 ms
[TIME] Q preds:    5.06 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.24 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1510.46 ms
[TIME] grad clipping:    2.84 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4756.78 ms
[TIME] encode(future):  451.58 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1638.76 ms
[TIME] rollout & consistency (vectorized):  453.79 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 799.90 ms
[TIME-FLAT] Total flat forward: 801.80 ms
[TIME-VEC] _forward_flat: 801.95 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 802.17 ms
[TIME] reward_preds total:  802.27 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  802.45 ms
[TIME] Q preds:    5.15 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1513.41 ms
[TIME] grad clipping:    2.82 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4874.66 ms
[TIME] encode(future):  451.39 ms
[TIME] compute TD targets:    1.09 ms
[TIME] encode(initial): 1530.11 ms
[TIME] rollout & consistency (vectorized):  453.61 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.50 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 820.47 ms
[TIME-FLAT] Total flat forward: 822.41 ms
[TIME-VEC] _forward_flat: 822.54 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 822.77 ms
[TIME] reward_preds total:  822.92 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  823.10 ms
[TIME] Q preds:    5.30 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.22 ms
[TIME] backward: 1506.15 ms
[TIME] grad clipping:    3.29 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4779.89 ms
[TIME] encode(future):  448.06 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1550.05 ms
[TIME] rollout & consistency (vectorized):  454.36 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 889.07 ms
[TIME-FLAT] Total flat forward: 890.97 ms
[TIME-VEC] _forward_flat: 891.09 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 891.32 ms
[TIME] reward_preds total:  891.41 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  891.60 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1512.83 ms
[TIME] grad clipping:    3.08 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4871.75 ms
[TIME] encode(future):  454.05 ms
[TIME] compute TD targets:    1.08 ms
[TIME] encode(initial): 1520.80 ms
[TIME] rollout & consistency (vectorized):  455.09 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.43 ms
[TIME-FLAT] Sparse Expert Forward: 784.81 ms
[TIME-FLAT] Total flat forward: 786.76 ms
[TIME-VEC] _forward_flat: 786.88 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 787.10 ms
[TIME] reward_preds total:  787.20 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  787.37 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1487.59 ms
[TIME] grad clipping:    2.52 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4719.73 ms
[TIME] encode(future):  446.48 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1513.05 ms
[TIME] rollout & consistency (vectorized):  451.46 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 893.05 ms
[TIME-FLAT] Total flat forward: 894.89 ms
[TIME-VEC] _forward_flat: 895.02 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 895.24 ms
[TIME] reward_preds total:  895.34 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  895.63 ms
[TIME] Q preds:    5.12 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1498.92 ms
[TIME] grad clipping:    2.07 ms
[TIME] optim.step:    1.17 ms
[TIME] _update total: 4821.25 ms
[TIME] encode(future):  452.33 ms
[TIME] compute TD targets:    1.08 ms
[TIME] encode(initial): 1531.14 ms
[TIME] rollout & consistency (vectorized):  454.96 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.09 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.54 ms
[TIME-FLAT] Gate forward: 0.88 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 794.33 ms
[TIME-FLAT] Total flat forward: 796.33 ms
[TIME-VEC] _forward_flat: 796.48 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 796.78 ms
[TIME] reward_preds total:  796.91 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  797.11 ms
[TIME] Q preds:    5.22 ms
[TIME] reward loss:    0.87 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.19 ms
[TIME] backward: 1500.53 ms
[TIME] grad clipping:    2.01 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4750.28 ms
[TIME] encode(future):  452.64 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1529.63 ms
[TIME] rollout & consistency (vectorized):  448.89 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 792.30 ms
[TIME-FLAT] Total flat forward: 794.19 ms
[TIME-VEC] _forward_flat: 794.32 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 794.58 ms
[TIME] reward_preds total:  794.68 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  794.86 ms
[TIME] Q preds:    5.20 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1491.65 ms
[TIME] grad clipping:    2.37 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4732.56 ms
[TIME] encode(future):  452.67 ms
[TIME] compute TD targets:    1.09 ms
[TIME] encode(initial): 1631.18 ms
[TIME] rollout & consistency (vectorized):  454.81 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 792.97 ms
[TIME-FLAT] Total flat forward: 794.89 ms
[TIME-VEC] _forward_flat: 795.00 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 795.23 ms
[TIME] reward_preds total:  795.33 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.50 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1512.94 ms
[TIME] grad clipping:    3.66 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4862.67 ms
[TIME] encode(future):  453.55 ms
[TIME] compute TD targets:    1.16 ms
[TIME] encode(initial): 1547.69 ms
[TIME] rollout & consistency (vectorized):  453.69 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.43 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.31 ms
[TIME-FLAT] Sparse Expert Forward: 792.88 ms
[TIME-FLAT] Total flat forward: 794.62 ms
[TIME-VEC] _forward_flat: 794.75 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 794.98 ms
[TIME] reward_preds total:  795.07 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.21 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1499.07 ms
[TIME] grad clipping:    2.39 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4763.52 ms
[TIME] encode(future):  454.03 ms
[TIME] compute TD targets:    0.98 ms
[TIME] encode(initial): 1616.79 ms
[TIME] rollout & consistency (vectorized):  450.64 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.53 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 791.24 ms
[TIME-FLAT] Total flat forward: 793.17 ms
[TIME-VEC] _forward_flat: 793.31 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 793.56 ms
[TIME] reward_preds total:  793.65 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  793.82 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.23 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1506.47 ms
[TIME] grad clipping:    2.41 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4835.97 ms
[TIME] encode(future):  450.75 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1538.02 ms
[TIME] rollout & consistency (vectorized):  455.53 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 787.35 ms
[TIME-FLAT] Total flat forward: 789.23 ms
[TIME-VEC] _forward_flat: 789.35 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 789.56 ms
[TIME] reward_preds total:  789.65 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  789.82 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1509.98 ms
[TIME] grad clipping:    2.81 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4758.73 ms
[TIME] encode(future):  454.17 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1620.70 ms
[TIME] rollout & consistency (vectorized):  449.68 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 1.01 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 793.76 ms
[TIME-FLAT] Total flat forward: 795.82 ms
[TIME-VEC] _forward_flat: 795.95 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 796.17 ms
[TIME] reward_preds total:  796.26 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  796.44 ms
[TIME] Q preds:    5.26 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1511.61 ms
[TIME] grad clipping:    3.12 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4847.71 ms
[TIME] encode(future):  455.75 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1530.08 ms
[TIME] rollout & consistency (vectorized):  452.70 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 789.69 ms
[TIME-FLAT] Total flat forward: 791.54 ms
[TIME-VEC] _forward_flat: 791.68 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 791.89 ms
[TIME] reward_preds total:  791.98 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  792.15 ms
[TIME] Q preds:    5.05 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1506.93 ms
[TIME] grad clipping:    2.96 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4752.32 ms
[TIME] encode(future):  453.78 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1525.88 ms
[TIME] rollout & consistency (vectorized):  447.98 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.80 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 887.42 ms
[TIME-FLAT] Total flat forward: 889.21 ms
[TIME-VEC] _forward_flat: 889.34 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 889.57 ms
[TIME] reward_preds total:  889.67 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  889.83 ms
[TIME] Q preds:    5.06 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.45 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1514.69 ms
[TIME] grad clipping:    2.58 ms
[TIME] optim.step:    1.15 ms
[TIME] _update total: 4847.01 ms
[TIME] encode(future):  450.35 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1535.58 ms
[TIME] rollout & consistency (vectorized):  452.77 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 793.21 ms
[TIME-FLAT] Total flat forward: 795.03 ms
[TIME-VEC] _forward_flat: 795.15 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 795.39 ms
[TIME] reward_preds total:  795.48 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.67 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.46 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1491.57 ms
[TIME] grad clipping:    2.70 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4740.87 ms
[TIME] encode(future):  451.41 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1515.65 ms
[TIME] rollout & consistency (vectorized):  441.12 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 888.65 ms
[TIME-FLAT] Total flat forward: 890.45 ms
[TIME-VEC] _forward_flat: 890.56 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 890.78 ms
[TIME] reward_preds total:  890.87 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  891.02 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1505.09 ms
[TIME] grad clipping:    2.92 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4819.02 ms
[TIME] encode(future):  447.66 ms
[TIME] compute TD targets:    1.27 ms
[TIME] encode(initial): 1514.72 ms
[TIME] rollout & consistency (vectorized):  453.06 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 804.61 ms
[TIME-FLAT] Total flat forward: 806.53 ms
[TIME-VEC] _forward_flat: 806.66 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 806.90 ms
[TIME] reward_preds total:  807.01 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  807.18 ms
[TIME] Q preds:    5.15 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1493.99 ms
[TIME] grad clipping:    2.71 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4731.76 ms
[TIME] encode(future):  451.71 ms
[TIME] compute TD targets:    1.08 ms
[TIME] encode(initial): 1520.15 ms
[TIME] rollout & consistency (vectorized):  451.40 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.89 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 790.35 ms
[TIME-FLAT] Total flat forward: 792.33 ms
[TIME-VEC] _forward_flat: 792.44 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 792.67 ms
[TIME] reward_preds total:  792.78 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  792.96 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1549.04 ms
[TIME] grad clipping:    2.00 ms
[TIME] optim.step:    1.91 ms
[TIME] _update total: 4779.82 ms
[TIME] encode(future):  495.06 ms
[TIME] compute TD targets:    1.10 ms
[TIME] encode(initial): 1688.77 ms
[TIME] rollout & consistency (vectorized):  455.73 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 3.50 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 855.88 ms
[TIME-FLAT] Total flat forward: 860.79 ms
[TIME-VEC] _forward_flat: 860.90 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 861.12 ms
[TIME] reward_preds total:  861.22 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  861.40 ms
[TIME] Q preds:    5.04 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1517.93 ms
[TIME] grad clipping:    1.98 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 5033.00 ms
[TIME] encode(future):  453.75 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1607.36 ms
[TIME] rollout & consistency (vectorized):  469.69 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.88 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 796.30 ms
[TIME-FLAT] Total flat forward: 798.21 ms
[TIME-VEC] _forward_flat: 798.36 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 798.59 ms
[TIME] reward_preds total:  798.72 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  798.91 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.84 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1503.28 ms
[TIME] grad clipping:    2.66 ms
[TIME] optim.step:    1.16 ms
[TIME] _update total: 4847.61 ms
[TIME] encode(future):  446.37 ms
[TIME] compute TD targets:    1.20 ms
[TIME] encode(initial): 1624.71 ms
[TIME] rollout & consistency (vectorized):  453.47 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 797.01 ms
[TIME-FLAT] Total flat forward: 798.88 ms
[TIME-VEC] _forward_flat: 799.01 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 799.22 ms
[TIME] reward_preds total:  799.33 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  799.50 ms
[TIME] Q preds:    5.13 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1503.81 ms
[TIME] grad clipping:    2.62 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4842.53 ms
[TIME] encode(future):  447.69 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1525.00 ms
[TIME] rollout & consistency (vectorized):  447.94 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 797.91 ms
[TIME-FLAT] Total flat forward: 799.71 ms
[TIME-VEC] _forward_flat: 799.83 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 800.04 ms
[TIME] reward_preds total:  800.15 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.30 ms
[TIME] Q preds:    5.16 ms
[TIME] reward loss:    0.78 ms
[TIME] value loss:    2.22 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1508.35 ms
[TIME] grad clipping:    1.87 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4743.10 ms
[TIME] encode(future):  448.81 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1618.67 ms
[TIME] rollout & consistency (vectorized):  451.71 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 788.80 ms
[TIME-FLAT] Total flat forward: 790.64 ms
[TIME-VEC] _forward_flat: 790.78 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 791.02 ms
[TIME] reward_preds total:  791.12 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  791.30 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.18 ms
[TIME] backward: 1490.78 ms
[TIME] grad clipping:    1.79 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4815.01 ms
[TIME] encode(future):  446.91 ms
[TIME] compute TD targets:    1.12 ms
[TIME] encode(initial): 1533.62 ms
[TIME] rollout & consistency (vectorized):  448.52 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.62 ms
[TIME-FLAT] Gate forward: 0.92 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 796.22 ms
[TIME-FLAT] Total flat forward: 798.34 ms
[TIME-VEC] _forward_flat: 798.49 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 798.75 ms
[TIME] reward_preds total:  798.84 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  799.01 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.01 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1496.66 ms
[TIME] grad clipping:    2.11 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4738.71 ms
[TIME] encode(future):  453.77 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1521.10 ms
[TIME] rollout & consistency (vectorized):  453.52 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 900.48 ms
[TIME-FLAT] Total flat forward: 902.40 ms
[TIME-VEC] _forward_flat: 902.52 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 902.77 ms
[TIME] reward_preds total:  902.88 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  903.08 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.20 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1514.63 ms
[TIME] grad clipping:    2.68 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4860.76 ms
[TIME] encode(future):  464.67 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1560.07 ms
[TIME] rollout & consistency (vectorized):  451.91 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.91 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 800.11 ms
[TIME-FLAT] Total flat forward: 802.05 ms
[TIME-VEC] _forward_flat: 802.16 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 802.40 ms
[TIME] reward_preds total:  802.49 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  802.70 ms
[TIME] Q preds:    5.08 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1505.99 ms
[TIME] grad clipping:    2.70 ms
[TIME] optim.step:    1.18 ms
[TIME] _update total: 4799.79 ms
[TIME] encode(future):  460.14 ms
[TIME] compute TD targets:    1.04 ms
[TIME] encode(initial): 1554.91 ms
[TIME] rollout & consistency (vectorized):  472.65 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.49 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.37 ms
[TIME-FLAT] Sparse Expert Forward: 848.17 ms
[TIME-FLAT] Total flat forward: 850.12 ms
[TIME-VEC] _forward_flat: 850.26 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 850.50 ms
[TIME] reward_preds total:  850.59 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  850.81 ms
[TIME] Q preds:    5.26 ms
[TIME] reward loss:    0.85 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1547.11 ms
[TIME] grad clipping:    2.68 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4900.23 ms
[TIME] encode(future):  451.86 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1655.13 ms
[TIME] rollout & consistency (vectorized):  458.85 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 823.43 ms
[TIME-FLAT] Total flat forward: 825.33 ms
[TIME-VEC] _forward_flat: 825.46 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 825.69 ms
[TIME] reward_preds total:  825.80 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  825.98 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1554.00 ms
[TIME] grad clipping:    2.63 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 4960.25 ms
[TIME] encode(future):  456.92 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1582.12 ms
[TIME] rollout & consistency (vectorized):  460.39 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.09 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.62 ms
[TIME-FLAT] Gate forward: 1.13 ms
[TIME-FLAT] Softmax + TopK: 0.36 ms
[TIME-FLAT] Sparse Expert Forward: 828.69 ms
[TIME-FLAT] Total flat forward: 831.07 ms
[TIME-VEC] _forward_flat: 831.19 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 831.47 ms
[TIME] reward_preds total:  831.61 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  831.86 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1590.90 ms
[TIME] grad clipping:    4.41 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4938.47 ms
[TIME] encode(future):  494.62 ms
[TIME] compute TD targets:    1.16 ms
[TIME] encode(initial): 1726.09 ms
[TIME] rollout & consistency (vectorized):  474.28 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.48 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 801.80 ms
[TIME-FLAT] Total flat forward: 803.70 ms
[TIME-VEC] _forward_flat: 803.81 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 804.02 ms
[TIME] reward_preds total:  804.11 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  804.36 ms
[TIME] Q preds:    5.09 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1504.79 ms
[TIME] grad clipping:    3.63 ms
[TIME] optim.step:    1.13 ms
[TIME] _update total: 5019.91 ms
[TIME] encode(future):  442.89 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1552.46 ms
[TIME] rollout & consistency (vectorized):  456.76 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.82 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 809.48 ms
[TIME-FLAT] Total flat forward: 811.32 ms
[TIME-VEC] _forward_flat: 811.42 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 811.64 ms
[TIME] reward_preds total:  811.74 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  811.89 ms
[TIME] Q preds:    5.17 ms
[TIME] reward loss:    0.82 ms
[TIME] value loss:    2.29 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1524.08 ms
[TIME] grad clipping:    1.97 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4802.85 ms
[TIME] encode(future):  453.74 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1652.77 ms
[TIME] rollout & consistency (vectorized):  463.66 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 800.67 ms
[TIME-FLAT] Total flat forward: 802.55 ms
[TIME-VEC] _forward_flat: 802.68 ms
[TIME-VEC] Reshape back: 0.09 ms
[TIME-VEC] Total vector reward: 802.92 ms
[TIME] reward_preds total:  803.01 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  803.16 ms
[TIME] Q preds:    5.15 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.27 ms
[TIME] loss computation:    0.19 ms
[TIME] backward: 1545.48 ms
[TIME] grad clipping:    1.88 ms
[TIME] optim.step:    1.41 ms
[TIME] _update total: 4933.09 ms
[TIME] encode(future):  461.21 ms
[TIME] compute TD targets:    0.97 ms
[TIME] encode(initial): 1549.89 ms
[TIME] rollout & consistency (vectorized):  448.49 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 830.19 ms
[TIME-FLAT] Total flat forward: 832.02 ms
[TIME-VEC] _forward_flat: 832.13 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 832.34 ms
[TIME] reward_preds total:  832.43 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  832.59 ms
[TIME] Q preds:    5.07 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.06 ms
[TIME] loss computation:    0.57 ms
[TIME] backward: 1637.25 ms
[TIME] grad clipping:    2.60 ms
[TIME] optim.step:    1.22 ms
[TIME] _update total: 4944.15 ms
[TIME] encode(future):  464.67 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1681.29 ms
[TIME] rollout & consistency (vectorized):  453.03 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 798.38 ms
[TIME-FLAT] Total flat forward: 800.24 ms
[TIME-VEC] _forward_flat: 800.37 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 800.60 ms
[TIME] reward_preds total:  800.70 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  800.87 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    1.33 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1513.50 ms
[TIME] grad clipping:    2.69 ms
[TIME] optim.step:    1.11 ms
[TIME] _update total: 4928.50 ms
[TIME] encode(future):  453.22 ms
[TIME] compute TD targets:    1.07 ms
[TIME] encode(initial): 1528.28 ms
[TIME] rollout & consistency (vectorized):  450.91 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.45 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 798.69 ms
[TIME-FLAT] Total flat forward: 800.63 ms
[TIME-VEC] _forward_flat: 800.73 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 800.99 ms
[TIME] reward_preds total:  801.08 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  801.24 ms
[TIME] Q preds:    5.22 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.19 ms
[TIME] backward: 1549.74 ms
[TIME] grad clipping:    1.95 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4797.88 ms
[TIME] encode(future):  449.42 ms
[TIME] compute TD targets:    0.99 ms
[TIME] encode(initial): 1549.67 ms
[TIME] rollout & consistency (vectorized):  459.07 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.06 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 901.40 ms
[TIME-FLAT] Total flat forward: 903.26 ms
[TIME-VEC] _forward_flat: 903.38 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 903.62 ms
[TIME] reward_preds total:  903.72 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  903.88 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    0.99 ms
[TIME] value loss:    2.08 ms
[TIME] loss computation:    0.17 ms
[TIME] backward: 1489.43 ms
[TIME] grad clipping:    2.14 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4866.10 ms
[TIME] encode(future):  448.59 ms
[TIME] compute TD targets:    1.20 ms
[TIME] encode(initial): 1520.10 ms
[TIME] rollout & consistency (vectorized):  456.65 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.51 ms
[TIME-FLAT] Gate forward: 0.85 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 792.87 ms
[TIME-FLAT] Total flat forward: 794.81 ms
[TIME-VEC] _forward_flat: 794.95 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 795.20 ms
[TIME] reward_preds total:  795.30 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.48 ms
[TIME] Q preds:    5.11 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.14 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1496.01 ms
[TIME] grad clipping:    1.88 ms
[TIME] optim.step:    1.08 ms
[TIME] _update total: 4730.79 ms
[TIME] encode(future):  447.67 ms
[TIME] compute TD targets:    1.00 ms
[TIME] encode(initial): 1532.71 ms
[TIME] rollout & consistency (vectorized):  474.02 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.86 ms
[TIME-FLAT] Softmax + TopK: 0.37 ms
[TIME-FLAT] Sparse Expert Forward: 852.43 ms
[TIME-FLAT] Total flat forward: 854.38 ms
[TIME-VEC] _forward_flat: 854.49 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 854.71 ms
[TIME] reward_preds total:  854.82 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  855.00 ms
[TIME] Q preds:    5.33 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.09 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1609.99 ms
[TIME] grad clipping:    2.23 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4934.20 ms
[TIME] encode(future):  470.70 ms
[TIME] compute TD targets:    1.03 ms
[TIME] encode(initial): 1651.48 ms
[TIME] rollout & consistency (vectorized):  465.73 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.51 ms
[TIME-FLAT] Gate forward: 0.93 ms
[TIME-FLAT] Softmax + TopK: 0.34 ms
[TIME-FLAT] Sparse Expert Forward: 814.38 ms
[TIME-FLAT] Total flat forward: 816.42 ms
[TIME-VEC] _forward_flat: 816.54 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 816.76 ms
[TIME] reward_preds total:  816.85 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  817.02 ms
[TIME] Q preds:    5.25 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.12 ms
[TIME] loss computation:    0.15 ms
[TIME] backward: 1527.93 ms
[TIME] grad clipping:    2.05 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4946.91 ms
[TIME] encode(future):  461.13 ms
[TIME] compute TD targets:    1.21 ms
[TIME] encode(initial): 1537.35 ms
[TIME] rollout & consistency (vectorized):  452.95 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 789.19 ms
[TIME-FLAT] Total flat forward: 791.07 ms
[TIME-VEC] _forward_flat: 791.20 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 791.42 ms
[TIME] reward_preds total:  791.53 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  791.71 ms
[TIME] Q preds:    5.21 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.07 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1488.23 ms
[TIME] grad clipping:    2.03 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4745.40 ms
[TIME] encode(future):  445.87 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1629.12 ms
[TIME] rollout & consistency (vectorized):  453.60 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.07 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.35 ms
[TIME-FLAT] Sparse Expert Forward: 802.34 ms
[TIME-FLAT] Total flat forward: 804.23 ms
[TIME-VEC] _forward_flat: 804.34 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 804.57 ms
[TIME] reward_preds total:  804.68 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  804.85 ms
[TIME] Q preds:    5.51 ms
[TIME] reward loss:    0.80 ms
[TIME] value loss:    2.05 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1503.89 ms
[TIME] grad clipping:    1.96 ms
[TIME] optim.step:    1.14 ms
[TIME] _update total: 4851.35 ms
[TIME] encode(future):  449.38 ms
[TIME] compute TD targets:    1.06 ms
[TIME] encode(initial): 1528.33 ms
[TIME] rollout & consistency (vectorized):  449.04 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.44 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 793.31 ms
[TIME-FLAT] Total flat forward: 795.15 ms
[TIME-VEC] _forward_flat: 795.26 ms
[TIME-VEC] Reshape back: 0.08 ms
[TIME-VEC] Total vector reward: 795.49 ms
[TIME] reward_preds total:  795.59 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  795.78 ms
[TIME] Q preds:    5.22 ms
[TIME] reward loss:    0.83 ms
[TIME] value loss:    2.10 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1503.26 ms
[TIME] grad clipping:    1.92 ms
[TIME] optim.step:    1.09 ms
[TIME] _update total: 4739.63 ms
[TIME] encode(future):  460.29 ms
[TIME] compute TD targets:    1.05 ms
[TIME] encode(initial): 1674.57 ms
[TIME] rollout & consistency (vectorized):  455.83 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.84 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
[TIME-FLAT] Sparse Expert Forward: 795.70 ms
[TIME-FLAT] Total flat forward: 797.51 ms
[TIME-VEC] _forward_flat: 797.62 ms
[TIME-VEC] Reshape back: 0.06 ms
[TIME-VEC] Total vector reward: 797.82 ms
[TIME] reward_preds total:  797.92 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  798.10 ms
[TIME] Q preds:    5.10 ms
[TIME] reward loss:    0.81 ms
[TIME] value loss:    2.17 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1531.34 ms
[TIME] grad clipping:    2.05 ms
[TIME] optim.step:    1.12 ms
[TIME] _update total: 4934.05 ms
[TIME] encode(future):  520.38 ms
[TIME] compute TD targets:    0.97 ms
[TIME] encode(initial): 1531.74 ms
[TIME] rollout & consistency (vectorized):  447.50 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.08 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.47 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.32 ms
[TIME-FLAT] Sparse Expert Forward: 789.80 ms
[TIME-FLAT] Total flat forward: 791.66 ms
[TIME-VEC] _forward_flat: 791.79 ms
[TIME-VEC] Reshape back: 0.07 ms
[TIME-VEC] Total vector reward: 792.04 ms
[TIME] reward_preds total:  792.13 ms
[TIME] reward_preds shape: torch.Size([3, 256, 101])
[TIME] reward preds:  792.28 ms
[TIME] Q preds:    5.14 ms
[TIME] reward loss:    0.79 ms
[TIME] value loss:    2.04 ms
[TIME] loss computation:    0.16 ms
[TIME] backward: 1568.91 ms
[TIME] grad clipping:    2.05 ms
[TIME] optim.step:    1.10 ms
[TIME] _update total: 4874.49 ms
[TIME] encode(future):  446.21 ms
[TIME] compute TD targets:    1.01 ms
[TIME] encode(initial): 1617.32 ms
[TIME] rollout & consistency (vectorized):  443.05 ms
[TIME] reward_preds starting... shapes: z=torch.Size([3, 256, 512]), a=torch.Size([3, 256, 6])
[TIME] z_a_seq created: torch.Size([3, 256, 518]), now calling model.reward...
[DEBUG] vectorized_reward H=3, B=256, Dp=518
[TIME-VEC] Reshape: 0.05 ms
[TIME-FLAT] Contiguous: 0.00 ms, shape=torch.Size([768, 518])
[TIME-FLAT] Pre-projection: 0.46 ms
[TIME-FLAT] Gate forward: 0.83 ms
[TIME-FLAT] Softmax + TopK: 0.33 ms
Traceback (most recent call last):
  File "/home/levi/Documents/MOORE-TDMPC/moore_tdmpc/train_moore.py", line 126, in <module>
    train()
  File "/home/levi/Documents/MOORE-TDMPC/moore_tdmpc/utils/profiler.py", line 28, in wrapper
    result = func(*args, **kwargs)
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/home/levi/Documents/MOORE-TDMPC/moore_tdmpc/train_moore.py", line 106, in train
    trainer.train()
  File "/home/levi/Documents/MOORE-TDMPC/moore_tdmpc/utils/profiler.py", line 28, in wrapper
    result = func(*args, **kwargs)
  File "/home/levi/Documents/MOORE-TDMPC/TDMPC2/algorithms/trainer/online_trainer.py", line 126, in train
    _train_metrics = self.agent.update(self.buffer)
  File "/home/levi/Documents/MOORE-TDMPC/TDMPC2/algorithms/tdmpc2.py", line 329, in update
    return self._update(obs, action, reward, **kwargs)
  File "/home/levi/Documents/MOORE-TDMPC/moore_tdmpc/utils/profiler.py", line 28, in wrapper
    result = func(*args, **kwargs)
  File "/home/levi/Documents/MOORE-TDMPC/moore_tdmpc/utils/timer.py", line 94, in wrapper
    result = f(*args, **kwargs)
  File "/home/levi/Documents/MOORE-TDMPC/moore_tdmpc/train_moore_agent.py", line 763, in _update
    reward_preds = self.model.reward(_v_c_seq, action, task=None)
  File "/home/levi/Documents/MOORE-TDMPC/moore_tdmpc/world_model.py", line 340, in reward
    reward = self._reward.forward_vectorized(z_a_seq)  # 显式调用向量化方法
  File "/home/levi/Documents/MOORE-TDMPC/moore_tdmpc/moe_rewards.py", line 117, in forward_vectorized
    out_flat = self._forward_flat(flat)  # [H*B, R]
  File "/home/levi/Documents/MOORE-TDMPC/moore_tdmpc/moe_rewards.py", line 204, in _forward_flat
    expert_output = expert_nets[expert_idx](input_i)  # [1, reward_dim]
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1740, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1751, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1740, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1751, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/site-packages/torch/fx/traceback.py", line 83, in format_stack
    return traceback.format_list(traceback.extract_stack()[:-1])
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/traceback.py", line 211, in extract_stack
    stack = StackSummary.extract(walk_stack(f), limit=limit)
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/traceback.py", line 362, in extract
    linecache.checkcache(filename)
  File "/home/levi/anaconda3/envs/tdmpc2/lib/python3.9/linecache.py", line 72, in checkcache
    stat = os.stat(fullname)
KeyboardInterrupt